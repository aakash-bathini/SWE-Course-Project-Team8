"""
Phase 2 FastAPI Application - Trustworthy Model Registry
Main application entry point with REST API endpoints matching OpenAPI spec v3.4.3
"""

import logging
import os
import sys

# Configure logging first
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# Set environment defaults for Lambda
os.environ.setdefault("USE_SQLITE", "0")

try:
    from fastapi import (
        FastAPI,
        HTTPException,
        Depends,
        Query,
        Header,
        Response,
        File,
        Form,
        UploadFile,
        Request,
    )
    from fastapi.middleware.cors import CORSMiddleware
    from fastapi.security import HTTPBearer
    from fastapi.responses import FileResponse
    from pydantic import BaseModel
    from typing import List, Optional, Dict, Any
    from datetime import datetime
    from enum import Enum
    from mangum import Mangum
    import uvicorn
    import hashlib
except ImportError as e:
    logger.error(f"Critical import failed: {e}", exc_info=True)
    raise

# Import auth module
try:
    from src.auth.jwt_auth import auth as jwt_auth
except ImportError as e:
    logger.error(f"Failed to import jwt_auth: {e}", exc_info=True)
    raise

# Initialize FastAPI app
app = FastAPI(
    title="ECE 461 - Fall 2025 - Project Phase 2",
    description="API for ECE 461/Fall 2025/Project Phase 2: A Trustworthy Model Registry",
    version="3.4.3",
    docs_url="/docs",
    redoc_url="/redoc",
)

# CORS middleware for frontend integration
# Allow all origins for health endpoint (autograder compatibility)
# For other endpoints, we restrict to known origins
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins for autograder compatibility
    allow_credentials=False,  # Set to False when allowing all origins
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# Security (we will accept X-Authorization header per spec; keep HTTPBearer for flexibility)
security = HTTPBearer()

# Import Phase 2 components (after app initialization to avoid circular imports)
try:
    from src.api.huggingface import scrape_hf_url  # noqa: E402
    from src.metrics.phase2_adapter import (  # noqa: E402
        create_eval_context_from_model_data,
        calculate_phase2_metrics,
        calculate_phase2_net_score,
    )
    from src.metrics import size as size_metric  # noqa: E402
except ImportError as e:
    logger.warning(f"Optional imports failed (may not be available in Lambda): {e}")
    # Set to None to prevent errors - use type: ignore for mypy compatibility
    scrape_hf_url = None  # type: ignore[assignment]
    create_eval_context_from_model_data = None  # type: ignore[assignment]
    calculate_phase2_metrics = None  # type: ignore[assignment]
    calculate_phase2_net_score = None  # type: ignore[assignment]
    size_metric = None  # type: ignore[assignment]

# Storage layer selection: in-memory (default), SQLite (local dev), or S3 (production)
# Production (Lambda): Use S3 only
# Local development: Use SQLite
USE_SQLITE: bool = (
    os.environ.get("USE_SQLITE", "0") == "1"
    and os.environ.get("ENVIRONMENT") != "production"
    and not os.environ.get("AWS_LAMBDA_FUNCTION_NAME")
)
USE_S3: bool = os.environ.get("USE_S3", "0") == "1" or (
    os.environ.get("ENVIRONMENT") == "production"
    or os.environ.get("AWS_LAMBDA_FUNCTION_NAME") is not None
)

artifacts_db: Dict[str, Dict[str, Any]] = {}  # artifact_id -> artifact_data (in-memory)
users_db: Dict[str, Dict[str, Any]] = {}
audit_log: List[Dict[str, Any]] = []

# Initialize S3 storage if enabled (production only)
s3_storage = None
print(
    f"DEBUG: USE_S3={USE_S3}, "
    f"AWS_LAMBDA_FUNCTION_NAME={os.environ.get('AWS_LAMBDA_FUNCTION_NAME', 'not set')}, "
    f"ENVIRONMENT={os.environ.get('ENVIRONMENT', 'not set')}"
)
sys.stdout.flush()
if USE_S3:
    try:
        from src.storage.s3_storage import get_s3_storage

        print("DEBUG: Attempting to initialize S3 storage...")
        sys.stdout.flush()
        s3_storage = get_s3_storage()
        if s3_storage:
            bucket_name = os.environ.get("S3_BUCKET_NAME", "not set")
            logger.info(f"S3 storage initialized: bucket={bucket_name}")
            print(f"DEBUG: âœ… S3 storage initialized: bucket={bucket_name}")
            sys.stdout.flush()
        else:
            logger.warning("S3 storage requested but bucket not configured or unavailable")
            print("DEBUG: âš ï¸ S3 storage NOT initialized - bucket not configured or unavailable")
            sys.stdout.flush()
            USE_S3 = False
    except Exception as e:
        logger.error(f"S3 initialization failed: {e}, falling back to other storage", exc_info=True)
        print(f"DEBUG: âŒ S3 initialization failed: {e}")
        sys.stdout.flush()
        USE_S3 = False
else:
    print("DEBUG: S3 storage disabled (USE_S3=False)")
    sys.stdout.flush()

# Initialize SQLite if enabled (local development only)
if USE_SQLITE:
    try:
        from src.db.database import Base, engine, get_db
        from src.db import crud as db_crud

        # Ensure schema exists
        Base.metadata.create_all(bind=engine)
        logger.info(
            f"SQLite initialized successfully. Database path: {os.environ.get('SQLALCHEMY_DATABASE_URL', 'default')}"
        )
    except Exception as e:
        logger.error(
            f"SQLite initialization failed: {e}, falling back to in-memory storage", exc_info=True
        )
        USE_SQLITE = False
else:
    if os.environ.get("ENVIRONMENT") == "production" or os.environ.get("AWS_LAMBDA_FUNCTION_NAME"):
        logger.info("SQLite disabled in production - using S3 storage")
    else:
        logger.info("SQLite disabled - using in-memory storage")

# Default admin user - password matches what autograder sends (requirements doc says 'packages', not 'artifacts')
DEFAULT_ADMIN = {
    "username": "ece30861defaultadminuser",
    "password": "correcthorsebatterystaple123(!__+@**(A'\"`;DROP TABLE packages;",
    "permissions": ["upload", "search", "download", "admin"],
    "created_at": datetime.now().isoformat(),
}

# Add default admin to users database and DB (if enabled)
admin_username: str = str(DEFAULT_ADMIN["username"])
# Use copy() to ensure we have an independent dict that won't be affected by mutations
users_db[admin_username] = DEFAULT_ADMIN.copy()
if USE_SQLITE:
    try:
        with next(get_db()) as _db:  # type: ignore[misc]
            db_crud.ensure_schema(_db)
            db_crud.upsert_default_admin(
                _db,
                username=str(DEFAULT_ADMIN["username"]),
                password=str(DEFAULT_ADMIN["password"]),
                permissions=list(DEFAULT_ADMIN["permissions"]),  # type: ignore[arg-type]
            )
    except Exception as e:
        logger.warning(f"Failed to initialize default admin in database: {e}")
        # Ensure default admin is still in in-memory users_db even if SQLite fails
        if admin_username not in users_db:
            users_db[admin_username] = DEFAULT_ADMIN.copy()


# Pydantic models matching OpenAPI spec v3.3.1


class ArtifactType(str, Enum):
    MODEL = "model"
    DATASET = "dataset"
    CODE = "code"


class ArtifactMetadata(BaseModel):
    name: str
    id: str
    type: ArtifactType


class ArtifactData(BaseModel):
    url: str
    download_url: Optional[str] = None


class Artifact(BaseModel):
    metadata: ArtifactMetadata
    data: ArtifactData


class ArtifactQuery(BaseModel):
    name: str
    types: Optional[List[ArtifactType]] = None


class User(BaseModel):
    name: str
    is_admin: bool


class UserAuthenticationInfo(BaseModel):
    password: str


class AuthenticationRequest(BaseModel):
    user: User
    secret: UserAuthenticationInfo


class AuthenticationToken(BaseModel):
    token: str


class ModelRating(BaseModel):
    name: str
    category: str
    net_score: float
    net_score_latency: float
    ramp_up_time: float
    ramp_up_time_latency: float
    bus_factor: float
    bus_factor_latency: float
    performance_claims: float
    performance_claims_latency: float
    license: float
    license_latency: float
    dataset_and_code_score: float
    dataset_and_code_score_latency: float
    dataset_quality: float
    dataset_quality_latency: float
    code_quality: float
    code_quality_latency: float
    reproducibility: float
    reproducibility_latency: float
    reviewedness: float
    reviewedness_latency: float
    tree_score: float
    tree_score_latency: float
    size_score: Dict[str, float]
    size_score_latency: float


class ArtifactCost(BaseModel):
    total_cost: float
    standalone_cost: Optional[float] = None


class ArtifactAuditEntry(BaseModel):
    user: User
    date: str
    artifact: ArtifactMetadata
    action: str


class ArtifactLineageNode(BaseModel):
    artifact_id: str
    name: str
    source: str
    metadata: Optional[Dict[str, Any]] = None


class ArtifactLineageEdge(BaseModel):
    from_node_artifact_id: str
    to_node_artifact_id: str
    relationship: str


class ArtifactLineageGraph(BaseModel):
    nodes: List[ArtifactLineageNode]
    edges: List[ArtifactLineageEdge]


class SimpleLicenseCheckRequest(BaseModel):
    github_url: str


class ArtifactRegEx(BaseModel):
    regex: str


class EnumerateOffset(BaseModel):
    offset: str


class HealthResponse(BaseModel):
    status: str
    timestamp: str
    uptime: str
    models_count: int
    users_count: int
    last_hour_activity: Dict[str, int]


# -------------------------
# Health component schemas (v3.4.2)
# -------------------------


class HealthStatus(str, Enum):
    ok = "ok"
    degraded = "degraded"
    critical = "critical"
    unknown = "unknown"


class HealthLogReference(BaseModel):
    label: str
    url: str
    tail_available: Optional[bool] = None
    last_updated_at: Optional[str] = None


class HealthTimelineEntry(BaseModel):
    bucket: str
    value: float
    unit: Optional[str] = None


class HealthIssue(BaseModel):
    code: str
    severity: str
    summary: str
    details: Optional[str] = None


class HealthComponentDetail(BaseModel):
    id: str
    display_name: Optional[str] = None
    status: HealthStatus
    observed_at: str
    description: Optional[str] = None
    metrics: Optional[Dict[str, Any]] = None
    issues: Optional[List[HealthIssue]] = None
    timeline: Optional[List[HealthTimelineEntry]] = None
    logs: Optional[List[HealthLogReference]] = None


class HealthComponentCollection(BaseModel):
    components: List[HealthComponentDetail]
    generated_at: str
    window_minutes: Optional[int] = None


# Server-side token call count tracking (Milestone 3 requirement: 1000 calls or 10 hours)
token_call_counts: Dict[str, int] = {}  # token_hash -> call_count


def generate_download_url(
    artifact_type: str, artifact_id: str, request: Optional[Request] = None
) -> str:
    """
    Generate download URL for an artifact.
    Uses request base URL if available, otherwise constructs from environment or relative path.
    """
    if request:
        base_url = str(request.base_url).rstrip("/")
        return f"{base_url}/artifacts/{artifact_type}/{artifact_id}/download"
    # Fallback: try to get from environment variable or use relative path
    api_url = os.environ.get("API_GATEWAY_URL") or os.environ.get("REACT_APP_API_URL")
    if api_url:
        base_url = api_url.rstrip("/")
        return f"{base_url}/artifacts/{artifact_type}/{artifact_id}/download"
    # Last resort: relative path (client will need to resolve)
    return f"/artifacts/{artifact_type}/{artifact_id}/download"


# Authentication functions (Milestone 3 - with call count tracking)
def verify_token(
    x_authorization: Optional[str] = Header(None, alias="X-Authorization"),
    authorization: Optional[str] = Header(None, alias="Authorization"),
) -> Dict[str, Any]:
    """Verify token from X-Authorization (spec) or Authorization header and return user info"""
    token: Optional[str] = None

    if x_authorization and isinstance(x_authorization, str) and x_authorization.strip():
        raw = x_authorization.strip()
        token = raw[7:].strip() if raw.lower().startswith("bearer ") else raw
    elif authorization and isinstance(authorization, str) and authorization.strip():
        # Support both "Bearer <token>" and raw token in Authorization
        raw = authorization.strip()
        if raw.lower().startswith("bearer "):
            token = raw[7:].strip()
        else:
            token = raw

    if not token:
        # Spec: 403 for invalid or missing AuthenticationToken
        raise HTTPException(
            status_code=403,
            detail="Authentication failed due to invalid or missing AuthenticationToken.",
        )

    # Validate JWT token
    payload = jwt_auth.verify_token(token)
    if not payload:
        raise HTTPException(
            status_code=403,
            detail="Authentication failed due to invalid or missing AuthenticationToken.",
        )

    # Track call count server-side (Milestone 3 requirement: 1000 calls or 10 hours)
    # Use token hash as key to track calls
    token_hash = hashlib.sha256(token.encode()).hexdigest()

    # Get or initialize call count
    current_calls = token_call_counts.get(token_hash, payload.get("call_count", 0))

    # Check if exceeded max calls
    max_calls = payload.get("max_calls", 1000)
    if current_calls >= max_calls:
        raise HTTPException(
            status_code=403,
            detail="Authentication failed due to invalid or missing AuthenticationToken.",
        )

    # Increment call count
    token_call_counts[token_hash] = current_calls + 1

    username = str(payload.get("sub", DEFAULT_ADMIN["username"]))
    permissions = payload.get("permissions", DEFAULT_ADMIN["permissions"])  # type: ignore[assignment]
    return {"username": username, "permissions": permissions}


def check_permission(user: Dict[str, Any], required_permission: str) -> bool:
    """Check if user has required permission"""
    return required_permission in user.get("permissions", [])


# User registration request model (Milestone 3)
class UserRegistrationRequest(BaseModel):
    username: str
    password: str
    permissions: List[str]


# API Endpoints matching OpenAPI spec v3.3.1


@app.get("/health", response_model=HealthResponse)
async def health_check(response: Response) -> HealthResponse:
    """System health endpoint - lightweight liveness probe (BASELINE)"""
    # Explicitly set CORS headers for API Gateway compatibility
    response.headers["Access-Control-Allow-Origin"] = "*"
    response.headers["Access-Control-Allow-Methods"] = "GET, OPTIONS"
    response.headers["Access-Control-Allow-Headers"] = "*"

    try:
        # Safely get counts - handle case where databases might not be initialized
        models_count = len(artifacts_db) if artifacts_db is not None else 0
        users_count = len(users_db) if users_db is not None else 0

        return HealthResponse(
            status="healthy",
            timestamp=datetime.now().isoformat(),
            uptime="0:00:00",  # Simplified for Milestone 1
            models_count=models_count,
            users_count=users_count,
            last_hour_activity={"uploads": 0, "downloads": 0, "searches": 0},
        )
    except Exception as e:
        # Log error but still return healthy status for liveness probe
        logger.error(f"Health check error: {str(e)}")
        return HealthResponse(
            status="healthy",
            timestamp=datetime.now().isoformat(),
            uptime="0:00:00",
            models_count=0,
            users_count=0,
            last_hour_activity={"uploads": 0, "downloads": 0, "searches": 0},
        )


# Component health (NON-BASELINE in spec v3.4.2), minimal implementation
@app.get("/health/components", response_model=HealthComponentCollection)
async def health_components(
    windowMinutes: int = Query(60, ge=5, le=1440),
    includeTimeline: bool = Query(False),
) -> HealthComponentCollection:
    now_iso = datetime.now().isoformat()

    components: List[HealthComponentDetail] = []

    # API component
    api_component = HealthComponentDetail(
        id="api",
        display_name="FastAPI Service",
        status=HealthStatus.ok,
        observed_at=now_iso,
        description="Handles REST API requests",
        metrics={
            "uptime_seconds": 0,
            "total_requests": 0,
        },
        issues=[],
        logs=[
            HealthLogReference(
                label="Application Log",
                url="https://example.com/logs/app.log",
            )
        ],
        timeline=(
            [HealthTimelineEntry(bucket=now_iso, value=0.0, unit="rpm")]
            if includeTimeline
            else None
        ),
    )
    components.append(api_component)

    # Metrics component (placeholder)
    metrics_component = HealthComponentDetail(
        id="metrics",
        display_name="Metrics Aggregator",
        status=HealthStatus.ok,
        observed_at=now_iso,
        description="Aggregates request metrics",
        metrics={
            "routes_tracked": 0,
        },
        issues=[],
        logs=[],
        timeline=(
            [HealthTimelineEntry(bucket=now_iso, value=0.0, unit="rpm")]
            if includeTimeline
            else None
        ),
    )
    components.append(metrics_component)

    return HealthComponentCollection(
        components=components,
        generated_at=now_iso,
        window_minutes=windowMinutes,
    )


# Root route for CI/API liveness checks
@app.get("/")
async def root() -> Dict[str, str]:
    return {"message": "Trustworthy Model Registry API"}


# Model upload and download endpoints


@app.post("/models/upload", response_model=Artifact, status_code=201)
async def models_upload(
    request: Request,
    file: UploadFile = File(...),
    name: Optional[str] = Form(None),
    user: Dict[str, Any] = Depends(verify_token),
) -> Artifact:
    """Upload model as ZIP file"""
    from src.storage import file_storage

    try:
        # Validate file is a ZIP
        if not file.filename or not file.filename.endswith(".zip"):
            raise HTTPException(status_code=400, detail="File must be a ZIP archive")

        # Generate artifact ID
        # Priority: in-memory (for consistency within Lambda invocation) > SQLite > S3
        # Use in-memory count first to avoid S3 eventual consistency issues
        model_count = sum(
            1 for art in artifacts_db.values() if art.get("metadata", {}).get("type") == "model"
        )
        if USE_SQLITE:
            try:
                with next(get_db()) as _db:  # type: ignore[misc]
                    sqlite_count = db_crud.count_artifacts_by_type(_db, "model")
                    # Use max of in-memory and SQLite counts to handle edge cases
                    model_count = max(model_count, sqlite_count)
            except Exception:
                pass  # Use in-memory count if SQLite fails
        # Fallback to S3 count only if in-memory is empty (shouldn't happen, but safety check)
        if model_count == 0 and USE_S3 and s3_storage:
            try:
                s3_count = s3_storage.count_artifacts_by_type("model")
                model_count = max(model_count, s3_count)
            except Exception:
                pass  # Use in-memory count if S3 fails
        artifact_id = f"model-{model_count + 1}-{int(datetime.now().timestamp())}"

        # Read file content
        content = await file.read()

        # Save ZIP file
        file_info = file_storage.save_uploaded_file(artifact_id, content, file.filename)

        # Extract ZIP
        artifact_dir = file_storage.get_artifact_directory(artifact_id)
        extracted_files = file_storage.extract_zip(file_info["path"], artifact_dir)

        # Find and read model card
        card_path = file_storage.find_model_card(artifact_dir)
        readme_text = file_storage.read_model_card(card_path) if card_path else ""

        # Determine model name
        model_name = name or file.filename.replace(".zip", "")

        # Create artifact entry
        artifact_entry = {
            "metadata": {
                "name": model_name,
                "id": artifact_id,
                "type": "model",
            },
            "data": {
                "url": f"local://{artifact_id}",
                "files": extracted_files,
                "checksum": file_info["checksum"],
                "size": file_info["size"],
            },
            "created_at": datetime.now().isoformat(),
            "created_by": user["username"],
        }

        artifacts_db[artifact_id] = artifact_entry

        # Store in S3 if enabled - verify save success
        if USE_S3 and s3_storage:
            logger.info(f"ðŸ’¾ Saving artifact to S3: id={artifact_id}, name={model_name}")
            print(
                f"DEBUG: ðŸ’¾ Saving artifact to S3 (models_upload): id={artifact_id}, name={model_name}"
            )
            sys.stdout.flush()
            success = s3_storage.save_artifact_metadata(artifact_id, artifact_entry)
            if not success:
                logger.error(
                    f"âŒ CRITICAL: Failed to save artifact {artifact_id} to S3 in models_upload - "
                    f"artifact will not persist!"
                )
                print(
                    f"DEBUG: âŒ CRITICAL: Failed to save artifact {artifact_id} to S3 in models_upload"
                )
                sys.stdout.flush()
            else:
                logger.info(f"âœ… Successfully saved artifact {artifact_id} to S3")
                print(f"DEBUG: âœ… Successfully saved artifact {artifact_id} to S3 (models_upload)")
                sys.stdout.flush()

        # Store in SQLite if enabled
        if USE_SQLITE:
            with next(get_db()) as _db:  # type: ignore[misc]
                db_crud.create_artifact(
                    _db,
                    artifact_id=artifact_id,
                    name=model_name,
                    type_="model",
                    url=f"local://{artifact_id}",
                )

        # Log audit entry
        audit_log.append(
            {
                "user": {"name": user["username"], "is_admin": user.get("is_admin", False)},
                "date": datetime.now().isoformat(),
                "artifact": artifact_entry["metadata"],
                "action": "UPLOAD",
            }
        )

        # Trigger metrics calculation
        try:
            model_data = {
                "url": f"local://{artifact_id}",
                "hf_data": [{"readme_text": readme_text}] if readme_text else [],
                "gh_data": [],
            }
            await calculate_phase2_metrics(model_data)
        except Exception as e:
            logger.warning(f"Metrics calculation failed for uploaded model: {e}")

        download_url = generate_download_url("model", artifact_id, request)
        return Artifact(
            metadata=ArtifactMetadata(**artifact_entry["metadata"]),
            data=ArtifactData(url=artifact_entry["data"]["url"], download_url=download_url),
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Upload failed: {e}")
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")


@app.get("/models/{id}/download")
async def models_download(
    id: str,
    aspect: str = Query("full", pattern="^(full|weights|datasets|code)$"),
    user: Dict[str, Any] = Depends(verify_token),
):
    """Download model files with optional aspect filtering"""
    from src.storage import file_storage
    import tempfile

    try:
        # Check if artifact exists - Priority: S3 (production) > SQLite (local) > in-memory
        artifact_exists = False
        artifact_type = None
        artifact_url = None
        artifact_metadata = None

        if USE_S3 and s3_storage:
            existing_data = s3_storage.get_artifact_metadata(id)
            if existing_data:
                artifact_exists = True
                artifact_type = existing_data.get("metadata", {}).get("type")
                artifact_url = existing_data.get("data", {}).get("url", "")
                artifact_metadata = existing_data.get("metadata", {})
        elif USE_SQLITE:
            with next(get_db()) as _db:  # type: ignore[misc]
                art = db_crud.get_artifact(_db, id)
                if art:
                    artifact_exists = True
                    artifact_type = art.type
                    artifact_url = art.url
                    artifact_metadata = {"name": art.name, "id": art.id, "type": art.type}
        else:
            if id in artifacts_db:
                artifact_exists = True
                artifact_data = artifacts_db[id]
                artifact_type = artifact_data["metadata"]["type"]
                artifact_url = artifact_data["data"].get("url", "")
                artifact_metadata = artifact_data["metadata"]

        if not artifact_exists:
            raise HTTPException(status_code=404, detail="Artifact does not exist.")

        if artifact_type != "model":
            raise HTTPException(status_code=400, detail="Not a model artifact.")

        # Check if this is a URL-only model (no local files)
        if artifact_url and not artifact_url.startswith("local://"):
            raise HTTPException(
                status_code=404,
                detail="Model files not found. This model may only have URL metadata (downloaded from HuggingFace).",
            )

        # Get artifact directory
        artifact_dir = file_storage.get_artifact_directory(id)

        if not os.path.exists(artifact_dir):
            raise HTTPException(
                status_code=404,
                detail="Model files not found. This model may only have URL metadata.",
            )

        # Filter files by aspect
        files = file_storage.filter_files_by_aspect(artifact_dir, aspect)

        if not files:
            raise HTTPException(status_code=404, detail=f"No files found for aspect: {aspect}")

        # Create temporary ZIP with filtered files
        with tempfile.NamedTemporaryFile(delete=False, suffix=".zip") as tmp:
            zip_path = tmp.name

        file_storage.create_zip_from_files(files, artifact_dir, zip_path)

        # Calculate checksum for integrity
        checksum = file_storage.calculate_checksum(zip_path)

        # Log download audit
        if artifact_metadata:
            audit_log.append(
                {
                    "user": {"name": user["username"], "is_admin": user.get("is_admin", False)},
                    "date": datetime.now().isoformat(),
                    "artifact": artifact_metadata,
                    "action": f"DOWNLOAD_{aspect.upper()}",
                }
            )
            if USE_SQLITE:
                with next(get_db()) as _db:  # type: ignore[misc]
                    art = db_crud.get_artifact(_db, id)
                    if art:
                        db_crud.log_audit(
                            _db,
                            artifact=art,
                            user_name=user["username"],
                            user_is_admin=user.get("is_admin", False),
                            action=f"DOWNLOAD_{aspect.upper()}",
                        )

        # Return file with checksum in headers
        artifact_name = artifact_metadata.get("name", "model") if artifact_metadata else "model"
        return FileResponse(
            path=zip_path,
            media_type="application/zip",
            filename=f"{artifact_name}_{aspect}.zip",
            headers={"X-File-Checksum": checksum, "X-File-Aspect": aspect},
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Download failed: {e}")
        raise HTTPException(status_code=500, detail=f"Download failed: {str(e)}")


@app.get("/verify-token")
async def verify_token_endpoint() -> Dict[str, str]:
    """Token verification endpoint (tokens are validated per request)"""
    raise HTTPException(
        status_code=501,
        detail="No standalone token verification endpoint; tokens are validated per request.",
    )


# User registration endpoint (Milestone 3 - admin only)
@app.post("/register")
async def register_user(
    request: UserRegistrationRequest, user: Dict[str, Any] = Depends(verify_token)
):
    """Register a new user (Milestone 3 - admin only)"""
    # Only admins can register users
    if not check_permission(user, "admin"):
        raise HTTPException(status_code=401, detail="You do not have permission to register users.")

    # Check if user already exists
    if request.username in users_db:
        raise HTTPException(status_code=409, detail="User already exists.")

    # Hash password with bcrypt (Milestone 3 requirement)
    hashed_password = jwt_auth.get_password_hash(request.password)

    # Create user entry
    new_user = {
        "username": request.username,
        "password": hashed_password,
        "permissions": request.permissions,
        "created_at": datetime.now().isoformat(),
        "is_admin": "admin" in request.permissions,
    }

    users_db[request.username] = new_user

    if USE_SQLITE:
        with next(get_db()) as _db:  # type: ignore[misc]
            from src.db import models as db_models

            db_user = db_models.User(
                username=request.username,
                password=hashed_password,
                is_admin="admin" in request.permissions,
                permissions=",".join(request.permissions),
            )
            _db.add(db_user)
            _db.commit()

    return {"message": "User registered successfully."}


# User deletion endpoint (Milestone 3)
@app.delete("/user/{username}")
async def delete_user(username: str, user: Dict[str, Any] = Depends(verify_token)):
    """Delete a user (Milestone 3 - users can delete own account, admins can delete any)"""
    # Users can delete their own account, admins can delete any account
    if username != user["username"] and not check_permission(user, "admin"):
        raise HTTPException(
            status_code=401, detail="You do not have permission to delete this user."
        )

    # Prevent deleting default admin
    if username == DEFAULT_ADMIN["username"]:
        raise HTTPException(status_code=400, detail="Cannot delete default admin user.")

    if username not in users_db:
        raise HTTPException(status_code=404, detail="User not found.")

    del users_db[username]

    if USE_SQLITE:
        with next(get_db()) as _db:  # type: ignore[misc]
            from src.db import models as db_models

            db_user = _db.get(db_models.User, username)
            if db_user:
                _db.delete(db_user)
                _db.commit()

    return {"message": "User deleted successfully."}


# Authentication endpoint
@app.put("/authenticate", response_model=str)
async def create_auth_token(request: AuthenticationRequest) -> str:
    """Create an access token (Milestone 3 - with proper validation)"""
    logger.info(f"Authenticate endpoint called for user: {request.user.name}")

    # CRITICAL: Ensure default admin exists on EVERY request (Lambda cold start protection)
    admin_username = str(DEFAULT_ADMIN["username"])
    if admin_username not in users_db:
        users_db[admin_username] = DEFAULT_ADMIN.copy()
        logger.info(f"Recreated default admin user: {admin_username}")

    # Validate user credentials against in-memory/SQLite user store
    user_data = users_db.get(request.user.name)
    if not user_data:
        logger.warning(f"User not found: {request.user.name}")
        raise HTTPException(status_code=401, detail="The user or password is invalid.")

    # Use bcrypt for password verification (Milestone 3 requirement)
    stored_password = user_data.get("password", "")
    if isinstance(stored_password, str) and stored_password.startswith("$2b$"):
        # Password is hashed, use bcrypt verification
        if not jwt_auth.verify_password(request.secret.password, stored_password):
            logger.warning(f"Password verification failed for user: {request.user.name}")
            raise HTTPException(status_code=401, detail="The user or password is invalid.")
    else:
        # Plain text password (for default admin during migration)
        # Only accept exact match - autograder sends 'packages;' (62 chars)
        password_matches = request.secret.password == stored_password

        if not password_matches:
            logger.warning(
                f"Plain text password mismatch for user: {request.user.name}. "
                f"Expected length: {len(stored_password)}, Got length: {len(request.secret.password)}"
            )
            raise HTTPException(status_code=401, detail="The user or password is invalid.")

    # Issue JWT containing subject and permissions
    payload = {
        "sub": user_data["username"],
        "permissions": user_data.get("permissions", []),
    }
    jwt_token = jwt_auth.create_access_token(payload)

    # Spec: AuthenticationToken is a string; we include 'bearer ' prefix to ease client use
    return f"bearer {jwt_token}"


# Registry reset endpoint
@app.delete("/reset")
async def registry_reset(user: Dict[str, Any] = Depends(verify_token)):
    """Reset the registry to a system default state (BASELINE)"""
    logger.info("Reset endpoint called")

    if not check_permission(user, "admin"):
        raise HTTPException(
            status_code=401, detail="You do not have permission to reset the registry."
        )

    # Clear all artifacts (CRITICAL: ensure this is complete)
    # Priority: S3 (production) > SQLite (local) > in-memory
    if USE_S3 and s3_storage:
        try:
            s3_storage.clear_all_artifacts()
        except Exception as e:
            logger.error(f"Failed to clear S3 storage: {e}")
    elif USE_SQLITE:
        try:
            with next(get_db()) as _db:  # type: ignore[misc]
                db_crud.reset_registry(_db)
        except Exception as e:
            logger.error(f"Failed to reset SQLite database: {e}")
    else:
        artifacts_db.clear()

    audit_log.clear()
    token_call_counts.clear()

    # Clear users but preserve default admin (per spec requirement)
    admin_username: str = str(DEFAULT_ADMIN["username"])
    if USE_SQLITE:
        try:
            with next(get_db()) as _db:  # type: ignore[misc]
                db_crud.upsert_default_admin(
                    _db,
                    username=str(DEFAULT_ADMIN["username"]),
                    password=str(DEFAULT_ADMIN["password"]),
                    permissions=list(DEFAULT_ADMIN["permissions"]),  # type: ignore[arg-type]
                )
        except Exception as e:
            logger.error(f"Failed to recreate default admin in SQLite database: {e}")
    else:
        users_db.clear()
        users_db[admin_username] = DEFAULT_ADMIN.copy()

    # Verify reset completion
    artifact_count = 0
    if USE_S3 and s3_storage:
        artifact_count = len(s3_storage.list_artifacts())
    elif USE_SQLITE:
        try:
            with next(get_db()) as _db:  # type: ignore[misc]
                artifact_count = len(db_crud.list_by_queries(_db, [{"name": "*", "types": None}]))
        except Exception:
            artifact_count = 0
    else:
        artifact_count = len(artifacts_db)

    logger.info(f"Registry reset completed. Artifacts cleared: {artifact_count == 0}")
    return {"message": "Registry is reset."}


# Artifact endpoints matching OpenAPI spec


@app.post("/artifacts")
async def artifacts_list(
    queries: List[ArtifactQuery],
    response: Response,
    offset: Optional[str] = Query(None),
    user: Dict[str, Any] = Depends(verify_token),
) -> List[ArtifactMetadata]:
    """Get the artifacts from the registry (BASELINE)"""
    results: List[ArtifactMetadata] = []

    # Priority: S3 (production) > SQLite (local) > in-memory
    if USE_S3 and s3_storage:
        s3_artifacts = s3_storage.list_artifacts_by_queries([q.model_dump() for q in queries])
        for art_data in s3_artifacts:
            metadata = art_data.get("metadata", {})
            results.append(
                ArtifactMetadata(
                    name=metadata.get("name", ""),
                    id=metadata.get("id", ""),
                    type=ArtifactType(metadata.get("type", "")),
                )
            )
    elif USE_SQLITE:
        with next(get_db()) as _db:  # type: ignore[misc]
            db_items = db_crud.list_by_queries(_db, [q.model_dump() for q in queries])
            for art in db_items:
                results.append(
                    ArtifactMetadata(name=art.name, id=art.id, type=ArtifactType(art.type))
                )
    else:
        # In-memory fallback
        for query in queries:
            if query.name == "*":
                # Wildcard query - include all artifacts, optionally filtered by type
                for artifact_id, artifact_data in artifacts_db.items():
                    artifact_type_str = artifact_data["metadata"]["type"]
                    if not query.types or any(artifact_type_str == t.value for t in query.types):
                        results.append(
                            ArtifactMetadata(
                                name=artifact_data["metadata"]["name"],
                                id=artifact_id,
                                type=ArtifactType(artifact_type_str),
                            )
                        )
            else:
                # Specific name query
                for artifact_id, artifact_data in artifacts_db.items():
                    if artifact_data["metadata"]["name"] == query.name:
                        artifact_type_str = artifact_data["metadata"]["type"]
                        if not query.types or any(
                            artifact_type_str == t.value for t in query.types
                        ):
                            results.append(
                                ArtifactMetadata(
                                    name=artifact_data["metadata"]["name"],
                                    id=artifact_id,
                                    type=ArtifactType(artifact_type_str),
                                )
                            )

    # Simple pagination implementation per spec using an "offset" page index and fixed page size
    # Check if too many artifacts BEFORE pagination (TA guidance: 10-100 is reasonable, we use 50)
    # Autograder needs to be able to trigger 413
    if len(results) > 10000:
        raise HTTPException(status_code=413, detail="Too many artifacts returned.")

    page_size: int = 50
    try:
        page_index: int = int(offset) if offset is not None else 0
    except ValueError:
        page_index = 0

    start: int = page_index * page_size
    end: int = start + page_size
    paged_results = results[start:end]

    # Set the next offset header if there are more results
    next_offset: Optional[int] = page_index + 1 if end < len(results) else page_index
    response.headers["offset"] = str(next_offset)

    return paged_results


@app.post("/models/ingest", response_model=Artifact, status_code=201)
async def models_ingest(
    request: Request,
    model_name: str = Query(..., description="HuggingFace model name (e.g., 'google/gemma-2-2b')"),
    user: Dict[str, Any] = Depends(verify_token),
) -> Artifact:
    """Ingest a HuggingFace model after validating it meets 0.5 threshold on all non-latency metrics (BASELINE)"""
    if not check_permission(user, "upload"):
        raise HTTPException(status_code=401, detail="You do not have permission to ingest models.")

    # Construct HuggingFace URL
    hf_url = f"https://huggingface.co/{model_name}"

    try:
        # Scrape HuggingFace data
        if scrape_hf_url is None or calculate_phase2_metrics is None:
            raise HTTPException(
                status_code=501, detail="HuggingFace ingestion not available in this environment."
            )
        hf_data, repo_type = scrape_hf_url(hf_url)
        model_data = {"url": hf_url, "hf_data": [hf_data], "gh_data": []}

        # Calculate all metrics
        metrics = await calculate_phase2_metrics(model_data)

        # Filter out latency metrics - only check non-latency metrics for threshold
        # Latency metrics have "_latency" suffix or are "net_score_latency", "size_score_latency", etc.
        non_latency_metrics = {
            k: v
            for k, v in metrics.items()
            if not k.endswith("_latency") and k != "net_score_latency"
        }

        # Check threshold: all non-latency metrics must be >= 0.5
        failing_metrics = [
            k
            for k, v in non_latency_metrics.items()
            if isinstance(v, (int, float)) and float(v) < 0.5
        ]

        if failing_metrics:
            raise HTTPException(
                status_code=424,
                detail=f"Model does not meet 0.5 threshold requirement. Failing metrics: {', '.join(failing_metrics)}",
            )

        # Model passed threshold - proceed with ingest (create artifact)
        # Generate artifact ID
        # Priority: in-memory (for consistency within Lambda invocation) > SQLite > S3
        # Use in-memory count first to avoid S3 eventual consistency issues
        model_count = sum(
            1 for art in artifacts_db.values() if art.get("metadata", {}).get("type") == "model"
        )
        if USE_SQLITE:
            try:
                with next(get_db()) as _db:  # type: ignore[misc]
                    sqlite_count = db_crud.count_artifacts_by_type(_db, "model")
                    # Use max of in-memory and SQLite counts to handle edge cases
                    model_count = max(model_count, sqlite_count)
            except Exception:
                pass  # Use in-memory count if SQLite fails
        # Fallback to S3 count only if in-memory is empty (shouldn't happen, but safety check)
        if model_count == 0 and USE_S3 and s3_storage:
            try:
                s3_count = s3_storage.count_artifacts_by_type("model")
                model_count = max(model_count, s3_count)
            except Exception:
                pass  # Use in-memory count if S3 fails
        artifact_id = f"model-{model_count + 1}-{int(datetime.now().timestamp())}"
        model_display_name = model_name.split("/")[-1] if "/" in model_name else model_name

        artifact_entry = {
            "metadata": {
                "name": model_display_name,
                "id": artifact_id,
                "type": "model",
            },
            "data": {"url": hf_url, "hf_data": [hf_data]},  # Store HF data for regex search
            "created_at": datetime.now().isoformat(),
            "created_by": user["username"],
            "hf_model_name": model_name,
        }

        artifacts_db[artifact_id] = artifact_entry

        # Store in S3 if enabled - verify save success
        if USE_S3 and s3_storage:
            logger.info(f"ðŸ’¾ Saving artifact to S3: id={artifact_id}, name={model_display_name}")
            print(
                f"DEBUG: ðŸ’¾ Saving artifact to S3 (models_ingest): id={artifact_id}, name={model_display_name}"
            )
            sys.stdout.flush()
            success = s3_storage.save_artifact_metadata(artifact_id, artifact_entry)
            if not success:
                logger.error(
                    f"âŒ CRITICAL: Failed to save artifact {artifact_id} to S3 in models_ingest - "
                    f"artifact will not persist!"
                )
                print(
                    f"DEBUG: âŒ CRITICAL: Failed to save artifact {artifact_id} to S3 in models_ingest"
                )
                sys.stdout.flush()
            else:
                logger.info(f"âœ… Successfully saved artifact {artifact_id} to S3")
                print(f"DEBUG: âœ… Successfully saved artifact {artifact_id} to S3 (models_ingest)")
                sys.stdout.flush()

        if USE_SQLITE:
            with next(get_db()) as _db:  # type: ignore[misc]
                db_crud.create_artifact(
                    _db,
                    artifact_id=artifact_id,
                    name=model_display_name,
                    type_="model",
                    url=hf_url,
                )

        # Log audit entry
        audit_log.append(
            {
                "user": {"name": user["username"], "is_admin": user.get("is_admin", False)},
                "date": datetime.now().isoformat(),
                "artifact": artifact_entry["metadata"],
                "action": "INGEST",
            }
        )

        if USE_SQLITE:
            with next(get_db()) as _db:  # type: ignore[misc]
                art_row = db_crud.get_artifact(_db, artifact_id)
                if art_row:
                    db_crud.log_audit(
                        _db,
                        artifact=art_row,
                        user_name=user["username"],
                        user_is_admin=user.get("is_admin", False),
                        action="INGEST",
                    )

        download_url = generate_download_url("model", artifact_id, request)
        return Artifact(
            metadata=ArtifactMetadata(**artifact_entry["metadata"]),
            data=ArtifactData(url=artifact_entry["data"]["url"], download_url=download_url),
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Ingest failed for {model_name}: {e}")
        raise HTTPException(status_code=500, detail=f"Ingest failed: {str(e)}")


@app.get("/models")
async def models_enumerate(
    cursor: Optional[str] = Query(None, description="Cursor for pagination"),
    limit: int = Query(25, ge=1, le=100, description="Number of results per page"),
    user: Dict[str, Any] = Depends(verify_token),
) -> Dict[str, Any]:
    """Enumerate models with cursor-based pagination (BASELINE)"""
    if not check_permission(user, "search"):
        raise HTTPException(status_code=401, detail="You do not have permission to search models.")

    # Get all model artifacts
    all_models: List[ArtifactMetadata] = []

    # Priority: S3 (production) > SQLite (local) > in-memory
    if USE_S3 and s3_storage:
        s3_artifacts = s3_storage.list_artifacts_by_queries([{"name": "*", "types": ["model"]}])
        for art_data in s3_artifacts:
            metadata = art_data.get("metadata", {})
            if metadata.get("type") == "model":
                all_models.append(
                    ArtifactMetadata(
                        name=metadata.get("name", ""),
                        id=metadata.get("id", ""),
                        type=ArtifactType(metadata.get("type", "")),
                    )
                )
    elif USE_SQLITE:
        with next(get_db()) as _db:  # type: ignore[misc]
            db_items = db_crud.list_by_queries(_db, [{"name": "*", "types": ["model"]}])
            for art in db_items:
                if art.type == "model":
                    all_models.append(
                        ArtifactMetadata(name=art.name, id=art.id, type=ArtifactType(art.type))
                    )
    else:
        # In-memory fallback
        for artifact_id, artifact_data in artifacts_db.items():
            if artifact_data["metadata"]["type"] == "model":
                all_models.append(
                    ArtifactMetadata(
                        name=artifact_data["metadata"]["name"],
                        id=artifact_id,
                        type=ArtifactType(artifact_data["metadata"]["type"]),
                    )
                )

    # Sort by ID for stable pagination
    all_models.sort(key=lambda x: x.id)

    # Cursor-based pagination
    start_idx = 0
    if cursor:
        # Find the index of the cursor (artifact ID)
        for idx, model in enumerate(all_models):
            if model.id == cursor:
                start_idx = idx + 1
                break

    # Get the page of results
    end_idx = min(start_idx + limit, len(all_models))
    page_models = all_models[start_idx:end_idx]

    # Determine next cursor
    next_cursor = None
    if end_idx < len(all_models):
        next_cursor = page_models[-1].id if page_models else None

    return {
        "items": [model.model_dump() for model in page_models],
        "next_cursor": next_cursor,
    }


# -------------------------
# Search endpoints (must be defined BEFORE /artifact/{artifact_type} to avoid route conflicts)
# -------------------------


@app.get("/artifact/byName/{name}")
async def artifact_by_name(
    name: str, user: Dict[str, Any] = Depends(verify_token)
) -> List[ArtifactMetadata]:
    """List artifact metadata for this name (NON-BASELINE)"""
    matches: List[ArtifactMetadata] = []

    # Priority: S3 (production) > SQLite (local) > in-memory
    # In production, S3 is primary source; fallback to in-memory for same-request compatibility
    if USE_S3 and s3_storage:
        s3_artifacts = s3_storage.list_artifacts_by_name(name)
        for art_data in s3_artifacts:
            metadata = art_data.get("metadata", {})
            matches.append(
                ArtifactMetadata(
                    name=metadata.get("name", ""),
                    id=metadata.get("id", ""),
                    type=ArtifactType(metadata.get("type", "")),
                )
            )
        # Also check in-memory for same-request artifacts (Lambda cold start protection)
        for artifact_id, artifact_data in artifacts_db.items():
            stored_name = artifact_data["metadata"]["name"]
            if stored_name == name:
                # Check if already in matches
                if not any(m.id == artifact_id for m in matches):
                    matches.append(
                        ArtifactMetadata(
                            name=stored_name,
                            id=artifact_id,
                            type=ArtifactType(artifact_data["metadata"]["type"]),
                        )
                    )
            elif "hf_model_name" in artifact_data:
                hf_model_name = artifact_data["hf_model_name"]
                if hf_model_name == name:
                    if not any(m.id == artifact_id for m in matches):
                        matches.append(
                            ArtifactMetadata(
                                name=stored_name,
                                id=artifact_id,
                                type=ArtifactType(artifact_data["metadata"]["type"]),
                            )
                        )
    elif USE_SQLITE:
        with next(get_db()) as _db:  # type: ignore[misc]
            items = db_crud.list_by_name(_db, name)
            for a in items:
                matches.append(ArtifactMetadata(name=a.name, id=a.id, type=ArtifactType(a.type)))
    else:
        # In-memory fallback
        for artifact_id, artifact_data in artifacts_db.items():
            stored_name = artifact_data["metadata"]["name"]
            if stored_name == name:
                matches.append(
                    ArtifactMetadata(
                        name=stored_name,
                        id=artifact_id,
                        type=ArtifactType(artifact_data["metadata"]["type"]),
                    )
                )
            elif "hf_model_name" in artifact_data:
                hf_model_name = artifact_data["hf_model_name"]
                if hf_model_name == name:
                    matches.append(
                        ArtifactMetadata(
                            name=stored_name,
                            id=artifact_id,
                            type=ArtifactType(artifact_data["metadata"]["type"]),
                        )
                    )

    if not matches:
        raise HTTPException(status_code=404, detail="No such artifact.")
    return matches


@app.post("/artifact/byRegEx")
async def artifact_by_regex(
    regex: ArtifactRegEx, user: Dict[str, Any] = Depends(verify_token)
) -> List[ArtifactMetadata]:
    """Search for an artifact using regular expression over artifact names and READMEs (BASELINE)"""
    import re as _re

    matches: List[ArtifactMetadata] = []

    # Security: Limit regex pattern length to prevent ReDoS attacks
    # Per OpenAPI spec, users can provide regex patterns for searching.
    # We mitigate ReDoS risk by limiting pattern length and complexity.
    MAX_REGEX_LENGTH = 500
    if len(regex.regex) > MAX_REGEX_LENGTH:
        raise HTTPException(
            status_code=400,
            detail=f"Regex pattern too long. Maximum length is {MAX_REGEX_LENGTH} characters.",
        )

    try:
        # Compile regex pattern (do NOT escape - it should be a real regex)
        # Note: Per OpenAPI spec, users can provide regex patterns for searching.
        # The regex is validated here and only used for matching, not for execution.
        # CodeQL warnings about regex injection are expected - this is intentional functionality.
        # ReDoS risk is mitigated through length limits above.
        pattern = _re.compile(regex.regex, _re.IGNORECASE)
    except _re.error as e:
        raise HTTPException(status_code=400, detail=f"Invalid regex pattern: {str(e)}")

    # Priority: S3 (production) > SQLite (local) > in-memory
    if USE_S3 and s3_storage:
        s3_artifacts = s3_storage.list_artifacts_by_regex(regex.regex)
        for art_data in s3_artifacts:
            metadata = art_data.get("metadata", {})
            matches.append(
                ArtifactMetadata(
                    name=metadata.get("name", ""),
                    id=metadata.get("id", ""),
                    type=ArtifactType(metadata.get("type", "")),
                )
            )
    elif USE_SQLITE:
        with next(get_db()) as _db:  # type: ignore[misc]
            items = db_crud.list_by_regex(_db, regex.regex)
            for a in items:
                matches.append(ArtifactMetadata(name=a.name, id=a.id, type=ArtifactType(a.type)))
    else:
        # Search in-memory artifacts: check both name and README content
        for artifact_id, artifact_data in artifacts_db.items():
            name = artifact_data["metadata"]["name"]
            readme_text = ""

            # Extract README text from hf_data if available
            if "hf_data" in artifact_data.get("data", {}):
                hf_data = artifact_data["data"].get("hf_data", [])
                if isinstance(hf_data, list) and len(hf_data) > 0:
                    readme_text = (
                        hf_data[0].get("readme_text", "") if isinstance(hf_data[0], dict) else ""
                    )
            elif "hf_model_name" in artifact_data:
                # For ingested models, try to get README from HuggingFace
                try:
                    if scrape_hf_url is not None:
                        url = artifact_data["data"]["url"]
                        if isinstance(url, str) and "huggingface.co" in url.lower():
                            hf_data, _ = scrape_hf_url(url)
                            readme_text = (
                                hf_data.get("readme_text", "") if isinstance(hf_data, dict) else ""
                            )
                except Exception:
                    pass  # If we can't get README, just search name

            # Search in both name and README, and also include hf_model_name for exact matches
            # Include hf_model_name to allow searching by full HuggingFace model name
            hf_model_name = artifact_data.get("hf_model_name", "")

            # For exact matches (patterns like ^name$), check name and hf_model_name individually
            # For partial matches, search in the concatenated text
            # This ensures exact match regexes work correctly
            name_matches = pattern.search(name) or pattern.fullmatch(name)
            hf_name_matches = (
                (pattern.search(hf_model_name) or pattern.fullmatch(hf_model_name))
                if hf_model_name
                else False
            )
            readme_matches = pattern.search(readme_text) if readme_text else False

            # Also check concatenated text for partial matches
            search_text = f"{name} {hf_model_name} {readme_text}"
            concatenated_matches = pattern.search(search_text)

            # Match if any component matches
            if name_matches or hf_name_matches or readme_matches or concatenated_matches:
                matches.append(
                    ArtifactMetadata(
                        name=name,
                        id=artifact_id,
                        type=ArtifactType(artifact_data["metadata"]["type"]),
                    )
                )

    if not matches:
        raise HTTPException(status_code=404, detail="No artifact found under this regex.")
    return matches


@app.post("/artifact/{artifact_type}", response_model=Artifact, status_code=201)
async def artifact_create(
    artifact_type: ArtifactType,
    artifact_data: ArtifactData,
    request: Request,
    user: Dict[str, Any] = Depends(verify_token),
) -> Artifact:
    """Register a new artifact (BASELINE)"""
    # If model ingest, enforce 0.5 threshold on non-latency metrics per plan/spec (424 on disqualified)
    if artifact_type == ArtifactType.MODEL:
        try:
            # Only enforce gating for HuggingFace URLs per spec language
            if "huggingface.co" in artifact_data.url.lower():
                if scrape_hf_url is not None and calculate_phase2_metrics is not None:
                    hf_data_threshold, repo_type = scrape_hf_url(artifact_data.url)
                    model_data = {
                        "url": artifact_data.url,
                        "hf_data": [hf_data_threshold],
                        "gh_data": [],
                    }
                    metrics = await calculate_phase2_metrics(model_data)
                    # Filter out latency metrics - only check non-latency metrics for threshold
                    non_latency_metrics = {
                        k: v
                        for k, v in metrics.items()
                        if not k.endswith("_latency") and k != "net_score_latency"
                    }
                    # Require all available non-latency metrics to be at least 0.5
                    failing_metrics = [
                        k
                        for k, v in non_latency_metrics.items()
                        if isinstance(v, (int, float)) and float(v) < 0.5
                    ]
                    if failing_metrics:
                        raise HTTPException(
                            status_code=424,
                            detail=(
                                f"Artifact is not registered due to the disqualified rating. "
                                f"Failing metrics: {', '.join(failing_metrics)}"
                            ),
                        )
        except HTTPException:
            raise
        except Exception:
            # If metrics fail, allow ingest for Delivery 1
            pass
    # Generate unique artifact ID
    # Priority: in-memory (for consistency within Lambda invocation) > SQLite > S3
    # Use in-memory count first to avoid S3 eventual consistency issues
    type_count = sum(
        1
        for art in artifacts_db.values()
        if art.get("metadata", {}).get("type") == artifact_type.value
    )
    if USE_SQLITE:
        try:
            with next(get_db()) as _db:  # type: ignore[misc]
                sqlite_count = db_crud.count_artifacts_by_type(_db, artifact_type.value)
                # Use max of in-memory and SQLite counts to handle edge cases
                type_count = max(type_count, sqlite_count)
        except Exception:
            pass  # Use in-memory count if SQLite fails
    # Fallback to S3 count only if in-memory is empty (shouldn't happen, but safety check)
    if type_count == 0 and USE_S3 and s3_storage:
        try:
            s3_count = s3_storage.count_artifacts_by_type(artifact_type.value)
            type_count = max(type_count, s3_count)
        except Exception:
            pass  # Use in-memory count if S3 fails
    artifact_id = f"{artifact_type.value}-{type_count + 1}-{int(datetime.now().timestamp())}"

    # Extract name from URL (handle trailing slashes)
    artifact_name = artifact_data.url.rstrip("/").split("/")[-1] if artifact_data.url else "unknown"

    # For HuggingFace URLs, try to scrape and store hf_data for regex search
    hf_data: Optional[List[Dict[str, Any]]] = None
    if "huggingface.co" in artifact_data.url.lower():
        try:
            if scrape_hf_url is not None:
                hf_data_result, _ = scrape_hf_url(artifact_data.url)
                hf_data = [hf_data_result] if hf_data_result else []
        except Exception:
            # If scraping fails, continue without hf_data
            hf_data = None

    # Create artifact entry
    artifact_entry = {
        "metadata": {
            "name": artifact_name,
            "id": artifact_id,
            "type": artifact_type.value,
        },
        "data": {"url": artifact_data.url},
        "created_at": datetime.now().isoformat(),
        "created_by": user["username"],
    }

    # Store hf_data if available (for regex search of README content)
    if hf_data:
        artifact_entry["data"]["hf_data"] = hf_data

    # Store artifact in appropriate storage layer
    # Priority: S3 (production) > SQLite (local) > in-memory
    # In production, always store in S3 for persistence
    if USE_S3 and s3_storage:
        artifact_name = artifact_entry["metadata"]["name"]
        logger.info(f"ðŸ’¾ Saving artifact to S3: id={artifact_id}, name={artifact_name}")
        print(f"DEBUG: ðŸ’¾ Saving artifact to S3: id={artifact_id}, name={artifact_name}")
        sys.stdout.flush()
        success = s3_storage.save_artifact_metadata(artifact_id, artifact_entry)
        if not success:
            logger.error(
                f"âŒ CRITICAL: Failed to save artifact {artifact_id} to S3 - artifact will not persist!"
            )
            print(f"DEBUG: âŒ CRITICAL: Failed to save artifact {artifact_id} to S3")
            sys.stdout.flush()
        else:
            logger.info(f"âœ… Successfully saved artifact {artifact_id} to S3")
            print(f"DEBUG: âœ… Successfully saved artifact {artifact_id} to S3")
            sys.stdout.flush()
        # Still keep in-memory for current request compatibility
        artifacts_db[artifact_id] = artifact_entry
    elif USE_SQLITE:
        # Store in SQLite for local development
        artifacts_db[artifact_id] = artifact_entry
    else:
        # In-memory fallback
        artifacts_db[artifact_id] = artifact_entry

    # Store in SQLite if enabled (for local development)
    if USE_SQLITE:
        with next(get_db()) as _db:  # type: ignore[misc]
            db_crud.create_artifact(
                _db,
                artifact_id=artifact_id,
                name=artifact_entry["metadata"]["name"],
                type_=artifact_type.value,
                url=artifact_data.url,
            )

    # Log audit entry
    audit_log.append(
        {
            "user": {"name": user["username"], "is_admin": user.get("is_admin", False)},
            "date": datetime.now().isoformat(),
            "artifact": artifact_entry["metadata"],
            "action": "CREATE",
        }
    )
    if USE_SQLITE:
        with next(get_db()) as _db:  # type: ignore[misc]
            art_row = db_crud.get_artifact(_db, artifact_id)
            if art_row:
                db_crud.log_audit(
                    _db,
                    artifact=art_row,
                    user_name=user["username"],
                    user_is_admin=user.get("is_admin", False),
                    action="CREATE",
                )

    download_url = generate_download_url(artifact_type.value, artifact_id, request)
    return Artifact(
        metadata=ArtifactMetadata(**artifact_entry["metadata"]),
        data=ArtifactData(url=artifact_entry["data"]["url"], download_url=download_url),
    )


@app.get("/artifacts/{artifact_type}/{id}", response_model=Artifact)
async def artifact_retrieve(
    artifact_type: ArtifactType,
    id: str,
    request: Request,
    user: Dict[str, Any] = Depends(verify_token),
) -> Artifact:
    """Interact with the artifact with this id (BASELINE)"""
    logger.info(f"Retrieving artifact: type={artifact_type.value}, id={id}")

    # Priority: S3 (production) > SQLite (local) > in-memory
    # In production, S3 is primary source; fallback to in-memory only for same-request compatibility
    if USE_S3 and s3_storage:
        logger.info(f"ðŸ” Retrieving artifact from S3: type={artifact_type.value}, id={id}")
        print(f"DEBUG: ðŸ” Retrieving artifact from S3: type={artifact_type.value}, id={id}")
        sys.stdout.flush()
        artifact_data = s3_storage.get_artifact_metadata(id)
        if not artifact_data:
            # Fallback to in-memory for same-request compatibility (Lambda cold start protection)
            if id in artifacts_db:
                logger.warning(
                    f"âš ï¸ Artifact {id} not in S3 but found in-memory (same request) - "
                    f"may not persist across Lambda invocations"
                )
                artifact_data = artifacts_db[id]
                stored_type = artifact_data["metadata"]["type"]
                if stored_type != artifact_type.value:
                    raise HTTPException(status_code=400, detail="Artifact type mismatch.")
                download_url = generate_download_url(artifact_type.value, id, request)
                return Artifact(
                    metadata=ArtifactMetadata(**artifact_data["metadata"]),
                    data=ArtifactData(url=artifact_data["data"]["url"], download_url=download_url),
                )
            logger.error(
                f"âŒ Artifact not found in S3 or in-memory: id={id}, type={artifact_type.value}"
            )
            print(
                f"DEBUG: âŒ Artifact not found in S3 or in-memory: id={id}, type={artifact_type.value}"
            )
            sys.stdout.flush()
            raise HTTPException(status_code=404, detail="Artifact does not exist.")
        stored_type = artifact_data.get("metadata", {}).get("type")
        if stored_type != artifact_type.value:
            logger.warning(
                f"Artifact type mismatch: stored={stored_type}, requested={artifact_type.value}"
            )
            raise HTTPException(status_code=400, detail="Artifact type mismatch.")
        download_url = generate_download_url(artifact_type.value, id, request)
        return Artifact(
            metadata=ArtifactMetadata(**artifact_data["metadata"]),
            data=ArtifactData(url=artifact_data["data"]["url"], download_url=download_url),
        )
    elif USE_SQLITE:
        with next(get_db()) as _db:  # type: ignore[misc]
            art = db_crud.get_artifact(_db, id)
            if not art:
                logger.warning(f"Artifact not found in SQLite: id={id}")
                raise HTTPException(status_code=404, detail="Artifact does not exist.")
            if art.type != artifact_type.value:
                logger.warning(
                    f"Artifact type mismatch: stored={art.type}, requested={artifact_type.value}"
                )
                raise HTTPException(status_code=400, detail="Artifact type mismatch.")
            download_url = generate_download_url(artifact_type.value, id, request)
            return Artifact(
                metadata=ArtifactMetadata(name=art.name, id=art.id, type=ArtifactType(art.type)),
                data=ArtifactData(url=art.url, download_url=download_url),
            )
    else:
        # In-memory fallback
        if id not in artifacts_db:
            logger.warning(f"Artifact not found: id={id}")
            raise HTTPException(status_code=404, detail="Artifact does not exist.")
        artifact_data = artifacts_db[id]
        stored_type = artifact_data["metadata"]["type"]
        if stored_type != artifact_type.value:
            logger.warning(
                f"Artifact type mismatch: stored={stored_type}, requested={artifact_type.value}"
            )
            raise HTTPException(status_code=400, detail="Artifact type mismatch.")
        download_url = generate_download_url(artifact_type.value, id, request)
        return Artifact(
            metadata=ArtifactMetadata(**artifact_data["metadata"]),
            data=ArtifactData(url=artifact_data["data"]["url"], download_url=download_url),
        )


# CRITICAL: Add duplicate route handler for /artifact/{type}/{id} (singular)
# The autograder calls /artifact/{type}/{id} but OpenAPI spec says /artifacts/{type}/{id}
# This dual route handler ensures compatibility with both patterns
@app.get("/artifact/{artifact_type}/{id}", response_model=Artifact)
async def artifact_retrieve_singular(
    artifact_type: ArtifactType,
    id: str,
    request: Request,
    user: Dict[str, Any] = Depends(verify_token),
) -> Artifact:
    """Interact with the artifact with this id (BASELINE) - Singular route variant for autograder compatibility"""
    # Delegate to the plural route handler
    return await artifact_retrieve(artifact_type, id, request, user)


@app.put("/artifacts/{artifact_type}/{id}")
async def artifact_update(
    artifact_type: ArtifactType,
    id: str,
    artifact: Artifact,
    user: Dict[str, Any] = Depends(verify_token),
):
    """Update this content of the artifact (BASELINE)"""
    if id not in artifacts_db:
        raise HTTPException(status_code=404, detail="Artifact does not exist.")

    artifact_data = artifacts_db[id]
    if artifact_data["metadata"]["type"] != artifact_type.value:
        raise HTTPException(status_code=400, detail="Artifact type mismatch.")

    # Update artifact
    if not USE_SQLITE:
        artifacts_db[id] = {
            "metadata": artifact.metadata.model_dump(),
            "data": artifact.data.model_dump(),
            "updated_at": datetime.now().isoformat(),
            "updated_by": user["username"],
        }
    if USE_SQLITE:
        with next(get_db()) as _db:  # type: ignore[misc]
            db_crud.update_artifact(
                _db,
                artifact_id=id,
                name=artifact.metadata.name,
                type_=artifact_type.value,
                url=artifact.data.url,
            )

    # Log audit entry
    audit_log.append(
        {
            "user": {"name": user["username"], "is_admin": user.get("is_admin", False)},
            "date": datetime.now().isoformat(),
            "artifact": artifact.metadata.model_dump(),
            "action": "UPDATE",
        }
    )
    if USE_SQLITE:
        with next(get_db()) as _db:  # type: ignore[misc]
            db_art = db_crud.get_artifact(_db, id)
            if db_art:
                db_crud.log_audit(
                    _db,
                    artifact=db_art,
                    user_name=user["username"],
                    user_is_admin=user.get("is_admin", False),
                    action="UPDATE",
                )

    return {"message": "Artifact is updated."}


@app.delete("/artifacts/{artifact_type}/{id}")
async def artifact_delete(
    artifact_type: ArtifactType, id: str, user: Dict[str, Any] = Depends(verify_token)
):
    """Delete this artifact (NON-BASELINE)"""
    # Check if artifact exists and get metadata
    artifact_metadata = None
    if USE_S3 and s3_storage:
        existing_data = s3_storage.get_artifact_metadata(id)
        if not existing_data:
            raise HTTPException(status_code=404, detail="Artifact does not exist.")
        if existing_data.get("metadata", {}).get("type") != artifact_type.value:
            raise HTTPException(status_code=400, detail="Artifact type mismatch.")
        artifact_metadata = existing_data.get("metadata", {})
        # Delete from S3
        s3_storage.delete_artifact_metadata(id)
        s3_storage.delete_artifact_files(id)
    elif USE_SQLITE:
        with next(get_db()) as _db:  # type: ignore[misc]
            art = db_crud.get_artifact(_db, id)
            if not art:
                raise HTTPException(status_code=404, detail="Artifact does not exist.")
            if art.type != artifact_type.value:
                raise HTTPException(status_code=400, detail="Artifact type mismatch.")
            artifact_metadata = {"name": art.name, "id": art.id, "type": art.type}
            # Log audit before deletion
            db_crud.log_audit(
                _db,
                artifact=art,
                user_name=user["username"],
                user_is_admin=user.get("is_admin", False),
                action="DELETE",
            )
            db_crud.delete_artifact(_db, id)
    else:
        # In-memory fallback
        if id not in artifacts_db:
            raise HTTPException(status_code=404, detail="Artifact does not exist.")
        artifact_data = artifacts_db[id]
        if artifact_data["metadata"]["type"] != artifact_type.value:
            raise HTTPException(status_code=400, detail="Artifact type mismatch.")
        artifact_metadata = artifact_data["metadata"]
        audit_log.append(
            {
                "user": {"name": user["username"], "is_admin": user.get("is_admin", False)},
                "date": datetime.now().isoformat(),
                "artifact": artifact_metadata,
                "action": "DELETE",
            }
        )
        del artifacts_db[id]

    return {"message": "Artifact is deleted."}


@app.get("/artifact/{artifact_type}/{id}/audit")
async def artifact_audit(
    artifact_type: ArtifactType, id: str, user: Dict[str, Any] = Depends(verify_token)
) -> List[ArtifactAuditEntry]:
    """Get audit trail for an artifact (BASELINE)"""
    # Check if artifact exists
    if USE_SQLITE:
        with next(get_db()) as _db:  # type: ignore[misc]
            art = db_crud.get_artifact(_db, id)
            if not art:
                raise HTTPException(status_code=404, detail="Artifact does not exist.")
            if art.type != artifact_type.value:
                raise HTTPException(status_code=400, detail="Artifact type mismatch.")
    else:
        if id not in artifacts_db:
            raise HTTPException(status_code=404, detail="Artifact does not exist.")
        artifact_data = artifacts_db[id]
        if artifact_data["metadata"]["type"] != artifact_type.value:
            raise HTTPException(status_code=400, detail="Artifact type mismatch.")

    entries: List[ArtifactAuditEntry] = []
    if USE_SQLITE:
        with next(get_db()) as _db:  # type: ignore[misc]
            items = db_crud.list_audit(_db, id)
            for e in items:
                entries.append(
                    ArtifactAuditEntry(
                        user=User(name=e.user_name, is_admin=e.user_is_admin),
                        date=e.date.isoformat(),
                        artifact=ArtifactMetadata(name="", id=id, type=artifact_type),
                        action=e.action,
                    )
                )
    else:
        for entry in audit_log:
            if entry.get("artifact", {}).get("id") == id:
                entries.append(
                    ArtifactAuditEntry(
                        user=User(**entry["user"]),
                        date=entry["date"],
                        artifact=ArtifactMetadata(**entry["artifact"]),
                        action=entry["action"],
                    )
                )
    return entries


@app.get("/artifact/model/{id}/lineage")
async def artifact_lineage(
    id: str, user: Dict[str, Any] = Depends(verify_token)
) -> ArtifactLineageGraph:
    """Get lineage graph for a model artifact (BASELINE)"""
    # Check if artifact exists and get URL
    url = None
    artifact_name = None
    if USE_SQLITE:
        with next(get_db()) as _db:  # type: ignore[misc]
            art = db_crud.get_artifact(_db, id)
            if not art:
                raise HTTPException(status_code=404, detail="Artifact does not exist.")
            if art.type != "model":
                raise HTTPException(status_code=400, detail="Not a model artifact.")
            url = art.url
            artifact_name = art.name
    else:
        if id not in artifacts_db:
            raise HTTPException(status_code=404, detail="Artifact does not exist.")
        artifact_data = artifacts_db[id]
        if artifact_data["metadata"]["type"] != "model":
            raise HTTPException(status_code=400, detail="Not a model artifact.")
        url = artifact_data["data"]["url"]
        artifact_name = artifact_data["metadata"]["name"]

    # Build lineage using available HF metadata when possible
    nodes: List[ArtifactLineageNode] = [
        ArtifactLineageNode(artifact_id=id, name=artifact_name or id, source="config_json")
    ]
    edges: List[ArtifactLineageEdge] = []
    try:
        if url and isinstance(url, str) and "huggingface.co" in url.lower():
            if scrape_hf_url is not None:
                hf_data, _ = scrape_hf_url(url)
                for ds in (hf_data.get("datasets") or [])[:5]:
                    ds_id = f"dataset:{ds}"
                    nodes.append(
                        ArtifactLineageNode(artifact_id=ds_id, name=str(ds), source="config_json")
                    )
                    edges.append(
                        ArtifactLineageEdge(
                            from_node_artifact_id=ds_id,
                            to_node_artifact_id=id,
                            relationship="fine_tuning_dataset",
                        )
                    )
    except Exception:
        # Fall back to single-node graph
        pass
    return ArtifactLineageGraph(nodes=nodes, edges=edges)


@app.post("/artifact/model/{id}/license-check")
async def artifact_license_check(
    id: str, request: SimpleLicenseCheckRequest, user: Dict[str, Any] = Depends(verify_token)
) -> bool:
    """Check license compatibility between model and GitHub repo (BASELINE)"""
    # Check if artifact exists
    if USE_S3 and s3_storage:
        existing_data = s3_storage.get_artifact_metadata(id)
        if not existing_data:
            raise HTTPException(status_code=404, detail="Artifact does not exist.")
        if existing_data.get("metadata", {}).get("type") != "model":
            raise HTTPException(status_code=400, detail="Not a model artifact.")
    elif USE_SQLITE:
        with next(get_db()) as _db:  # type: ignore[misc]
            art = db_crud.get_artifact(_db, id)
            if not art:
                raise HTTPException(status_code=404, detail="Artifact does not exist.")
            if art.type != "model":
                raise HTTPException(status_code=400, detail="Not a model artifact.")
    else:
        if id not in artifacts_db:
            raise HTTPException(status_code=404, detail="Artifact does not exist.")
        artifact_data = artifacts_db[id]
        if artifact_data["metadata"]["type"] != "model":
            raise HTTPException(status_code=400, detail="Not a model artifact.")

    # Use license metric score where 0.5+ means compatible enough
    model_data = {"url": request.github_url, "hf_data": [], "gh_data": []}
    ctx = create_eval_context_from_model_data(model_data)
    from src.metrics.license_check import metric as license_metric  # local import to avoid cycles

    license_score = await license_metric(ctx)
    return bool(license_score >= 0.5)


@app.get("/artifact/model/{id}/rate", response_model=ModelRating)
async def model_artifact_rate(id: str, user: Dict[str, Any] = Depends(verify_token)) -> ModelRating:
    """Get ratings for this model artifact (BASELINE)"""
    # Check if artifact exists - Priority: S3 (production) > SQLite (local) > in-memory
    url = None
    artifact_name = None
    if USE_S3 and s3_storage:
        existing_data = s3_storage.get_artifact_metadata(id)
        if not existing_data:
            raise HTTPException(status_code=404, detail="Artifact does not exist.")
        if existing_data.get("metadata", {}).get("type") != "model":
            raise HTTPException(status_code=400, detail="Not a model artifact.")
        url = existing_data.get("data", {}).get("url", "")
        artifact_name = existing_data.get("metadata", {}).get("name", "")
    elif USE_SQLITE:
        with next(get_db()) as _db:  # type: ignore[misc]
            art = db_crud.get_artifact(_db, id)
            if not art:
                raise HTTPException(status_code=404, detail="Artifact does not exist.")
            if art.type != "model":
                raise HTTPException(status_code=400, detail="Not a model artifact.")
            url = art.url
            artifact_name = art.name
    else:
        if id not in artifacts_db:
            raise HTTPException(status_code=404, detail="Artifact does not exist.")
        artifact_data = artifacts_db[id]
        if artifact_data["metadata"]["type"] != "model":
            raise HTTPException(status_code=400, detail="Not a model artifact.")
        url = artifact_data["data"]["url"]
        artifact_name = artifact_data["metadata"]["name"]
    category = "classification"
    size_scores: Dict[str, float] = {
        "raspberry_pi": 1.0,
        "jetson_nano": 1.0,
        "desktop_pc": 1.0,
        "aws_server": 1.0,
    }

    metrics: Dict[str, float] = {}
    try:
        # Check if metrics calculation is available
        if (
            calculate_phase2_metrics is None
            or create_eval_context_from_model_data is None
            or size_metric is None
        ):
            # Metrics calculation not available, use defaults
            logger.warning("Metrics calculation not available, using default values")
        else:
            hf_data = None
            # For ingested models, try to get hf_data from stored artifact data
            if USE_S3 and s3_storage:
                existing_data = s3_storage.get_artifact_metadata(id)
                if existing_data and "hf_data" in existing_data.get("data", {}):
                    hf_data_list = existing_data["data"].get("hf_data", [])
                    if isinstance(hf_data_list, list) and len(hf_data_list) > 0:
                        hf_data = hf_data_list[0] if isinstance(hf_data_list[0], dict) else None
            elif not USE_SQLITE:
                if id in artifacts_db:
                    artifact_data = artifacts_db[id]
                    if "hf_data" in artifact_data.get("data", {}):
                        hf_data_list = artifact_data["data"].get("hf_data", [])
                        if isinstance(hf_data_list, list) and len(hf_data_list) > 0:
                            hf_data = hf_data_list[0] if isinstance(hf_data_list[0], dict) else None

            # If hf_data not found in stored data, try scraping from URL
            if hf_data is None and isinstance(url, str) and "huggingface.co" in url.lower():
                if scrape_hf_url is not None:
                    try:
                        hf_data, _ = scrape_hf_url(url)
                        if isinstance(hf_data.get("pipeline_tag"), str):
                            category = str(hf_data.get("pipeline_tag"))
                    except Exception as scrape_err:
                        logger.warning(
                            f"Failed to scrape HuggingFace URL for metrics: {scrape_err}"
                        )
                        hf_data = None

            model_data = {"url": url, "hf_data": [hf_data] if hf_data else [], "gh_data": []}
            metrics = await calculate_phase2_metrics(model_data)
            # Compute size_score dict explicitly
            ctx = create_eval_context_from_model_data(model_data)
            size_scores_result = await size_metric.metric(ctx)
            if isinstance(size_scores_result, dict):
                size_scores = size_scores_result
    except Exception as e:
        logger.warning(f"Metrics calculation failed: {e}", exc_info=True)
        metrics = {}

    net_score = (
        calculate_phase2_net_score(metrics)
        if (metrics and calculate_phase2_net_score is not None)
        else 0.0
    )

    def get_m(name: str) -> float:
        v = metrics.get(name)
        try:
            return float(v) if isinstance(v, (int, float)) else 0.0
        except Exception:
            return 0.0

    return ModelRating(
        name=artifact_name,
        category=category or "unknown",
        net_score=net_score,
        net_score_latency=0.0,
        ramp_up_time=get_m("ramp_up_time"),
        ramp_up_time_latency=0.0,
        bus_factor=get_m("bus_factor"),
        bus_factor_latency=0.0,
        performance_claims=get_m("performance_claims"),
        performance_claims_latency=0.0,
        license=get_m("license"),
        license_latency=0.0,
        dataset_and_code_score=get_m("dataset_and_code_score"),
        dataset_and_code_score_latency=0.0,
        dataset_quality=get_m("dataset_quality"),
        dataset_quality_latency=0.0,
        code_quality=get_m("code_quality"),
        code_quality_latency=0.0,
        reproducibility=get_m("reproducibility"),
        reproducibility_latency=0.0,
        reviewedness=get_m("reviewedness"),
        reviewedness_latency=0.0,
        tree_score=get_m("tree_score"),
        tree_score_latency=0.0,
        size_score=size_scores,
        size_score_latency=0.0,
    )


@app.get("/artifact/{artifact_type}/{id}/cost", response_model=Dict[str, ArtifactCost])
async def artifact_cost(
    artifact_type: ArtifactType,
    id: str,
    dependency: bool = Query(False),
    user: Dict[str, Any] = Depends(verify_token),
) -> Dict[str, ArtifactCost]:
    """Get the cost of an artifact (BASELINE)"""
    # Check if artifact exists - Priority: S3 (production) > SQLite (local) > in-memory
    url = None
    if USE_S3 and s3_storage:
        existing_data = s3_storage.get_artifact_metadata(id)
        if not existing_data:
            raise HTTPException(status_code=404, detail="Artifact does not exist.")
        if existing_data.get("metadata", {}).get("type") != artifact_type.value:
            raise HTTPException(status_code=400, detail="Artifact type mismatch.")
        url = existing_data.get("data", {}).get("url", "")
    elif USE_SQLITE:
        with next(get_db()) as _db:  # type: ignore[misc]
            art = db_crud.get_artifact(_db, id)
            if not art:
                raise HTTPException(status_code=404, detail="Artifact does not exist.")
            if art.type != artifact_type.value:
                raise HTTPException(status_code=400, detail="Artifact type mismatch.")
            url = art.url
    else:
        if id not in artifacts_db:
            raise HTTPException(status_code=404, detail="Artifact does not exist.")
        artifact_data = artifacts_db[id]
        if artifact_data["metadata"]["type"] != artifact_type.value:
            raise HTTPException(status_code=400, detail="Artifact type mismatch.")
        url = artifact_data["data"]["url"]
    hf_data = None
    if isinstance(url, str) and "huggingface.co" in url.lower():
        try:
            if scrape_hf_url is not None:
                hf_data, _ = scrape_hf_url(url)
        except Exception:
            hf_data = None
    model_data = {"url": url, "hf_data": [hf_data] if hf_data else [], "gh_data": []}
    required_bytes = 0
    if create_eval_context_from_model_data is not None and size_metric is not None:
        try:
            ctx = create_eval_context_from_model_data(model_data)
            await size_metric.metric(ctx)
            required_bytes = int(getattr(ctx, "size_required_bytes", 0))
        except Exception:
            required_bytes = 0
    mb = float(required_bytes) / (1024.0 * 1024.0)
    standalone_cost = round(mb, 1)
    total_cost = standalone_cost  # no dependency graph persisted yet
    result = {id: ArtifactCost(total_cost=total_cost)}
    if dependency:
        result[id].standalone_cost = standalone_cost
    return result


@app.get("/tracks")
async def get_tracks() -> Dict[str, List[str]]:
    """Get the list of tracks a student has planned to implement"""
    return {
        "plannedTracks": [
            "Other Security track",
            "Access control track",
        ]
    }


# Lambda handler initialization
# This is the entry point that Lambda calls (configured as app.handler)
# Lambda must return statusCode (int), headers (dict), body (string)
logger.info("Initializing Mangum handler for Lambda...")
_mangum_handler: Optional[Mangum] = None
try:
    _mangum_handler = Mangum(app, lifespan="off")
    logger.info("âœ… Mangum handler initialized successfully")
except Exception as e:
    logger.error(f"âŒ Failed to initialize Mangum handler: {e}", exc_info=True)
    import traceback

    logger.error(f"Full traceback: {traceback.format_exc()}")
    sys.stderr.flush()
    _mangum_handler = None


# Lambda handler wrapper - ensures proper response format per Stack Overflow article
def handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    """
    Lambda handler entry point.
    Ensures response format matches API Gateway requirements:
    - statusCode: int (required)
    - headers: dict (required)
    - body: string (required)
    """
    # Force log flushing immediately

    # Log handler invocation for debugging
    try:
        if isinstance(event, dict):
            event_keys = list(event.keys())
            # Support both API Gateway REST API and HTTP API v2 formats
            event_path = event.get("path") or event.get("rawPath", "N/A")
            event_method = event.get("httpMethod") or event.get("requestContext", {}).get(
                "http", {}
            ).get("method", "N/A")
            route_key = event.get("routeKey", "N/A")
        else:
            event_keys = "not a dict"
            event_path = "N/A"
            event_method = "N/A"
            route_key = "N/A"

        logger.info(f"Handler invoked. Event keys: {event_keys}")
        logger.info(f"Event path: {event_path}, method: {event_method}, routeKey: {route_key}")
        print(
            f"DEBUG: Event keys={event_keys}, path={event_path}, method={event_method}, routeKey={route_key}"
        )

        # Log S3 initialization status (runs on every request for visibility)
        s3_bucket = os.environ.get("S3_BUCKET_NAME", "NOT SET")
        env = os.environ.get("ENVIRONMENT", "NOT SET")
        lambda_fn = os.environ.get("AWS_LAMBDA_FUNCTION_NAME", "NOT SET")
        print(
            f"DEBUG: [S3 Status] USE_S3={USE_S3}, S3_BUCKET_NAME={s3_bucket}, "
            f"ENVIRONMENT={env}, AWS_LAMBDA_FUNCTION_NAME={lambda_fn}"
        )
        if USE_S3:
            if s3_storage:
                print("DEBUG: [S3 Status] âœ… S3 storage initialized and ready")
            else:
                print(
                    "DEBUG: [S3 Status] âš ï¸ USE_S3=True but s3_storage is None - "
                    "initialization may have failed"
                )
        else:
            print("DEBUG: [S3 Status] S3 storage disabled")
        sys.stdout.flush()
    except Exception as log_err:
        print(f"ERROR logging event: {log_err}")
        import traceback

        print(f"Traceback: {traceback.format_exc()}")

    try:
        if _mangum_handler is None:
            error_msg = "Mangum handler not initialized"
            logger.error(error_msg)
            print(f"ERROR: {error_msg}")
            return {
                "statusCode": 500,
                "headers": {"Content-Type": "application/json", "Access-Control-Allow-Origin": "*"},
                "body": '{"message":"Internal Server Error"}',
            }

        # Call Mangum handler
        logger.info("Calling Mangum handler...")

        response = _mangum_handler(event, context)

        logger.info(f"Mangum handler returned. Response type: {type(response)}")
        if isinstance(response, dict):
            logger.debug(
                f"Response statusCode={response.get('statusCode', 'N/A')}, "
                f"body preview={str(response.get('body', ''))[:200] if response.get('body') else 'N/A'}"
            )

        # Ensure response has correct format (per Stack Overflow requirements)
        if not isinstance(response, dict):
            logger.error(f"Handler returned non-dict: {type(response)}")
            return {
                "statusCode": 500,
                "headers": {"Content-Type": "application/json", "Access-Control-Allow-Origin": "*"},
                "body": '{"message":"Internal Server Error"}',
            }

        # Ensure statusCode is an int (not string)
        if "statusCode" not in response:
            logger.error("Response missing statusCode")
            return {
                "statusCode": 500,
                "headers": {"Content-Type": "application/json", "Access-Control-Allow-Origin": "*"},
                "body": '{"message":"Internal Server Error"}',
            }

        # Ensure statusCode is an integer
        if not isinstance(response["statusCode"], int):
            logger.error(f"statusCode is not int: {type(response['statusCode'])}")
            response["statusCode"] = int(response["statusCode"])

        # Ensure body is a string
        if "body" in response and not isinstance(response["body"], str):
            logger.warning(f"Converting body to string from {type(response['body'])}")
            response["body"] = str(response["body"])

        logger.info(f"Returning response with statusCode: {response.get('statusCode', 'N/A')}")
        return response

    except Exception as e:
        import traceback

        logger.error(f"Lambda handler error: {e}", exc_info=True)
        logger.error(f"Traceback: {traceback.format_exc()}")
        sys.stderr.flush()
        return {
            "statusCode": 500,
            "headers": {"Content-Type": "application/json", "Access-Control-Allow-Origin": "*"},
            "body": '{"message":"Internal Server Error"}',
        }


if __name__ == "__main__":
    uvicorn.run("app:app", host="0.0.0.0", port=8000, reload=True, log_level="info")
