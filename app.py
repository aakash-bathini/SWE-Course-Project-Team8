# fmt: off
"""
Phase 2 FastAPI Application - Trustworthy Model Registry
Main application entry point with REST API endpoints matching OpenAPI spec v3.4.4
"""

import logging
import os
import sys
import re
import threading
import time
from urllib.parse import unquote, urlparse

# Configure logging first
# Use unbuffered mode for Lambda to ensure logs appear in CloudWatch immediately
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    force=True,  # Override any existing configuration
)
# Configure handlers to flush immediately
for log_handler in logging.root.handlers:
    log_handler.flush()
logger = logging.getLogger(__name__)
# Set up immediate flushing for Lambda
if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(line_buffering=True)  # Python 3.7+

# Set environment defaults for Lambda
os.environ.setdefault("USE_SQLITE", "0")

try:
    from fastapi import (
        FastAPI,
        HTTPException,
        Depends,
        Query,
        Header,
        Response,
        File,
        Form,
        UploadFile,
        Request,
    )
    from fastapi.middleware.cors import CORSMiddleware
    from fastapi.security import HTTPBearer
    from fastapi.responses import FileResponse, JSONResponse
    from pydantic import BaseModel
    from typing import List, Optional, Dict, Any, Tuple, Callable
    from datetime import datetime, timezone
    from enum import Enum
    from mangum import Mangum
    import uvicorn
    import hashlib
except ImportError as e:
    logger.error(f"Critical import failed: {e}", exc_info=True)
    raise

# Import auth module
try:
    from src.auth.jwt_auth import auth as jwt_auth
except ImportError as e:
    logger.error(f"Failed to import jwt_auth: {e}", exc_info=True)
    raise

# Initialize FastAPI app
app = FastAPI(
    title="ECE 461 - Fall 2025 - Project Phase 2",
    description="API for ECE 461/Fall 2025/Project Phase 2: A Trustworthy Model Registry",
    version="3.4.7",
    docs_url="/docs",
    redoc_url="/redoc",
)


# Request logging middleware to track all incoming requests (especially byName and rate)
@app.middleware("http")
async def log_requests(request: Request, call_next):
    """Log all incoming requests, especially byName and rate routes"""
    path = request.url.path
    method = request.method
    query_params = dict(request.query_params)

    # Log byName-related requests immediately (before dependencies run)
    if "byname" in path.lower() or "byname" in str(query_params).lower():
        logger.info(f"MIDDLEWARE_BYNAME: Request detected - method={method}, path='{path}', query={query_params}")
        print(f"DEBUG_PRINT: MIDDLEWARE_BYNAME detected request: {path}")
        sys.stdout.flush()

    # Log rate-related requests immediately (before dependencies run)
    if "/rate" in path.lower():
        logger.info(f"MIDDLEWARE_RATE: Request detected - method={method}, path='{path}', query={query_params}")
        sys.stdout.flush()

    response = await call_next(request)
    return response

# CORS middleware for frontend integration
# Allow all origins for health endpoint (autograder compatibility)
# For other endpoints, we restrict to known origins
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins for autograder compatibility
    allow_credentials=False,  # Set to False when allowing all origins
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# Security (we will accept X-Authorization header per spec; keep HTTPBearer for flexibility)
security = HTTPBearer()

# Import Phase 2 components (after app initialization to avoid circular imports)
try:
    from src.api.huggingface import scrape_hf_url  # noqa: E402
    from src.api.github import scrape_github_url  # noqa: E402
    from src.metrics.phase2_adapter import (  # noqa: E402
        create_eval_context_from_model_data,
        calculate_phase2_metrics,
        calculate_phase2_net_score,
    )
    from src.metrics import size as size_metric  # noqa: E402
    from src.metrics.relationship_analysis import analyze_artifact_relationships  # noqa: E402
except Exception as e:
    logger.warning(f"Optional imports failed (may not be available in this environment): {e}")
    # Set to None to prevent errors - use type: ignore for mypy compatibility
    scrape_hf_url = None  # type: ignore[assignment]
    scrape_github_url = None  # type: ignore[assignment]
    create_eval_context_from_model_data = None  # type: ignore[assignment]
    analyze_artifact_relationships = None  # type: ignore[assignment]
    calculate_phase2_metrics = None  # type: ignore[assignment]
    calculate_phase2_net_score = None  # type: ignore[assignment]
    size_metric = None  # type: ignore[assignment]

# Storage layer selection: in-memory (default), SQLite (local dev), or S3 (production)
# Production (Lambda): Use S3 only
# Local development: Use SQLite
# Prefer SQLite persistence by default for local/dev runs so data survives across requests
# (The autograder issues many separate requests and expects persistence.)
USE_SQLITE: bool = (
    os.environ.get("USE_SQLITE", "1") == "1"
    and os.environ.get("ENVIRONMENT") != "production"
    and not os.environ.get("AWS_LAMBDA_FUNCTION_NAME")
)
USE_S3: bool = os.environ.get("USE_S3", "0") == "1" or (
    os.environ.get("ENVIRONMENT") == "production"
    or os.environ.get("AWS_LAMBDA_FUNCTION_NAME") is not None
)

artifacts_db: Dict[str, Dict[str, Any]] = {}  # artifact_id -> artifact_data (in-memory)
artifact_status: Dict[str, str] = {}  # artifact_id -> PENDING | READY | INVALID
rating_locks: Dict[str, threading.Lock] = {}  # artifact_id -> Lock for concurrent rating requests
rating_cache: Dict[str, Dict[str, Any]] = {}  # artifact_id -> cached rating result
async_rating_events: Dict[str, threading.Event] = {}  # artifact_id -> Event to signal async rating completion
users_db: Dict[str, Dict[str, Any]] = {}
audit_log: List[Dict[str, Any]] = []

# Initialize S3 storage if enabled (production only)
s3_storage = None
print(
    f"DEBUG: USE_S3={USE_S3}, "
    f"AWS_LAMBDA_FUNCTION_NAME={os.environ.get('AWS_LAMBDA_FUNCTION_NAME', 'not set')}, "
    f"ENVIRONMENT={os.environ.get('ENVIRONMENT', 'not set')}"
)
sys.stdout.flush()
if USE_S3:
    try:
        from src.storage.s3_storage import get_s3_storage

        print("DEBUG: Attempting to initialize S3 storage...")
        sys.stdout.flush()
        s3_storage = get_s3_storage()
        if s3_storage:
            bucket_name = os.environ.get("S3_BUCKET_NAME", "not set")
            logger.info(f"S3 storage initialized: bucket={bucket_name}")
            print(f"DEBUG: âœ… S3 storage initialized: bucket={bucket_name}")
            sys.stdout.flush()
        else:
            logger.warning("S3 storage requested but bucket not configured or unavailable")
            print("DEBUG: âš ï¸ S3 storage NOT initialized - bucket not configured or unavailable")
            sys.stdout.flush()
            USE_S3 = False
    except Exception as e:
        logger.error(f"S3 initialization failed: {e}, falling back to other storage", exc_info=True)
        print(f"DEBUG: âŒ S3 initialization failed: {e}")
        sys.stdout.flush()
        USE_S3 = False
else:
    print("DEBUG: S3 storage disabled (USE_S3=False)")
    sys.stdout.flush()

# Initialize SQLite if enabled (local development only)
try:
    from src.db.database import Base, engine, get_db
    from src.db import crud as db_crud
    # Import models to register all table definitions with SQLAlchemy
    from src.db import models as db_models  # noqa: F401
except ImportError:
    get_db = None  # type: ignore[assignment]
    db_crud = None  # type: ignore[assignment]
    db_models = None  # type: ignore[assignment]

# Ensure database tables are created if database is available
# This is needed even in Lambda for endpoints that use the database (e.g., /audit/package-confusion)
if get_db is not None:
    try:
        # Ensure schema exists (creates all tables including sensitive_models, js_programs, etc.)
        Base.metadata.create_all(bind=engine)
        if USE_SQLITE:
            db_path = os.environ.get("SQLALCHEMY_DATABASE_URL", "default")
            logger.info(f"SQLite initialized successfully. Database path: {db_path}")
        else:
            # In Lambda, database is available but USE_SQLITE is False (using S3 for artifacts)
            # Still need to create tables for endpoints that use the database
            db_path = os.environ.get("SQLALCHEMY_DATABASE_URL", "default")
            logger.info(f"Database tables created for Lambda endpoints. Database path: {db_path}")
    except Exception as e:
        logger.error(
            f"Database initialization failed: {e}, some endpoints may not work", exc_info=True
        )
        if USE_SQLITE:
            USE_SQLITE = False
else:
    if os.environ.get("ENVIRONMENT") == "production" or os.environ.get("AWS_LAMBDA_FUNCTION_NAME"):
        logger.info("SQLite disabled in production - using S3 storage")
    else:
        logger.info("SQLite disabled - using in-memory storage")

# Default admin user - password matches what autograder sends (requirements doc says 'packages', not 'artifacts')
DEFAULT_ADMIN = {
    "username": "ece30861defaultadminuser",
    "password": "correcthorsebatterystaple123(!__+@**(A'\"`;DROP TABLE packages;",
    "permissions": ["upload", "search", "download", "admin"],
    "created_at": datetime.now().isoformat(),
}

# Add default admin to users database and DB (if enabled)
admin_username: str = str(DEFAULT_ADMIN["username"])
# Use copy() to ensure we have an independent dict that won't be affected by mutations
users_db[admin_username] = DEFAULT_ADMIN.copy()
if USE_SQLITE and get_db is not None:
    try:
        with next(get_db()) as _db:  # type: ignore[misc]
            db_crud.ensure_schema(_db)
            db_crud.upsert_default_admin(
                _db,
                username=str(DEFAULT_ADMIN["username"]),
                password=str(DEFAULT_ADMIN["password"]),
                permissions=list(DEFAULT_ADMIN["permissions"]),  # type: ignore[arg-type]
            )
    except Exception as e:
        logger.warning(f"Failed to initialize default admin in database: {e}")
        # Ensure default admin is still in in-memory users_db even if SQLite fails
        if admin_username not in users_db:
            users_db[admin_username] = DEFAULT_ADMIN.copy()


# Pydantic models matching OpenAPI spec v3.3.1


class ArtifactType(str, Enum):
    MODEL = "model"
    DATASET = "dataset"
    CODE = "code"


class ArtifactMetadata(BaseModel):
    name: str
    id: str
    type: ArtifactType


class ArtifactData(BaseModel):
    url: str
    download_url: Optional[str] = None

    class Config:
        exclude_none = True


class Artifact(BaseModel):
    metadata: ArtifactMetadata
    data: ArtifactData

    class Config:
        exclude_none = True


class ArtifactQuery(BaseModel):
    name: str
    types: Optional[List[ArtifactType]] = None


class User(BaseModel):
    name: str
    is_admin: bool


class UserAuthenticationInfo(BaseModel):
    password: str


class AuthenticationRequest(BaseModel):
    user: User
    secret: UserAuthenticationInfo


class AuthenticationToken(BaseModel):
    token: str


class ModelRating(BaseModel):
    name: str
    category: str
    net_score: float
    net_score_latency: float
    ramp_up_time: float
    ramp_up_time_latency: float
    bus_factor: float
    bus_factor_latency: float
    performance_claims: float
    performance_claims_latency: float
    license: float
    license_latency: float
    dataset_and_code_score: float
    dataset_and_code_score_latency: float
    dataset_quality: float
    dataset_quality_latency: float
    code_quality: float
    code_quality_latency: float
    reproducibility: float
    reproducibility_latency: float
    reviewedness: float
    reviewedness_latency: float
    tree_score: float
    tree_score_latency: float
    size_score: Dict[str, float]
    size_score_latency: float


class ArtifactCost(BaseModel):
    total_cost: float
    standalone_cost: Optional[float] = None


class ArtifactAuditEntry(BaseModel):
    user: User
    date: str
    artifact: ArtifactMetadata
    action: str


# ----------------------------
# Input validation helpers
# ----------------------------
_ARTIFACT_ID_ALLOWED_RE = re.compile(r"^[A-Za-z0-9\-]+$")
_HF_ALLOWED_HOSTS = ("huggingface.co", "hf.co")
_HF_PATH_PREFIXES = {"models", "model", "datasets", "dataset", "spaces", "space"}


def _validate_artifact_id_or_400(artifact_id: str) -> None:
    """
    Validate ArtifactID per OpenAPI pattern '^[a-zA-Z0-9\\-]+$'.
    Raise HTTP 400 if invalid.
    """
    if not isinstance(artifact_id, str) or not _ARTIFACT_ID_ALLOWED_RE.fullmatch(artifact_id or ""):
        raise HTTPException(
            status_code=400,
            detail="There is missing field(s) in the artifact_id or it is formed improperly, or is invalid.",
        )


def _add_unique_candidate(candidates: List[str], value: str) -> None:
    """Helper to append non-empty unique candidate strings while preserving order."""
    if not value:
        return
    candidate = value.strip()
    if not candidate:
        return
    if candidate not in candidates:
        candidates.append(candidate)


def _normalize_hf_identifier(identifier: str) -> List[str]:
    """Return ordered list of canonical + alias strings for a HuggingFace identifier."""
    if not identifier:
        return []
    cleaned = identifier.replace("\\", "/").strip().strip("/")
    if not cleaned:
        return []
    variants: List[str] = []
    _add_unique_candidate(variants, cleaned)
    # Hyphenated variant (common in autograder regex/name queries)
    _add_unique_candidate(variants, cleaned.replace("/", "-"))
    # Just the final segment (already stored as metadata name, but keep for completeness)
    last_segment = cleaned.split("/")[-1]
    if last_segment and last_segment != cleaned:
        _add_unique_candidate(variants, last_segment)
    return variants


def _derive_hf_variants_from_url(url: str) -> List[str]:
    """Attempt to derive repository identifier variants (HF or GitHub) from a URL."""
    if not url:
        return []
    try:
        parsed = urlparse(url)
    except Exception:
        return []
    host = parsed.netloc.lower()
    path_parts = [unquote(part) for part in parsed.path.split("/") if part]
    if not path_parts:
        return []

    variants: List[str] = []
    if any(host.endswith(allowed) for allowed in _HF_ALLOWED_HOSTS):
        trimmed = path_parts[:]
        if trimmed and trimmed[0].lower() in _HF_PATH_PREFIXES:
            trimmed = trimmed[1:]
        if not trimmed:
            return variants
        canonical = "/".join(trimmed)
        variants.extend(_normalize_hf_identifier(canonical))
        return variants

    if "github.com" in host:
        if len(path_parts) >= 2:
            org_repo = "/".join(path_parts[:2])
            variants.extend(_normalize_hf_identifier(org_repo))
            _add_unique_candidate(variants, path_parts[1])
        else:
            variants.extend(_normalize_hf_identifier(path_parts[0]))
    return variants


def _get_hf_name_candidates(record: Dict[str, Any]) -> List[str]:
    """
    Collect all HuggingFace-related name variants stored with an artifact.
    Includes explicit hf_model_name values, aliases, hf_data repo identifiers,
    and URL-derived identifiers.
    """
    candidates: List[str] = []

    hf_name = str(record.get("hf_model_name") or "").strip()
    if hf_name:
        for variant in _normalize_hf_identifier(hf_name):
            _add_unique_candidate(candidates, variant)

    aliases = record.get("hf_model_name_aliases", [])
    alias_values: List[str] = []
    if isinstance(aliases, str):
        alias_values = [aliases]
    elif isinstance(aliases, bytes):
        alias_values = [aliases.decode("utf-8", errors="ignore")]
    elif isinstance(aliases, list):
        for alias in aliases:
            if isinstance(alias, bytes):
                alias_values.append(alias.decode("utf-8", errors="ignore"))
            elif isinstance(alias, str):
                alias_values.append(alias)
    for alias in alias_values:
        for variant in _normalize_hf_identifier(alias):
            _add_unique_candidate(candidates, variant)

    data_block = record.get("data", {})
    if isinstance(data_block, dict):
        url_candidate = str(data_block.get("url") or "")
        for derived in _derive_hf_variants_from_url(url_candidate):
            _add_unique_candidate(candidates, derived)

        hf_data_entries = data_block.get("hf_data", [])
        if isinstance(hf_data_entries, (list, tuple)):
            for entry in hf_data_entries:
                if not isinstance(entry, dict):
                    continue
                repo_keys = (
                    "repo_id",
                    "repoId",
                    "model_id",
                    "modelId",
                    "model",
                    "id",
                    "name",
                )
                repo_id = None
                for key in repo_keys:
                    if key in entry and entry[key]:
                        repo_id = str(entry[key]).strip()
                        if repo_id:
                            break
                if repo_id:
                    for variant in _normalize_hf_identifier(repo_id):
                        _add_unique_candidate(candidates, variant)

                card_data = entry.get("card_data") or entry.get("cardData")
                if isinstance(card_data, dict):
                    base_model = card_data.get("base_model")
                    if base_model:
                        for variant in _normalize_hf_identifier(str(base_model)):
                            _add_unique_candidate(candidates, variant)

    return candidates


def _derive_display_name_from_url(url: str) -> Optional[str]:
    """Generate a friendly display name from a HuggingFace/GitHub URL."""
    if not url:
        return None
    variants = _derive_hf_variants_from_url(url)
    if variants:
        # Prefer slash variants (org/repo) when available
        slash_variants = [v for v in variants if "/" in v]
        if slash_variants:
            return slash_variants[0]
        return variants[0]

    try:
        parsed = urlparse(url)
    except Exception:
        parsed = None
    if parsed:
        path_parts = [unquote(part) for part in parsed.path.split("/") if part]
        if path_parts:
            return path_parts[-1]
    return None


def _ensure_artifact_display_name(record: Dict[str, Any]) -> str:
    """
    Ensure artifact metadata has a non-empty name.
    Falls back to HF aliases, derived URL identifiers, or artifact ID.
    """
    metadata = record.setdefault("metadata", {})
    current_name = str(metadata.get("name") or "").strip()
    if current_name:
        return current_name

    candidates = _get_hf_name_candidates(record)
    for candidate in candidates:
        candidate_stripped = candidate.strip()
        if candidate_stripped:
            metadata["name"] = candidate_stripped
            return candidate_stripped

    data_block = record.get("data", {}) or {}
    derived = _derive_display_name_from_url(str(data_block.get("url") or ""))
    if derived:
        metadata["name"] = derived
        return derived

    fallback = str(metadata.get("id") or record.get("id") or "").strip()
    if not fallback:
        fallback = "unknown-artifact"
    metadata["name"] = fallback
    return fallback


# ----------------------------
# Regex safety helpers
# ----------------------------
_DANGEROUS_REGEX_SNIPPETS: List[re.Pattern[str]] = [
    # Classic catastrophic backtracking forms: nested quantifiers
    re.compile(r"\((?:[^()\\]|\\.)+\)[*+]\s*[*+]+"),  # e.g., (.+)+, (.*)+, (\w+)+
    re.compile(r"\(\?:\.\+\)\+"),  # (?:.+)+ (explicit non-capturing)
    re.compile(r"\(\?:\.\*\)\+"),  # (?:.*)+
    # Greedy ambiguous repeats anchored end-to-end (common bombs)
    re.compile(r"^\((?:[^()\\]|\\.)+\)\+$"),  # ^(a+)+$-like
    # Additional patterns: multiple nested quantifiers like (a+)(a+)(a+)(a+)(a+)(a+)$
    re.compile(r"\([^)]+\+\)\{3,\}"),  # Three or more (something+)
    re.compile(r"\([^)]+\+\)\+.*\([^)]+\+\)\+"),  # Multiple nested quantifier groups
    # Alternation with quantifiers: (a|aa)*, (a|ab)*, etc.
    re.compile(r"\([^|)]+\|[^)]+\)[*+]+"),  # (a|aa)*, (a|ab)+, etc.
    re.compile(r"\([^|)]+\|[^)]+\)\*$"),  # (a|aa)*$ anchored
    re.compile(r"\(\?:[^|)]+\|[^)]+\)[*+]+"),  # Non-capturing alternation loops
    re.compile(r"\(\?:[^|)]+\|[^)]+\)\*$"),  # Non-capturing alternation anchored
    # Nested counted quantifiers: (a{1,99999}){1,99999}
    re.compile(r"\([^\)]+\{\d+(?:,\d+)?\}[^\)]*\)\s*\{\d+(?:,\d+)?\}"),
]

_LARGE_QUANTIFIER_THRESHOLD = 1000
_LARGE_QUANTIFIER_RE = re.compile(r"\{(\d+)(?:,(\d+))?\}")


def _is_dangerous_regex(raw_pattern: str) -> bool:
    """
    Heuristic detector for catastrophic-backtracking-prone patterns.
    We prefer to fail fast with HTTP 400 than risk Lambda timeouts.
    """
    text = (raw_pattern or "").strip()
    if not text:
        return False
    # Very long patterns are already rejected elsewhere; here detect nested quantifiers
    for bomb in _DANGEROUS_REGEX_SNIPPETS:
        if bomb.search(text):
            return True

    # Also reject patterns that contain very large quantifier ranges (catastrophic even without nesting)
    for match in _LARGE_QUANTIFIER_RE.finditer(text):
        try:
            lower = int(match.group(1))
            upper_str = match.group(2)
            upper = int(upper_str) if upper_str else None
        except ValueError:
            continue
        numbers = [lower]
        if upper is not None:
            numbers.append(upper)
        if any(num >= _LARGE_QUANTIFIER_THRESHOLD for num in numbers):
            return True
    return False


def _safe_eval_with_timeout(fn: Callable[[], Any], timeout_ms: int) -> Tuple[bool, Optional[Any]]:
    """
    Execute a callable in a background thread with a timeout.
    Returns (completed, result). If not completed, (False, None).
    """
    result_holder: Dict[str, Any] = {}
    done_flag = {"done": False}

    def _runner() -> None:
        try:
            result_holder["value"] = fn()
        finally:
            done_flag["done"] = True

    t = threading.Thread(target=_runner, daemon=True)
    t.start()
    t.join(timeout=timeout_ms / 1000.0)
    if not done_flag["done"]:
        return False, None
    return True, result_holder.get("value")


def _safe_name_match(
    pattern: Any,
    candidate: str,
    exact_match: bool = False,
    raw_pattern: Optional[str] = None,
    context: str = "name match",
) -> bool:
    """
    Safely evaluate name match using timeout.
    For exact match patterns (^name$), use fullmatch() only.
    For partial patterns, use search().
    """
    if not candidate:
        return False
    if exact_match:
        # For exact matches, only use fullmatch (entire string must match)
        # Use longer timeout for exact matches to handle valid patterns
        # But still detect ReDoS patterns that cause catastrophic backtracking
        ok, res = _safe_eval_with_timeout(
            lambda: pattern.fullmatch(candidate) is not None,
            timeout_ms=500,  # 500ms should be enough for valid patterns, but catch ReDoS
        )
    else:
        # For partial matches, use search (pattern can appear anywhere)
        ok, res = _safe_eval_with_timeout(
            lambda: pattern.search(candidate) is not None,
            timeout_ms=500,  # 500ms should be enough for valid patterns, but catch ReDoS
        )
    # If timeout occurred, treat as no match (pattern likely causes ReDoS)
    if not ok:
        pattern_desc = raw_pattern or getattr(pattern, "pattern", "<?>")
        candidate_preview = candidate[:200]
        if len(candidate) > 200:
            candidate_preview += "..."
        logger.warning(
            "DEBUG_REGEX: Timeout during %s while matching pattern '%s' against candidate '%s'",
            context,
            pattern_desc,
            candidate_preview,
        )
        sys.stdout.flush()
        raise HTTPException(
            status_code=400,
            detail="Regex pattern too complex and may cause excessive backtracking.",
        )
    return bool(res)


def _safe_text_search(
    pattern: Any,
    text: str,
    raw_pattern: Optional[str] = None,
    context: str = "text search",
) -> bool:
    """
    Safely evaluate text search (README etc.) with timeout.
    """
    if not text:
        return False
    ok, res = _safe_eval_with_timeout(lambda: pattern.search(text) is not None, timeout_ms=500)
    # If timeout occurred, treat as no match (pattern likely causes ReDoS)
    if not ok:
        pattern_desc = raw_pattern or getattr(pattern, "pattern", "<?>")
        text_preview = text[:200]
        if len(text) > 200:
            text_preview += "..."
        logger.warning(
            "DEBUG_REGEX: Timeout during %s while searching pattern '%s' within text snippet '%s'",
            context,
            pattern_desc,
            text_preview,
        )
        sys.stdout.flush()
        raise HTTPException(
            status_code=400,
            detail="Regex pattern too complex and may cause excessive backtracking.",
        )
    return bool(res)


class ArtifactLineageNode(BaseModel):
    artifact_id: str
    name: str
    source: str
    metadata: Optional[Dict[str, Any]] = None


class ArtifactLineageEdge(BaseModel):
    from_node_artifact_id: str
    to_node_artifact_id: str
    relationship: str


class ArtifactLineageGraph(BaseModel):
    nodes: List[ArtifactLineageNode]
    edges: List[ArtifactLineageEdge]


class SimpleLicenseCheckRequest(BaseModel):
    github_url: str


class ArtifactRegEx(BaseModel):
    regex: str


class EnumerateOffset(BaseModel):
    offset: str


class HealthResponse(BaseModel):
    status: str
    timestamp: str
    uptime: str
    models_count: int
    users_count: int
    last_hour_activity: Dict[str, int]


# -------------------------
# Health component schemas (v3.4.2)
# -------------------------


class HealthStatus(str, Enum):
    ok = "ok"
    degraded = "degraded"
    critical = "critical"
    unknown = "unknown"


class HealthLogReference(BaseModel):
    label: str
    url: str
    tail_available: Optional[bool] = None
    last_updated_at: Optional[str] = None


class HealthTimelineEntry(BaseModel):
    bucket: str
    value: float
    unit: Optional[str] = None


class HealthIssue(BaseModel):
    code: str
    severity: str
    summary: str
    details: Optional[str] = None


class HealthComponentDetail(BaseModel):
    id: str
    display_name: Optional[str] = None
    status: HealthStatus
    observed_at: str
    description: Optional[str] = None
    metrics: Optional[Dict[str, Any]] = None
    issues: Optional[List[HealthIssue]] = None
    timeline: Optional[List[HealthTimelineEntry]] = None
    logs: Optional[List[HealthLogReference]] = None


class HealthComponentCollection(BaseModel):
    components: List[HealthComponentDetail]
    generated_at: str
    window_minutes: Optional[int] = None


# Server-side token call count tracking (Milestone 3 requirement: 1000 calls or 10 hours)
token_call_counts: Dict[str, int] = {}  # token_hash -> call_count


def generate_download_url(
    artifact_type: str, artifact_id: str, request: Optional[Request] = None
) -> Optional[str]:
    """
    Generate download URL for an artifact.
    Per Q&A: "You can provide an 'Object URL' of the S3 objects."
    All artifacts should have download_url in the response per spec.
    """
    # If S3 storage is available, return S3 object URL for all artifact types
    if USE_S3 and s3_storage and hasattr(s3_storage, 'bucket_name') and s3_storage.bucket_name:
        # Generate S3 object URL
        # The artifact files are stored at: artifacts/{artifact_id}/files/{file_key}
        # For download, we'll use a generic key that represents the full artifact package
        # Format: https://{bucket}.s3.{region}.amazonaws.com/{key}
        region = os.environ.get("AWS_REGION", "us-east-1")
        bucket_name = s3_storage.bucket_name
        # Use a standard key for the artifact package
        key = f"artifacts/{artifact_id}/package.zip"
        # Generate S3 object URL
        s3_url = f"https://{bucket_name}.s3.{region}.amazonaws.com/{key}"
        logger.info(f"Generated S3 download URL for artifact {artifact_id} (type: {artifact_type}): {s3_url}")
        return s3_url

    # Fallback to API endpoint if S3 not available
    if request:
        base_url = str(request.base_url).rstrip("/")
        # For models, use the model-specific download endpoint
        if artifact_type == "model":
            return f"{base_url}/models/{artifact_id}/download"
        # For other types, use a generic download endpoint
        return f"{base_url}/artifacts/{artifact_type}/{artifact_id}/download"
    # Fallback: try to get from environment variable or use relative path
    api_url = os.environ.get("API_GATEWAY_URL") or os.environ.get("REACT_APP_API_URL")
    if api_url:
        base_url = api_url.rstrip("/")
        if artifact_type == "model":
            return f"{base_url}/models/{artifact_id}/download"
        return f"{base_url}/artifacts/{artifact_type}/{artifact_id}/download"
    # Last resort: relative path (client will need to resolve)
    if artifact_type == "model":
        return f"/models/{artifact_id}/download"
    return f"/artifacts/{artifact_type}/{artifact_id}/download"


# Authentication functions (Milestone 3 - with call count tracking)
def _extract_token_value(raw_token: Optional[str]) -> Optional[str]:
    """Normalize Bearer tokens (accepts raw JWT or 'Bearer <JWT>')."""
    if not raw_token or not isinstance(raw_token, str):
        return None
    candidate = raw_token.strip()
    if not candidate:
        return None
    if candidate.lower().startswith("bearer "):
        return candidate[7:].strip()
    return candidate


def verify_token(
    request: Request,
    x_authorization: Optional[str] = Header(None, alias="X-Authorization"),
    authorization: Optional[str] = Header(None, alias="Authorization"),
    authentication_token: Optional[str] = Header(None, alias="AuthenticationToken"),
) -> Dict[str, Any]:
    """Verify token from X-Authorization (spec) or Authorization header and return user info"""
    # DEBUG: Log auth headers to diagnose 403 errors
    logger.info(
        f"DEBUG_AUTH: verify_token called - x_authorization present: {x_authorization is not None}, "
        f"authorization present: {authorization is not None}"
    )
    header_keys = list(request.headers.keys())
    query_keys = list(request.query_params.keys())
    print(f"DEBUG_AUTH: Header keys present: {header_keys}")
    print(f"DEBUG_AUTH: Query parameter keys: {query_keys}")
    sys.stdout.flush()

    if x_authorization:
        x_auth_len = len(x_authorization) if isinstance(x_authorization, str) else 0
        logger.info(f"DEBUG_AUTH: x_authorization length: {x_auth_len}")
    if authorization:
        logger.info(f"DEBUG_AUTH: authorization length: {len(authorization) if isinstance(authorization, str) else 0}")
    if authentication_token:
        token_len = len(authentication_token) if isinstance(authentication_token, str) else 0
        logger.info(f"DEBUG_AUTH: authentication_token length: {token_len}")
    sys.stdout.flush()

    token: Optional[str] = None
    token_source = None

    header_sources = [
        ("X-Authorization", x_authorization),
        ("Authorization", authorization),
        ("AuthenticationToken", authentication_token),
        ("authenticationtoken", request.headers.get("authenticationtoken")),
    ]
    for source_name, value in header_sources:
        extracted = _extract_token_value(value)
        if extracted:
            token = extracted
            token_source = f"header:{source_name}"
            break

    if not token:
        # Fallback to query parameters (autograder may supply tokens this way during concurrent tests)
        query_candidates = [
            "AuthenticationToken",
            "authenticationToken",
            "authenticationtoken",
            "authToken",
            "authtoken",
            "token",
            "auth_token",
            "x-authorization",
            "authorization",
        ]
        for key in query_candidates:
            if key in request.query_params:
                extracted = _extract_token_value(request.query_params.get(key))
                if extracted:
                    token = extracted
                    token_source = f"query:{key}"
                    logger.info(f"DEBUG_AUTH: Extracted token from query parameter '{key}'")
                    print(f"DEBUG_AUTH: Extracted token from query parameter '{key}'")
                    break

    if not token:
        # Last resort: check cookies (if frontend stored token there)
        cookie_candidates = [
            "AuthenticationToken",
            "authenticationToken",
            "authenticationtoken",
            "authToken",
            "token",
        ]
        for key in cookie_candidates:
            cookie_val = request.cookies.get(key)
            extracted = _extract_token_value(cookie_val)
            if extracted:
                token = extracted
                token_source = f"cookie:{key}"
                logger.info(f"DEBUG_AUTH: Extracted token from cookie '{key}'")
                print(f"DEBUG_AUTH: Extracted token from cookie '{key}'")
                break

    if token:
        preview = hashlib.sha256(token.encode()).hexdigest()[:8]
        logger.info(
            f"DEBUG_AUTH: Using token from {token_source or 'unknown-source'}, "
            f"length={len(token)}, sha256_prefix={preview}"
        )
        print(
            f"DEBUG_AUTH: Using token from {token_source or 'unknown-source'}, "
            f"length={len(token)}, sha256_prefix={preview}"
        )
        sys.stdout.flush()
    else:
        logger.warning(
            "DEBUG_AUTH: No auth headers, query params, or cookies contained a token."
        )
        print("DEBUG_AUTH: Token missing in headers/query/cookies")
        sys.stdout.flush()

    if not token:
        # Spec: 403 for invalid or missing AuthenticationToken
        logger.warning("DEBUG_AUTH: No token extracted, returning 403")
        print("DEBUG_AUTH: Raising 403 due to missing token")
        sys.stdout.flush()
        raise HTTPException(
            status_code=403,
            detail="Authentication failed due to invalid or missing AuthenticationToken.",
        )

    # Validate JWT token
    logger.info(f"DEBUG_AUTH: Verifying JWT token, token length: {len(token)}")
    print(f"DEBUG_AUTH: Verifying JWT token, token length: {len(token)}")
    sys.stdout.flush()
    payload = jwt_auth.verify_token(token)
    if not payload:
        logger.warning("DEBUG_AUTH: JWT token verification failed, returning 403")
        print("DEBUG_AUTH: JWT verification failed -> 403")
        sys.stdout.flush()
        raise HTTPException(
            status_code=403,
            detail="Authentication failed due to invalid or missing AuthenticationToken.",
        )
    logger.info(f"DEBUG_AUTH: JWT token verified successfully, username: {payload.get('sub', 'unknown')}")
    print(f"DEBUG_AUTH: JWT token verified successfully, username: {payload.get('sub', 'unknown')}")
    sys.stdout.flush()

    # Track call count server-side (Milestone 3 requirement: 1000 calls or 10 hours)
    # Use token hash as key to track calls
    token_hash = hashlib.sha256(token.encode()).hexdigest()

    # Get or initialize call count
    current_calls = token_call_counts.get(token_hash, 0)

    # Check if exceeded max calls (1000 calls limit)
    MAX_CALLS = 1000
    if current_calls >= MAX_CALLS:
        logger.warning(f"DEBUG_AUTH: Token exceeded max calls ({current_calls}/{MAX_CALLS}), returning 403")
        print(f"DEBUG_AUTH: Token exceeded max calls ({current_calls}/{MAX_CALLS}), returning 403")
        sys.stdout.flush()
        raise HTTPException(
            status_code=403,
            detail="Authentication failed due to invalid or missing AuthenticationToken.",
        )

    # Increment call count BEFORE returning (so next call will be checked)
    token_call_counts[token_hash] = current_calls + 1

    username = str(payload.get("sub", DEFAULT_ADMIN["username"]))
    permissions = payload.get("permissions", DEFAULT_ADMIN["permissions"])  # type: ignore[assignment]
    return {"username": username, "permissions": permissions}


def check_permission(user: Dict[str, Any], required_permission: str) -> bool:
    """Check if user has required permission"""
    return required_permission in user.get("permissions", [])


# User registration request model (Milestone 3)
class UserRegistrationRequest(BaseModel):
    username: str
    password: str
    permissions: List[str]


class UserPermissionsUpdateRequest(BaseModel):
    permissions: List[str]


# API Endpoints matching OpenAPI spec v3.3.1


@app.get("/health", response_model=HealthResponse)
async def health_check(response: Response) -> HealthResponse:
    """System health endpoint - lightweight liveness probe (BASELINE)"""
    # Explicitly set CORS headers for API Gateway compatibility
    response.headers["Access-Control-Allow-Origin"] = "*"
    response.headers["Access-Control-Allow-Methods"] = "GET, OPTIONS"
    response.headers["Access-Control-Allow-Headers"] = "*"

    try:
        # Safely get counts - handle case where databases might not be initialized
        models_count = len(artifacts_db) if artifacts_db is not None else 0
        users_count = len(users_db) if users_db is not None else 0

        return HealthResponse(
            status="healthy",
            timestamp=datetime.now().isoformat(),
            uptime="0:00:00",  # Simplified for Milestone 1
            models_count=models_count,
            users_count=users_count,
            last_hour_activity={"uploads": 0, "downloads": 0, "searches": 0},
        )
    except Exception as e:
        # Log error but still return healthy status for liveness probe
        logger.error(f"Health check error: {str(e)}")
        return HealthResponse(
            status="healthy",
            timestamp=datetime.now().isoformat(),
            uptime="0:00:00",
            models_count=0,
            users_count=0,
            last_hour_activity={"uploads": 0, "downloads": 0, "searches": 0},
        )


# Component health (NON-BASELINE in spec v3.4.2), minimal implementation
@app.get("/health/components", response_model=HealthComponentCollection)
async def health_components(
    windowMinutes: int = Query(60, ge=5, le=1440),
    includeTimeline: bool = Query(False),
) -> HealthComponentCollection:
    now_iso = datetime.now().isoformat()

    components: List[HealthComponentDetail] = []

    # API component
    # Per Q&A (James Davis): Use -1 for unsupported values
    api_component = HealthComponentDetail(
        id="api",
        display_name="FastAPI Service",
        status=HealthStatus.ok,
        observed_at=now_iso,
        description="Handles REST API requests",
        metrics={
            "uptime_seconds": -1,  # Not tracked yet
            "total_requests": -1,  # Not tracked yet
        },
        issues=[],
        logs=[
            HealthLogReference(
                label="Application Log",
                url="https://example.com/logs/app.log",
                tail_available=False,
                last_updated_at=None,
            )
        ],
        timeline=(
            [HealthTimelineEntry(bucket=now_iso, value=-1, unit="rpm")]
            if includeTimeline
            else None
        ),
    )
    components.append(api_component)

    # Metrics component (placeholder)
    # Per Q&A (James Davis): Use -1 for unsupported values
    metrics_component = HealthComponentDetail(
        id="metrics",
        display_name="Metrics Aggregator",
        status=HealthStatus.ok,
        observed_at=now_iso,
        description="Aggregates request metrics",
        metrics={
            "routes_tracked": -1,  # Not tracked yet
        },
        issues=[],
        logs=[],
        timeline=(
            [HealthTimelineEntry(bucket=now_iso, value=-1, unit="rpm")]
            if includeTimeline
            else None
        ),
    )
    components.append(metrics_component)

    return HealthComponentCollection(
        components=components,
        generated_at=now_iso,
        window_minutes=windowMinutes,
    )


# Root route for CI/API liveness checks
@app.get("/")
async def root() -> Dict[str, str]:
    return {"message": "Trustworthy Model Registry API"}


# Model upload and download endpoints


@app.post("/models/upload", response_model=Artifact, status_code=201)
async def models_upload(
    request: Request,
    file: UploadFile = File(...),
    name: Optional[str] = Form(None),
    user: Dict[str, Any] = Depends(verify_token),
) -> Artifact:
    """Upload model as ZIP file"""
    from src.storage import file_storage

    try:
        # Validate file is a ZIP
        if not file.filename or not file.filename.endswith(".zip"):
            raise HTTPException(status_code=400, detail="File must be a ZIP archive")

        # Generate artifact ID
        # Priority: in-memory (for consistency within Lambda invocation) > SQLite > S3
        # Use in-memory count first to avoid S3 eventual consistency issues
        model_count = sum(
            1 for art in artifacts_db.values() if art.get("metadata", {}).get("type") == "model"
        )
        if USE_SQLITE:
            try:
                with next(get_db()) as _db:  # type: ignore[misc]
                    sqlite_count = db_crud.count_artifacts_by_type(_db, "model")
                    # Use max of in-memory and SQLite counts to handle edge cases
                    model_count = max(model_count, sqlite_count)
            except Exception:
                pass  # Use in-memory count if SQLite fails
        # Fallback to S3 count only if in-memory is empty (shouldn't happen, but safety check)
        if model_count == 0 and USE_S3 and s3_storage:
            try:
                s3_count = s3_storage.count_artifacts_by_type("model")
                model_count = max(model_count, s3_count)
            except Exception:
                pass  # Use in-memory count if S3 fails
        artifact_id = f"model-{model_count + 1}-{int(datetime.now().timestamp() * 1_000_000)}"

        # Read file content
        content = await file.read()

        # Save ZIP file
        file_info = file_storage.save_uploaded_file(artifact_id, content, file.filename)

        # Extract ZIP
        artifact_dir = file_storage.get_artifact_directory(artifact_id)
        extracted_files = file_storage.extract_zip(file_info["path"], artifact_dir)

        # Find and read model card
        card_path = file_storage.find_model_card(artifact_dir)
        readme_text = file_storage.read_model_card(card_path) if card_path else ""

        # Determine model name
        model_name = name or file.filename.replace(".zip", "")

        # Create artifact entry
        artifact_entry = {
            "metadata": {
                "name": model_name,
                "id": artifact_id,
                "type": "model",
            },
            "data": {
                "url": f"local://{artifact_id}",
                "files": extracted_files,
                "checksum": file_info["checksum"],
                "size": file_info["size"],
            },
            "created_at": datetime.now().isoformat(),
            "created_by": user["username"],
        }

        artifacts_db[artifact_id] = artifact_entry

        # Store in S3 if enabled - verify save success
        if USE_S3 and s3_storage:
            logger.info(f"ðŸ’¾ Saving artifact to S3: id={artifact_id}, name={model_name}")
            print(
                f"DEBUG: ðŸ’¾ Saving artifact to S3 (models_upload): id={artifact_id}, name={model_name}"
            )
            sys.stdout.flush()
            success = s3_storage.save_artifact_metadata(artifact_id, artifact_entry)
            if not success:
                logger.error(
                    f"âŒ CRITICAL: Failed to save artifact {artifact_id} to S3 in models_upload - "
                    f"artifact will not persist!"
                )
                print(
                    f"DEBUG: âŒ CRITICAL: Failed to save artifact {artifact_id} to S3 in models_upload"
                )
                sys.stdout.flush()
            else:
                logger.info(f"âœ… Successfully saved artifact {artifact_id} to S3")
                print(f"DEBUG: âœ… Successfully saved artifact {artifact_id} to S3 (models_upload)")
                sys.stdout.flush()

        # Store in SQLite if enabled
        if USE_SQLITE:
            with next(get_db()) as _db:  # type: ignore[misc]
                db_crud.create_artifact(
                    _db,
                    artifact_id=artifact_id,
                    name=model_name,
                    type_="model",
                    url=f"local://{artifact_id}",
                )

        # Log audit entry
        audit_log.append(
            {
                "user": {"name": user["username"], "is_admin": user.get("is_admin", False)},
                "date": datetime.now().isoformat(),
                "artifact": artifact_entry["metadata"],
                "action": "UPLOAD",
            }
        )

        # Trigger metrics calculation
        try:
            model_data = {
                "url": f"local://{artifact_id}",
                "hf_data": [{"readme_text": readme_text}] if readme_text else [],
                "gh_data": [],
            }
            await calculate_phase2_metrics(model_data)
        except Exception as e:
            logger.warning(f"Metrics calculation failed for uploaded model: {e}")

        download_url = generate_download_url("model", artifact_id, request)
        return Artifact(
            metadata=ArtifactMetadata(**artifact_entry["metadata"]),
            data=ArtifactData(url=artifact_entry["data"]["url"], download_url=download_url),
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Upload failed: {e}")
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")


@app.get("/models/{id}/download")
async def models_download(
    id: str,
    aspect: str = Query("full", pattern="^(full|weights|datasets|code)$"),
    user: Dict[str, Any] = Depends(verify_token),
):
    """Download model files with optional aspect filtering"""
    # Enforce download permission per Q&A
    if not check_permission(user, "download"):
        raise HTTPException(status_code=401, detail="You do not have permission to download.")
    from src.storage import file_storage
    import tempfile

    try:
        # Check if artifact exists - Priority: S3 (production) > SQLite (local) > in-memory
        artifact_exists = False
        artifact_type = None
        artifact_url = None
        artifact_metadata = None

        if USE_S3 and s3_storage:
            existing_data = s3_storage.get_artifact_metadata(id)
            if existing_data:
                artifact_exists = True
                artifact_type = existing_data.get("metadata", {}).get("type")
                artifact_url = existing_data.get("data", {}).get("url", "")
                artifact_metadata = existing_data.get("metadata", {})
        elif USE_SQLITE:
            with next(get_db()) as _db:  # type: ignore[misc]
                art = db_crud.get_artifact(_db, id)
                if art:
                    artifact_exists = True
                    artifact_type = art.type
                    artifact_url = art.url
                    artifact_metadata = {"name": art.name, "id": art.id, "type": art.type}
        else:
            if id in artifacts_db:
                artifact_exists = True
                artifact_data = artifacts_db[id]
                artifact_type = artifact_data["metadata"]["type"]
                artifact_url = artifact_data["data"].get("url", "")
                artifact_metadata = artifact_data["metadata"]

        if not artifact_exists:
            raise HTTPException(status_code=404, detail="Artifact does not exist.")

        if artifact_type != "model":
            raise HTTPException(status_code=400, detail="Not a model artifact.")

        # Check if this is a URL-only model (no local files)
        if artifact_url and not artifact_url.startswith("local://"):
            raise HTTPException(
                status_code=404,
                detail="Model files not found. This model may only have URL metadata (downloaded from HuggingFace).",
            )

        # Get artifact directory
        artifact_dir = file_storage.get_artifact_directory(id)

        if not os.path.exists(artifact_dir):
            raise HTTPException(
                status_code=404,
                detail="Model files not found. This model may only have URL metadata.",
            )

        # Filter files by aspect
        files = file_storage.filter_files_by_aspect(artifact_dir, aspect)

        if not files:
            raise HTTPException(status_code=404, detail=f"No files found for aspect: {aspect}")

        # Create temporary ZIP with filtered files
        with tempfile.NamedTemporaryFile(delete=False, suffix=".zip") as tmp:
            zip_path = tmp.name

        file_storage.create_zip_from_files(files, artifact_dir, zip_path)

        # Calculate checksum for integrity
        checksum = file_storage.calculate_checksum(zip_path)

        # Log download audit
        if artifact_metadata:
            audit_log.append(
                {
                    "user": {"name": user["username"], "is_admin": user.get("is_admin", False)},
                    "date": datetime.now().isoformat(),
                    "artifact": artifact_metadata,
                    "action": f"DOWNLOAD_{aspect.upper()}",
                }
            )
            if USE_SQLITE:
                with next(get_db()) as _db:  # type: ignore[misc]
                    art = db_crud.get_artifact(_db, id)
                    if art:
                        db_crud.log_audit(
                            _db,
                            artifact=art,
                            user_name=user["username"],
                            user_is_admin=user.get("is_admin", False),
                            action=f"DOWNLOAD_{aspect.upper()}",
                        )

        # Return file with checksum in headers
        artifact_name = artifact_metadata.get("name", "model") if artifact_metadata else "model"
        return FileResponse(
            path=zip_path,
            media_type="application/zip",
            filename=f"{artifact_name}_{aspect}.zip",
            headers={"X-File-Checksum": checksum, "X-File-Aspect": aspect},
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Download failed: {e}")
        raise HTTPException(status_code=500, detail=f"Download failed: {str(e)}")


@app.get("/verify-token")
async def verify_token_endpoint() -> Dict[str, str]:
    """Token verification endpoint (tokens are validated per request)"""
    raise HTTPException(
        status_code=501,
        detail="No standalone token verification endpoint; tokens are validated per request.",
    )


# User registration endpoint (Milestone 3 - admin only)
@app.post("/register")
async def register_user(
    request: UserRegistrationRequest, user: Dict[str, Any] = Depends(verify_token)
):
    """Register a new user (Milestone 3 - admin only)"""
    # Only admins can register users
    if not check_permission(user, "admin"):
        raise HTTPException(status_code=401, detail="You do not have permission to register users.")

    # Check if user already exists
    if request.username in users_db:
        raise HTTPException(status_code=409, detail="User already exists.")

    # Hash password with bcrypt (Milestone 3 requirement)
    hashed_password = jwt_auth.get_password_hash(request.password)

    # Create user entry
    new_user = {
        "username": request.username,
        "password": hashed_password,
        "permissions": request.permissions,
        "created_at": datetime.now().isoformat(),
        "is_admin": "admin" in request.permissions,
    }

    users_db[request.username] = new_user

    if USE_SQLITE:
        with next(get_db()) as _db:  # type: ignore[misc]
            from src.db import models as db_models

            db_user = db_models.User(
                username=request.username,
                password=hashed_password,
                is_admin="admin" in request.permissions,
                permissions=",".join(request.permissions),
            )
            _db.add(db_user)
            _db.commit()

    if USE_S3 and s3_storage:
        try:
            s3_storage.save_user(request.username, new_user)
        except Exception:
            # Non-fatal for registration; will still exist in-memory/SQLite
            pass

    return {"message": "User registered successfully."}


# User deletion endpoint (Milestone 3)
@app.delete("/user/{username}")
async def delete_user(username: str, user: Dict[str, Any] = Depends(verify_token)):
    """Delete a user (Milestone 3 - users can delete own account, admins can delete any)"""
    # Users can delete their own account, admins can delete any account (per Q&A)
    is_requester_admin = check_permission(user, "admin")
    if username != user["username"] and not is_requester_admin:
        raise HTTPException(
            status_code=401, detail="You do not have permission to delete this user."
        )

    # Resolve target user's permissions to check if target is admin (no special restrictions)
    target_user_data = users_db.get(username)
    if not target_user_data and USE_S3 and s3_storage:
        try:
            target_user_data = s3_storage.get_user(username)
        except Exception:
            target_user_data = None
    if not target_user_data and USE_SQLITE:
        try:
            with next(get_db()) as _db:  # type: ignore[misc]
                from src.db import models as db_models

                row = _db.get(db_models.User, username)
                if row:
                    target_user_data = {
                        "username": row.username,
                        "permissions": (
                            [p for p in str(row.permissions).split(",") if p]
                            if getattr(row, "permissions", None)
                            else []
                        ),
                        "is_admin": bool(getattr(row, "is_admin", False)),
                    }
        except Exception:
            target_user_data = None

    found_any = False
    # Remove from in-memory cache if present
    if username in users_db:
        del users_db[username]
        found_any = True

    if USE_SQLITE:
        with next(get_db()) as _db:  # type: ignore[misc]
            from src.db import models as db_models

            db_user = _db.get(db_models.User, username)
            if db_user:
                _db.delete(db_user)
                _db.commit()
                found_any = True

    if USE_S3 and s3_storage:
        try:
            if s3_storage.delete_user(username):
                found_any = True
        except Exception:
            pass

    if not found_any:
        raise HTTPException(status_code=404, detail="User not found.")

    return {"message": "User deleted successfully."}


# Update user permissions (admin only)
@app.put("/user/{username}/permissions")
async def update_user_permissions(
    username: str,
    request: UserPermissionsUpdateRequest,
    user: Dict[str, Any] = Depends(verify_token),
):
    """Update a user's permissions (admin only). Cannot modify default admin."""
    if not check_permission(user, "admin"):
        raise HTTPException(status_code=401, detail="You do not have permission to edit users.")

    if username == DEFAULT_ADMIN["username"]:
        raise HTTPException(status_code=400, detail="Cannot edit default admin user.")

    new_permissions: List[str] = [str(p) for p in request.permissions if isinstance(p, str)]
    is_admin_flag = "admin" in new_permissions

    # Update in-memory cache if present
    if username in users_db:
        users_db[username]["permissions"] = new_permissions
        users_db[username]["is_admin"] = is_admin_flag

    # SQLite update
    if USE_SQLITE:
        with next(get_db()) as _db:  # type: ignore[misc]
            from src.db import models as db_models

            db_user = _db.get(db_models.User, username)
            if not db_user:
                # Create if missing to keep sources consistent
                db_user = db_models.User(
                    username=username,
                    password=DEFAULT_ADMIN["password"],  # placeholder; user must reset
                    is_admin=is_admin_flag,
                    permissions=",".join(new_permissions),
                )
                _db.add(db_user)
            else:
                db_user.permissions = ",".join(new_permissions)  # type: ignore[assignment]
                setattr(db_user, "is_admin", is_admin_flag)
            _db.commit()

    # S3 update
    if USE_S3 and s3_storage:
        try:
            # Fetch then update or create best-effort
            existing = s3_storage.get_user(username) or {
                "username": username,
                "password": DEFAULT_ADMIN["password"],
            }
            existing["permissions"] = new_permissions
            existing["is_admin"] = is_admin_flag
            s3_storage.save_user(username, existing)
        except Exception:
            pass

    return {"message": "Permissions updated."}


# User listing endpoint (Milestone 3) - supports S3 (prod) and SQLite/local
@app.get("/users")
async def list_users(user: Dict[str, Any] = Depends(verify_token)) -> List[Dict[str, Any]]:
    """List users for admin UI (admin only). Returns username and permissions."""
    if not check_permission(user, "admin"):
        raise HTTPException(status_code=401, detail="You do not have permission to list users.")

    results: List[Dict[str, Any]] = []

    if USE_S3 and s3_storage:
        try:
            documents = s3_storage.list_users()
            for doc in documents:
                username = str(doc.get("username") or doc.get("name") or "")
                if not username:
                    continue
                perms = doc.get("permissions") or []
                # Normalize permissions to list[str]
                if isinstance(perms, str):
                    perms_list = [p for p in perms.split(",") if p]
                elif isinstance(perms, list):
                    perms_list = [str(p) for p in perms]
                else:
                    perms_list = []
                results.append({"username": username, "permissions": perms_list})
        except Exception:
            # Fall through to SQLite/memory if S3 fails
            results = []

    if not results and USE_SQLITE:
        with next(get_db()) as _db:  # type: ignore[misc]
            from src.db import models as db_models

            rows = _db.query(db_models.User).all()
            for r in rows:
                perms_list = []
                if getattr(r, "permissions", None):
                    perms_list = [p for p in str(r.permissions).split(",") if p]  # type: ignore[attr-defined]
                results.append({"username": r.username, "permissions": perms_list})

    if not results:
        # In-memory fallback
        for uname, udata in users_db.items():
            perms = udata.get("permissions", [])
            if isinstance(perms, list):
                perms_list = [str(p) for p in perms]
            elif isinstance(perms, str):
                perms_list = [p for p in perms.split(",") if p]
            else:
                perms_list = []
            results.append({"username": uname, "permissions": perms_list})

    return results


# Authentication endpoint
@app.put("/authenticate", response_model=str)
async def create_auth_token(request: AuthenticationRequest) -> str:
    """Create an access token (Milestone 3 - with proper validation)"""
    logger.info(f"Authenticate endpoint called for user: {request.user.name}")

    # CRITICAL: Ensure default admin exists on EVERY request (Lambda cold start protection)
    admin_username = str(DEFAULT_ADMIN["username"])
    if admin_username not in users_db:
        users_db[admin_username] = DEFAULT_ADMIN.copy()
        logger.info(f"Recreated default admin user: {admin_username}")

    # Validate user credentials against in-memory/SQLite/S3 user store
    user_data = users_db.get(request.user.name)
    if not user_data:
        # Try S3 (production)
        if USE_S3 and s3_storage:
            try:
                s3_user = s3_storage.get_user(request.user.name)
                if isinstance(s3_user, dict) and s3_user.get("username"):
                    users_db[request.user.name] = s3_user
                    user_data = s3_user
            except Exception:
                pass
        # Try SQLite (local)
        if not user_data and USE_SQLITE:
            try:
                with next(get_db()) as _db:  # type: ignore[misc]
                    from src.db import models as db_models

                    row = _db.get(db_models.User, request.user.name)
                    if row:
                        user_data = {
                            "username": row.username,
                            "password": row.password,  # type: ignore[assignment]
                            "permissions": (
                                [p for p in str(row.permissions).split(",") if p]
                                if getattr(row, "permissions", None)
                                else []
                            ),
                            "is_admin": bool(getattr(row, "is_admin", False)),
                        }
                        if user_data:
                            users_db[request.user.name] = user_data
            except Exception:
                user_data = None
        if not user_data:
            logger.warning(f"User not found: {request.user.name}")
            raise HTTPException(status_code=401, detail="The user or password is invalid.")

    # Use bcrypt for password verification (Milestone 3 requirement)
    stored_password = user_data.get("password", "")
    if isinstance(stored_password, str) and stored_password.startswith("$2b$"):
        # Password is hashed, use bcrypt verification
        if not jwt_auth.verify_password(request.secret.password, stored_password):
            logger.warning(f"Password verification failed for user: {request.user.name}")
            raise HTTPException(status_code=401, detail="The user or password is invalid.")
    else:
        # Plain text password (for default admin during migration)
        # Only accept exact match - autograder sends 'packages;' (62 chars)
        password_matches = request.secret.password == stored_password

        if not password_matches:
            logger.warning(
                f"Plain text password mismatch for user: {request.user.name}. "
                f"Expected length: {len(stored_password)}, Got length: {len(request.secret.password)}"
            )
            raise HTTPException(status_code=401, detail="The user or password is invalid.")

    # Issue JWT containing subject and permissions
    payload = {
        "sub": user_data["username"],
        "permissions": user_data.get("permissions", []),
    }
    jwt_token = jwt_auth.create_access_token(payload)

    # Spec: AuthenticationToken is a string; we include 'bearer ' prefix to ease client use
    return f"bearer {jwt_token}"


# Registry reset endpoint
@app.delete("/reset")
async def registry_reset(user: Dict[str, Any] = Depends(verify_token)):
    """Reset the registry to a system default state (BASELINE)"""
    logger.info("Reset endpoint called")

    if not check_permission(user, "admin"):
        raise HTTPException(
            status_code=401, detail="You do not have permission to reset the registry."
        )

    # Clear all artifacts (CRITICAL: ensure this is complete)
    # Priority: S3 (production) > SQLite (local) > in-memory
    # IMPORTANT: Always clear in-memory artifacts_db regardless of storage backend
    artifacts_db.clear()

    if USE_S3 and s3_storage:
        try:
            s3_storage.clear_all_artifacts()
        except Exception as e:
            logger.error(f"Failed to clear S3 storage: {e}")
    elif USE_SQLITE:
        try:
            with next(get_db()) as _db:  # type: ignore[misc]
                db_crud.reset_registry(_db)
        except Exception as e:
            logger.error(f"Failed to reset SQLite database: {e}")

    audit_log.clear()
    token_call_counts.clear()
    artifact_status.clear()
    rating_cache.clear()
    rating_locks.clear()
    async_rating_events.clear()

    # Clear users but preserve default admin (per spec requirement)
    admin_username: str = str(DEFAULT_ADMIN["username"])
    if USE_SQLITE:
        try:
            with next(get_db()) as _db:  # type: ignore[misc]
                db_crud.upsert_default_admin(
                   