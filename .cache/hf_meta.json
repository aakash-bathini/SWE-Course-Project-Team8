{"model:openai/whisper-tiny": {"payload": {"url": "https://huggingface.co/openai/whisper-tiny", "repo_id": "openai/whisper-tiny", "repo_type": "model", "license": "apache-2.0", "downloads": 611131, "likes": 374, "last_modified": "2024-02-29T10:57:33+00:00", "tags": ["transformers", "pytorch", "tf", "jax", "safetensors", "whisper", "automatic-speech-recognition", "audio", "hf-asr-leaderboard", "en", "zh", "de", "es", "ru", "ko", "fr", "ja", "pt", "tr", "pl", "ca", "nl", "ar", "sv", "it", "id", "hi", "fi", "vi", "he", "uk", "el", "ms", "cs", "ro", "da", "hu", "ta", "no", "th", "ur", "hr", "bg", "lt", "la", "mi", "ml", "cy", "sk", "te", "fa", "lv", "bn", "sr", "az", "sl", "kn", "et", "mk", "br", "eu", "is", "hy", "ne", "mn", "bs", "kk", "sq", "sw", "gl", "mr", "pa", "si", "km", "sn", "yo", "so", "af", "oc", "ka", "be", "tg", "sd", "gu", "am", "yi", "lo", "uz", "fo", "ht", "ps", "tk", "nn", "mt", "sa", "lb", "my", "bo", "tl", "mg", "as", "tt", "haw", "ln", "ha", "ba", "jw", "su", "arxiv:2212.04356", "license:apache-2.0", "model-index", "endpoints_compatible", "region:us"], "pipeline_tag": "automatic-speech-recognition", "card_yaml": "language:\n- en\n- zh\n- de\n- es\n- ru\n- ko\n- fr\n- ja\n- pt\n- tr\n- pl\n- ca\n- nl\n- ar\n- sv\n- it\n- id\n- hi\n- fi\n- vi\n- he\n- uk\n- el\n- ms\n- cs\n- ro\n- da\n- hu\n- ta\n- 'no'\n- th\n- ur\n- hr\n- bg\n- lt\n- la\n- mi\n- ml\n- cy\n- sk\n- te\n- fa\n- lv\n- bn\n- sr\n- az\n- sl\n- kn\n- et\n- mk\n- br\n- eu\n- is\n- hy\n- ne\n- mn\n- bs\n- kk\n- sq\n- sw\n- gl\n- mr\n- pa\n- si\n- km\n- sn\n- yo\n- so\n- af\n- oc\n- ka\n- be\n- tg\n- sd\n- gu\n- am\n- yi\n- lo\n- uz\n- fo\n- ht\n- ps\n- tk\n- nn\n- mt\n- sa\n- lb\n- my\n- bo\n- tl\n- mg\n- as\n- tt\n- haw\n- ln\n- ha\n- ba\n- jw\n- su\nlicense: apache-2.0\npipeline_tag: automatic-speech-recognition\ntags:\n- audio\n- automatic-speech-recognition\n- hf-asr-leaderboard\nwidget:\n- example_title: Librispeech sample 1\n  src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n- example_title: Librispeech sample 2\n  src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\nmodel-index:\n- name: whisper-tiny\n  results:\n  - task:\n      type: automatic-speech-recognition\n      name: Automatic Speech Recognition\n    dataset:\n      name: LibriSpeech (clean)\n      type: librispeech_asr\n      config: clean\n      split: test\n      args:\n        language: en\n    metrics:\n    - type: wer\n      value: 7.54\n      name: Test WER\n      verified: false\n  - task:\n      type: automatic-speech-recognition\n      name: Automatic Speech Recognition\n    dataset:\n      name: LibriSpeech (other)\n      type: librispeech_asr\n      config: other\n      split: test\n      args:\n        language: en\n    metrics:\n    - type: wer\n      value: 17.15\n      name: Test WER\n      verified: false\n  - task:\n      type: automatic-speech-recognition\n      name: Automatic Speech Recognition\n    dataset:\n      name: Common Voice 11.0\n      type: mozilla-foundation/common_voice_11_0\n      config: hi\n      split: test\n      args:\n        language: hi\n    metrics:\n    - type: wer\n      value: 141\n      name: Test WER\n      verified: false", "readme_text": "\n# Whisper\n\nWhisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours \nof labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains **without** the need \nfor fine-tuning.\n\nWhisper was proposed in the paper [Robust Speech Recognition via Large-Scale Weak Supervision](https://arxiv.org/abs/2212.04356) \nby Alec Radford et al from OpenAI. The original code repository can be found [here](https://github.com/openai/whisper).\n\n**Disclaimer**: Content for this model card has partly been written by the Hugging Face team, and parts of it were \ncopied and pasted from the original model card.\n\n## Model details\n\nWhisper is a Transformer based encoder-decoder model, also referred to as a _sequence-to-sequence_ model. \nIt was trained on 680k hours of labelled speech data annotated using large-scale weak supervision. \n\nThe models were trained on either English-only data or multilingual data. The English-only models were trained \non the task of speech recognition. The multilingual models were trained on both speech recognition and speech \ntranslation. For speech recognition, the model predicts transcriptions in the *same* language as the audio. \nFor speech translation, the model predicts transcriptions to a *different* language to the audio.\n\nWhisper checkpoints come in five configurations of varying model sizes.\nThe smallest four are trained on either English-only or multilingual data.\nThe largest checkpoints are multilingual only. All ten of the pre-trained checkpoints \nare available on the [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). The \ncheckpoints are summarised in the following table with links to the models on the Hub:\n\n| Size     | Parameters | English-only                                         | Multilingual                                        |\n|----------|------------|------------------------------------------------------|-----------------------------------------------------|\n| tiny     | 39 M       | [\u2713](https://huggingface.co/openai/whisper-tiny.en)   | [\u2713](https://huggingface.co/openai/whisper-tiny)     |\n| base     | 74 M       | [\u2713](https://huggingface.co/openai/whisper-base.en)   | [\u2713](https://huggingface.co/openai/whisper-base)     |\n| small    | 244 M      | [\u2713](https://huggingface.co/openai/whisper-small.en)  | [\u2713](https://huggingface.co/openai/whisper-small)    |\n| medium   | 769 M      | [\u2713](https://huggingface.co/openai/whisper-medium.en) | [\u2713](https://huggingface.co/openai/whisper-medium)   |\n| large    | 1550 M     | x                                                    | [\u2713](https://huggingface.co/openai/whisper-large)    |\n| large-v2 | 1550 M     | x                                                    | [\u2713](https://huggingface.co/openai/whisper-large-v2) |\n\n# Usage\n\nTo transcribe audio samples, the model has to be used alongside a [`WhisperProcessor`](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperProcessor).\n\nThe `WhisperProcessor` is used to:\n1. Pre-process the audio inputs (converting them to log-Mel spectrograms for the model)\n2. Post-process the model outputs (converting them from tokens to text)\n\nThe model is informed of which task to perform (transcription or translation) by passing the appropriate \"context tokens\". These context tokens \nare a sequence of tokens that are given to the decoder at the start of the decoding process, and take the following order:\n1. The transcription always starts with the `<|startoftranscript|>` token\n2. The second token is the language token (e.g. `<|en|>` for English)\n3. The third token is the \"task token\". It can take one of two values: `<|transcribe|>` for speech recognition or `<|translate|>` for speech translation\n4. In addition, a `<|notimestamps|>` token is added if the model should not include timestamp prediction\n\nThus, a typical sequence of context tokens might look as follows:\n```\n<|startoftranscript|> <|en|> <|transcribe|> <|notimestamps|>\n```\nWhich tells the model to decode in English, under the task of speech recognition, and not to predict timestamps.\n\nThese tokens can either be forced or un-forced. If they are forced, the model is made to predict each token at \neach position. This allows one to control the output language and task for the Whisper model. If they are un-forced, \nthe Whisper model will automatically predict the output langauge and task itself.\n\nThe context tokens can be set accordingly:\n\n```python\nmodel.config.forced_decoder_ids = WhisperProcessor.get_decoder_prompt_ids(language=\"english\", task=\"transcribe\")\n```\n\nWhich forces the model to predict in English under the task of speech recognition.\n\n## Transcription\n\n### English to English \nIn this example, the context tokens are 'unforced', meaning the model automatically predicts the output language\n(English) and task (transcribe).\n\n```python\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n>>> model.config.forced_decoder_ids = None\n\n>>> # load dummy dataset and read audio files\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> sample = ds[0][\"audio\"]\n>>> input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features \n\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n['<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.<|endoftext|>']\n\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.']\n```\nThe context tokens can be removed from the start of the transcription by setting `skip_special_tokens=True`.\n\n### French to French \nThe following example demonstrates French to French transcription by setting the decoder ids appropriately. \n\n```python\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"transcribe\")\n\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset(\"common_voice\", \"fr\", split=\"test\", streaming=True)\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))[\"audio\"]\n>>> input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\").input_features\n\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids)\n['<|startoftranscript|><|fr|><|transcribe|><|notimestamps|> Un vrai travail int\u00e9ressant va enfin \u00eatre men\u00e9 sur ce sujet.<|endoftext|>']\n\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' Un vrai travail int\u00e9ressant va enfin \u00eatre men\u00e9 sur ce sujet.']\n```\n\n## Translation \nSetting the task to \"translate\" forces the Whisper model to perform speech translation.\n\n### French to English\n\n```python\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"translate\")\n\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset(\"common_voice\", \"fr\", split=\"test\", streaming=True)\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))[\"audio\"]\n>>> input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\").input_features\n\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' A very interesting work, we will finally be given on this subject.']\n```\n\n## Evaluation\n\nThis code snippet shows how to evaluate Whisper Tiny on [LibriSpeech test-clean](https://huggingface.co/datasets/librispeech_asr):\n \n```python\n>>> from datasets import load_dataset\n>>> from transformers import WhisperForConditionalGeneration, WhisperProcessor\n>>> import torch\n>>> from evaluate import load\n\n>>> librispeech_test_clean = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\n\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\").to(\"cuda\")\n\n>>> def map_to_pred(batch):\n>>>     audio = batch[\"audio\"]\n>>>     input_features = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>>     batch[\"reference\"] = processor.tokenizer._normalize(batch['text'])\n>>> \n>>>     with torch.no_grad():\n>>>         predicted_ids = model.generate(input_features.to(\"cuda\"))[0]\n>>>     transcription = processor.decode(predicted_ids)\n>>>     batch[\"prediction\"] = processor.tokenizer._normalize(transcription)\n>>>     return batch\n\n>>> result = librispeech_test_clean.map(map_to_pred)\n\n>>> wer = load(\"wer\")\n>>> print(100 * wer.compute(references=result[\"reference\"], predictions=result[\"prediction\"]))\n7.547098647858638\n```\n\n## Long-Form Transcription\n\nThe Whisper model is intrinsically designed to work on audio samples of up to 30s in duration. However, by using a chunking \nalgorithm, it can be used to transcribe audio samples of up to arbitrary length. This is possible through Transformers \n[`pipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline) \nmethod. Chunking is enabled by setting `chunk_length_s=30` when instantiating the pipeline. With chunking enabled, the pipeline \ncan be run with batched inference. It can also be extended to predict sequence level timestamps by passing `return_timestamps=True`:\n\n```python\n>>> import torch\n>>> from transformers import pipeline\n>>> from datasets import load_dataset\n\n>>> device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n>>> pipe = pipeline(\n>>>   \"automatic-speech-recognition\",\n>>>   model=\"openai/whisper-tiny\",\n>>>   chunk_length_s=30,\n>>>   device=device,\n>>> )\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> sample = ds[0][\"audio\"]\n\n>>> prediction = pipe(sample.copy(), batch_size=8)[\"text\"]\n\" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.\"\n\n>>> # we can also return timestamps for the predictions\n>>> prediction = pipe(sample.copy(), batch_size=8, return_timestamps=True)[\"chunks\"]\n[{'text': ' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.',\n  'timestamp': (0.0, 5.44)}]\n```\n\nRefer to the blog post [ASR Chunking](https://huggingface.co/blog/asr-chunking) for more details on the chunking algorithm.\n\n## Fine-Tuning\n\nThe pre-trained Whisper model demonstrates a strong ability to generalise to different datasets and domains. However, \nits predictive capabilities can be improved further for certain languages and tasks through *fine-tuning*. The blog \npost [Fine-Tune Whisper with \ud83e\udd17 Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step \nguide to fine-tuning the Whisper model with as little as 5 hours of labelled data.\n\n### Evaluated Use\n\nThe primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only \u201cintended\u201d uses or to draw reasonable guidelines around what is or is not research.\n\nThe models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them.\n\nIn particular, we caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification. We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes. The models are intended to transcribe and translate speech, use of the model for classification is not only not evaluated but also not appropriate, particularly to infer human attributes.\n\n\n## Training Data\n\nThe models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages. \n\nAs discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.\n\n\n## Performance and Limitations\n\nOur studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. \n\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\n\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf). \n\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in [the paper](https://cdn.openai.com/papers/whisper.pdf). It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.\n\n\n## Broader Implications\n\nWe anticipate that Whisper models\u2019 transcription capabilities may be used for improving accessibility tools. While Whisper models cannot be used for real-time transcription out of the box \u2013 their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation. The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications.\n\nThere are also potential dual use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\n\n\n### BibTeX entry and citation info\n```bibtex\n@misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}\n```\n", "files": [{"path": ".gitattributes", "size": 1438}, {"path": "README.md", "size": 19787}, {"path": "added_tokens.json", "size": 34604}, {"path": "config.json", "size": 1983}, {"path": "flax_model.msgpack", "size": 151048591}, {"path": "generation_config.json", "size": 3747}, {"path": "merges.txt", "size": 493869}, {"path": "model.safetensors", "size": 151061672}, {"path": "normalizer.json", "size": 52666}, {"path": "preprocessor_config.json", "size": 184990}, {"path": "pytorch_model.bin", "size": 151095027}, {"path": "special_tokens_map.json", "size": 2194}, {"path": "tf_model.h5", "size": 151253960}, {"path": "tokenizer.json", "size": 2480466}, {"path": "tokenizer_config.json", "size": 282683}, {"path": "vocab.json", "size": 835550}], "size": 608853227, "datasets": [], "github_links": ["https://github.com/openai/whisper"], "_source": {"fetched_at": 1761677185.3057349, "last_modified": "2024-02-29T10:57:33+00:00"}}, "fetched_at": 1761677185.3057349}, "model:google-bert/bert-base-uncased": {"payload": {"url": "https://huggingface.co/google-bert/bert-base-uncased", "repo_id": "google-bert/bert-base-uncased", "repo_type": "model", "license": "apache-2.0", "downloads": 54334019, "likes": 2453, "last_modified": "2024-02-19T11:06:12+00:00", "tags": ["transformers", "pytorch", "tf", "jax", "rust", "coreml", "onnx", "safetensors", "bert", "fill-mask", "exbert", "en", "dataset:bookcorpus", "dataset:wikipedia", "arxiv:1810.04805", "license:apache-2.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "pipeline_tag": "fill-mask", "card_yaml": "datasets:\n- bookcorpus\n- wikipedia\nlanguage: en\nlicense: apache-2.0\ntags:\n- exbert", "readme_text": "\n# BERT base model (uncased)\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https://arxiv.org/abs/1810.04805) and first released in\n[this repository](https://github.com/google-research/bert). This model is uncased: it does not make a difference\nbetween english and English.\n\nDisclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it\nwas pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\n\n- Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\n  the entire masked sentence through the model and has to predict the masked words. This is different from traditional\n  recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\n  GPT which internally masks the future tokens. It allows the model to learn a bidirectional representation of the\n  sentence.\n- Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\n  they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\n  predict if the two sentences were following each other or not.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences, for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\n## Model variations\n\nBERT has originally been released in base and large variations, for cased and uncased input text. The uncased models also strips out an accent markers.  \nChinese and multilingual uncased and cased versions followed shortly after.  \nModified preprocessing with whole word masking has replaced subpiece masking in a following work, with the release of two models.  \nOther 24 smaller models are released afterward.  \n\nThe detailed release history can be found on the [google-research/bert readme](https://github.com/google-research/bert/blob/master/README.md) on github.\n\n| Model | #params | Language |\n|------------------------|--------------------------------|-------|\n| [`bert-base-uncased`](https://huggingface.co/bert-base-uncased) | 110M   | English |\n| [`bert-large-uncased`](https://huggingface.co/bert-large-uncased)              | 340M    | English | sub \n| [`bert-base-cased`](https://huggingface.co/bert-base-cased)        | 110M    | English |\n| [`bert-large-cased`](https://huggingface.co/bert-large-cased) | 340M    |  English |\n| [`bert-base-chinese`](https://huggingface.co/bert-base-chinese) | 110M    | Chinese |\n| [`bert-base-multilingual-cased`](https://huggingface.co/bert-base-multilingual-cased) | 110M | Multiple |\n| [`bert-large-uncased-whole-word-masking`](https://huggingface.co/bert-large-uncased-whole-word-masking) | 340M | English |\n| [`bert-large-cased-whole-word-masking`](https://huggingface.co/bert-large-cased-whole-word-masking) | 340M | English |\n\n## Intended uses & limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for\nfine-tuned versions of a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.1073106899857521,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.08774490654468536,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a new model. [SEP]\",\n  'score': 0.05338378623127937,\n  'token': 2047,\n  'token_str': 'new'},\n {'sequence': \"[CLS] hello i'm a super model. [SEP]\",\n  'score': 0.04667217284440994,\n  'token': 3565,\n  'token_str': 'super'},\n {'sequence': \"[CLS] hello i'm a fine model. [SEP]\",\n  'score': 0.027095865458250046,\n  'token': 2986,\n  'token_str': 'fine'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the man worked as a carpenter. [SEP]',\n  'score': 0.09747550636529922,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the man worked as a waiter. [SEP]',\n  'score': 0.0523831807076931,\n  'token': 15610,\n  'token_str': 'waiter'},\n {'sequence': '[CLS] the man worked as a barber. [SEP]',\n  'score': 0.04962705448269844,\n  'token': 13362,\n  'token_str': 'barber'},\n {'sequence': '[CLS] the man worked as a mechanic. [SEP]',\n  'score': 0.03788609802722931,\n  'token': 15893,\n  'token_str': 'mechanic'},\n {'sequence': '[CLS] the man worked as a salesman. [SEP]',\n  'score': 0.037680890411138535,\n  'token': 18968,\n  'token_str': 'salesman'}]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the woman worked as a nurse. [SEP]',\n  'score': 0.21981462836265564,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the woman worked as a waitress. [SEP]',\n  'score': 0.1597415804862976,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the woman worked as a maid. [SEP]',\n  'score': 0.1154729500412941,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the woman worked as a prostitute. [SEP]',\n  'score': 0.037968918681144714,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the woman worked as a cook. [SEP]',\n  'score': 0.03042375110089779,\n  'token': 5660,\n  'token_str': 'cook'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038\nunpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and\nheaders).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus, and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size\nof 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer\nused is Adam with a learning rate of 1e-4, \\\\(\\beta_{1} = 0.9\\\\) and \\\\(\\beta_{2} = 0.999\\\\), a weight decay of 0.01,\nlearning rate warmup for 10,000 steps and linear decay of the learning rate after.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI-(m/mm) | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  | Average |\n|:----:|:-----------:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|:-------:|\n|      | 84.6/83.4   | 71.2 | 90.5 | 93.5  | 52.1 | 85.8  | 88.9 | 66.4 | 79.6    |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n<a href=\"https://huggingface.co/exbert/?model=bert-base-uncased\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n", "files": [{"path": ".gitattributes", "size": 491}, {"path": "LICENSE", "size": 11356}, {"path": "README.md", "size": 10517}, {"path": "config.json", "size": 570}, {"path": "coreml/fill-mask/float32_model.mlpackage/Data/com.apple.CoreML/model.mlmodel", "size": 164911}, {"path": "coreml/fill-mask/float32_model.mlpackage/Data/com.apple.CoreML/weights/weight.bin", "size": 531833856}, {"path": "coreml/fill-mask/float32_model.mlpackage/Manifest.json", "size": 617}, {"path": "flax_model.msgpack", "size": 438064459}, {"path": "model.onnx", "size": 532091246}, {"path": "model.safetensors", "size": 440449768}, {"path": "pytorch_model.bin", "size": 440473133}, {"path": "rust_model.ot", "size": 534240408}, {"path": "tf_model.h5", "size": 536063208}, {"path": "tokenizer.json", "size": 466062}, {"path": "tokenizer_config.json", "size": 48}, {"path": "vocab.txt", "size": 231508}], "size": 3454102158, "datasets": ["bookcorpus", "wikipedia"], "github_links": ["https://github.com/google-research/bert", "https://github.com/google-research/bert/blob/master/README.md"], "_source": {"fetched_at": 1761772638.9224901, "last_modified": "2024-02-19T11:06:12+00:00"}}, "fetched_at": 1761772638.9224901}, "model:parvk11/audience_classifier_model": {"payload": {"url": "https://huggingface.co/parvk11/audience_classifier_model", "repo_id": "parvk11/audience_classifier_model", "repo_type": "model", "license": null, "downloads": 1134, "likes": 0, "last_modified": "2025-04-25T04:49:24+00:00", "tags": ["transformers", "pytorch", "distilbert", "text-classification", "arxiv:1910.09700", "autotrain_compatible", "endpoints_compatible", "region:us"], "pipeline_tag": "text-classification", "card_yaml": "library_name: transformers\ntags: []", "readme_text": "\n# Model Card for Model ID\n\n<!-- Provide a quick summary of what the model is/does. -->\n\n\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nThis is the model card of a \ud83e\udd17 transformers model that has been pushed on the Hub. This model card has been automatically generated.\n\n- **Developed by:** [More Information Needed]\n- **Funded by [optional]:** [More Information Needed]\n- **Shared by [optional]:** [More Information Needed]\n- **Model type:** [More Information Needed]\n- **Language(s) (NLP):** [More Information Needed]\n- **License:** [More Information Needed]\n- **Finetuned from model [optional]:** [More Information Needed]\n\n### Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** [More Information Needed]\n- **Paper [optional]:** [More Information Needed]\n- **Demo [optional]:** [More Information Needed]\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n### Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\n[More Information Needed]\n\n### Downstream Use [optional]\n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\n\n[More Information Needed]\n\n### Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\n[More Information Needed]\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\n[More Information Needed]\n\n### Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n[More Information Needed]\n\n## Training Details\n\n### Training Data\n\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\n[More Information Needed]\n\n### Training Procedure\n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\n#### Preprocessing [optional]\n\n[More Information Needed]\n\n\n#### Training Hyperparameters\n\n- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->\n\n#### Speeds, Sizes, Times [optional]\n\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\n\n[More Information Needed]\n\n## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n### Testing Data, Factors & Metrics\n\n#### Testing Data\n\n<!-- This should link to a Dataset Card if possible. -->\n\n[More Information Needed]\n\n#### Factors\n\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\n\n[More Information Needed]\n\n#### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n\n[More Information Needed]\n\n### Results\n\n[More Information Needed]\n\n#### Summary\n\n\n\n## Model Examination [optional]\n\n<!-- Relevant interpretability work for the model goes here -->\n\n[More Information Needed]\n\n## Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** [More Information Needed]\n- **Hours used:** [More Information Needed]\n- **Cloud Provider:** [More Information Needed]\n- **Compute Region:** [More Information Needed]\n- **Carbon Emitted:** [More Information Needed]\n\n## Technical Specifications [optional]\n\n### Model Architecture and Objective\n\n[More Information Needed]\n\n### Compute Infrastructure\n\n[More Information Needed]\n\n#### Hardware\n\n[More Information Needed]\n\n#### Software\n\n[More Information Needed]\n\n## Citation [optional]\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n[More Information Needed]\n\n**APA:**\n\n[More Information Needed]\n\n## Glossary [optional]\n\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n\n[More Information Needed]\n\n## More Information [optional]\n\n[More Information Needed]\n\n## Model Card Authors [optional]\n\n[More Information Needed]\n\n## Model Card Contact\n\n[More Information Needed]", "files": [{"path": ".gitattributes", "size": 1519}, {"path": "README.md", "size": 5174}, {"path": "config.json", "size": 757}, {"path": "pytorch_model.bin", "size": 267857642}, {"path": "special_tokens_map.json", "size": 732}, {"path": "tokenizer_config.json", "size": 1336}, {"path": "vocab.txt", "size": 262030}], "size": 268129190, "datasets": [], "github_links": [], "_source": {"fetched_at": 1760032288.2379708, "last_modified": "2025-04-25T04:49:24+00:00"}}, "fetched_at": 1760032288.2379708}, "model:google/gemma-3-270m": {"payload": {"url": "https://huggingface.co/google/gemma-3-270m", "repo_id": "google/gemma-3-270m", "repo_type": "model", "license": "gemma", "downloads": 152553, "likes": 862, "last_modified": "2025-08-14T07:35:01+00:00", "tags": ["transformers", "safetensors", "gemma3_text", "text-generation", "gemma3", "gemma", "google", "arxiv:2503.19786", "arxiv:1905.07830", "arxiv:1905.10044", "arxiv:1911.11641", "arxiv:1705.03551", "arxiv:1911.01547", "arxiv:1907.10641", "arxiv:2311.07911", "arxiv:2311.12022", "arxiv:2411.04368", "arxiv:1904.09728", "arxiv:1903.00161", "arxiv:2009.03300", "arxiv:2304.06364", "arxiv:2103.03874", "arxiv:2110.14168", "arxiv:2108.07732", "arxiv:2107.03374", "arxiv:2403.07974", "arxiv:2305.03111", "arxiv:2405.04520", "arxiv:2210.03057", "arxiv:2106.03193", "arxiv:1910.11856", "arxiv:2502.12404", "arxiv:2502.21228", "arxiv:2404.16816", "arxiv:2104.12756", "arxiv:2311.16502", "arxiv:2203.10244", "arxiv:2404.12390", "arxiv:1810.12440", "arxiv:1908.02660", "arxiv:2310.02255", "arxiv:2312.11805", "license:gemma", "autotrain_compatible", "text-generation-inference", "endpoints_compatible", "region:us"], "pipeline_tag": "text-generation", "card_yaml": "library_name: transformers\nlicense: gemma\npipeline_tag: text-generation\ntags:\n- gemma3\n- gemma\n- google\nextra_gated_heading: Access Gemma on Hugging Face\nextra_gated_prompt: To access Gemma on Hugging Face, you\u2019re required to review and\n  agree to Google\u2019s usage license. To do this, please ensure you\u2019re logged in to Hugging\n  Face and click below. Requests are processed immediately.\nextra_gated_button_content: Acknowledge license", "readme_text": "\n# Gemma 3 model card\n\n**Model Page**: [Gemma](https://ai.google.dev/gemma/docs/core)\n\n**Resources and Technical Documentation**:\n\n* [Gemma 3 Technical Report][g3-tech-report]\n* [Responsible Generative AI Toolkit][rai-toolkit]\n* [Gemma on Kaggle][kaggle-gemma]\n* [Gemma on Vertex Model Garden][vertex-mg-gemma3]\n\n**Terms of Use**: [Terms][terms]\n\n**Authors**: Google DeepMind\n\n## Model Information\n\nSummary description and brief definition of inputs and outputs.\n\n### Description\n\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\n\n### Inputs and outputs\n\n-   **Input:**\n    -  Text string, such as a question, a prompt, or a document to be summarized\n    -  Images, normalized to 896 x 896 resolution and encoded to 256 tokens\n       each, for the 4B, 12B, and 27B sizes.\n    -  Total input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n       32K tokens for the 1B and 270M sizes.\n\n-   **Output:**\n    -   Generated text in response to the input, such as an answer to a\n        question, analysis of image content, or a summary of a document\n    -   Total output context up to 128K tokens for the 4B, 12B, and 27B sizes,\n        and 32K tokens for the 1B and 270M sizes per request, subtracting the\n        request input tokens\n\n### Citation\n\n```none\n@article{gemma_2025,\n    title={Gemma 3},\n    url={https://arxiv.org/abs/2503.19786},\n    publisher={Google DeepMind},\n    author={Gemma Team},\n    year={2025}\n}\n```\n\n## Model Data\n\nData used for model training and how the data was processed.\n\n### Training Dataset\n\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens,\nthe 1B with 2 trillion tokens, and the 270M with 6 trillion tokens. The \nknowledge cutoff date for the training data was August 2024. Here are the key \ncomponents:\n\n-   Web Documents: A diverse collection of web text ensures the model is\n    exposed to a broad range of linguistic styles, topics, and vocabulary. The\n    training dataset includes content in over 140 languages.\n-   Code: Exposing the model to code helps it to learn the syntax and\n    patterns of programming languages, which improves its ability to generate\n    code and understand code-related questions.\n-   Mathematics: Training on mathematical text helps the model learn logical\n    reasoning, symbolic representation, and to address mathematical queries.\n-   Images: A wide range of images enables the model to perform image\n    analysis and visual data extraction tasks.\n\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.\n\n### Data Preprocessing\n\nHere are the key data cleaning and filtering methods applied to the training\ndata:\n\n-   CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\n    was applied at multiple stages in the data preparation process to ensure\n    the exclusion of harmful and illegal content.\n-   Sensitive Data Filtering: As part of making Gemma pre-trained models\n    safe and reliable, automated techniques were used to filter out certain\n    personal information and other sensitive data from training sets.\n-   Additional methods: Filtering based on content quality and safety in\n    line with [our policies][safety-policies].\n\n## Implementation Information\n\nDetails about the model internals.\n\n### Hardware\n\nGemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\n\n-   Performance: TPUs are specifically designed to handle the massive\n    computations involved in training VLMs. They can speed up training\n    considerably compared to CPUs.\n-   Memory: TPUs often come with large amounts of high-bandwidth memory,\n    allowing for the handling of large models and batch sizes during training.\n    This can lead to better model quality.\n-   Scalability: TPU Pods (large clusters of TPUs) provide a scalable\n    solution for handling the growing complexity of large foundation models.\n    You can distribute training across multiple TPU devices for faster and more\n    efficient processing.\n-   Cost-effectiveness: In many scenarios, TPUs can provide a more\n    cost-effective solution for training large models compared to CPU-based\n    infrastructure, especially when considering the time and resources saved\n    due to faster training.\n-   These advantages are aligned with\n    [Google's commitments to operate sustainably][sustainability].\n\n### Software\n\nTraining was done using [JAX][jax] and [ML Pathways][ml-pathways].\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models][gemini-2-paper]; *\"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"*\n\n## Evaluation\n\nModel evaluation metrics and results.\n\n### Benchmark Results\n\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation. Evaluation results marked\nwith **IT** are for instruction-tuned models. Evaluation results marked with\n**PT** are for pre-trained models.\n\n#### Gemma 3 270M\n\n| **Benchmark**             |  **n-shot**   | **Gemma 3 PT 270M** |\n| :------------------------ | :-----------: | ------------------: |\n| [HellaSwag][hellaswag]    |    10-shot    |                40.9 |\n| [BoolQ][boolq]            |    0-shot     |                61.4 |\n| [PIQA][piqa]              |    0-shot     |                67.7 |\n| [TriviaQA][triviaqa]      |    5-shot     |                15.4 |\n| [ARC-c][arc]              |    25-shot    |                29.0 |\n| [ARC-e][arc]              |    0-shot     |                57.7 |\n| [WinoGrande][winogrande]  |    5-shot     |                52.0 |\n\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[boolq]: https://arxiv.org/abs/1905.10044\n[piqa]: https://arxiv.org/abs/1911.11641\n[triviaqa]: https://arxiv.org/abs/1705.03551\n[arc]: https://arxiv.org/abs/1911.01547\n[winogrande]: https://arxiv.org/abs/1907.10641\n\n| **Benchmark**             |  **n-shot**   | **Gemma 3 IT 270m** |\n| :------------------------ | :-----------: | ------------------: |\n| [HellaSwag][hellaswag]    |    0-shot     |                37.7 |\n| [PIQA][piqa]              |    0-shot     |                66.2 |\n| [ARC-c][arc]              |    0-shot     |                28.2 |\n| [WinoGrande][winogrande]  |    0-shot     |                52.3 |\n| [BIG-Bench Hard][bbh]     |   few-shot    |                26.7 |\n| [IF Eval][ifeval]         |    0-shot     |                51.2 |\n\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[piqa]: https://arxiv.org/abs/1911.11641\n[arc]: https://arxiv.org/abs/1911.01547\n[winogrande]: https://arxiv.org/abs/1907.10641\n[bbh]: https://paperswithcode.com/dataset/bbh\n[bbh]: https://paperswithcode.com/dataset/bbh\n[ifeval]: https://arxiv.org/abs/2311.07911\n\n#### Gemma 3 1B, 4B, 12B & 27B\n\n##### Reasoning and factuality\n\n| Benchmark                      | n-shot | Gemma 3 IT 1B | Gemma 3 IT 4B | Gemma 3 IT 12B | Gemma 3 IT 27B |\n|--------------------------------|--------|:-------------:|:-------------:|:--------------:|:--------------:|\n| [GPQA][gpqa] Diamond           | 0-shot |      19.2     |      30.8     |      40.9      |      42.4      |\n| [SimpleQA][simpleqa]           | 0-shot |      2.2      |      4.0      |       6.3      |      10.0      |\n| [FACTS Grounding][facts-grdg]  |    -   |      36.4     |      70.1     |      75.8      |      74.9      |\n| [BIG-Bench Hard][bbh]          | 0-shot |      39.1     |      72.2     |      85.7      |      87.6      |\n| [BIG-Bench Extra Hard][bbeh]   | 0-shot |      7.2      |      11.0     |      16.3      |      19.3      |\n| [IFEval][ifeval]               | 0-shot |      80.2     |      90.2     |      88.9      |      90.4      |\n\n| Benchmark                      | n-shot   | Gemma 3 PT 1B  | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------|:--------------:|:-------------:|:--------------:|:--------------:|\n| [HellaSwag][hellaswag]         | 10-shot  |      62.3      |      77.2     |      84.2      |      85.6      |\n| [BoolQ][boolq]                 | 0-shot   |      63.2      |      72.3     |      78.8      |      82.4      |\n| [PIQA][piqa]                   | 0-shot   |      73.8      |      79.6     |      81.8      |      83.3      |\n| [SocialIQA][socialiqa]         | 0-shot   |      48.9      |      51.9     |      53.4      |      54.9      |\n| [TriviaQA][triviaqa]           | 5-shot   |      39.8      |      65.8     |      78.2      |      85.5      |\n| [Natural Questions][naturalq]  | 5-shot   |      9.48      |      20.0     |      31.4      |      36.1      |\n| [ARC-c][arc]                   | 25-shot  |      38.4      |      56.2     |      68.9      |      70.6      |\n| [ARC-e][arc]                   | 0-shot   |      73.0      |      82.4     |      88.3      |      89.0      |\n| [WinoGrande][winogrande]       | 5-shot   |      58.2      |      64.7     |      74.3      |      78.8      |\n| [BIG-Bench Hard][bbh]          | few-shot |      28.4      |      50.9     |      72.6      |      77.7      |\n| [DROP][drop]                   | 1-shot   |      42.4      |      60.1     |      72.2      |      77.2      |\n\n[gpqa]: https://arxiv.org/abs/2311.12022\n[simpleqa]: https://arxiv.org/abs/2411.04368\n[facts-grdg]: https://goo.gle/FACTS_paper\n[bbeh]: https://github.com/google-deepmind/bbeh\n[ifeval]: https://arxiv.org/abs/2311.07911\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[boolq]: https://arxiv.org/abs/1905.10044\n[piqa]: https://arxiv.org/abs/1911.11641\n[socialiqa]: https://arxiv.org/abs/1904.09728\n[triviaqa]: https://arxiv.org/abs/1705.03551\n[naturalq]: https://github.com/google-research-datasets/natural-questions\n[arc]: https://arxiv.org/abs/1911.01547\n[winogrande]: https://arxiv.org/abs/1907.10641\n[bbh]: https://paperswithcode.com/dataset/bbh\n[drop]: https://arxiv.org/abs/1903.00161\n\n##### STEM and code\n\n| Benchmark                  | n-shot | Gemma 3 IT 1B | Gemma 3 IT 4B | Gemma 3 IT 12B | Gemma 3 IT 27B |\n|----------------------------|--------|:-------------:|:-------------:|:--------------:|:--------------:|\n| [MMLU][mmlu] (Pro)         | 0-shot |      14.7     |      43.6     |      60.6      |      67.5      |\n| [LiveCodeBench][lcb]       | 0-shot |      1.9      |      12.6     |      24.6      |      29.7      |\n| [Bird-SQL][bird-sql] (dev) |    -   |      6.4      |      36.3     |      47.9      |      54.4      |\n| [Math][math]               | 0-shot |      48.0     |      75.6     |      83.8      |      89.0      |\n| HiddenMath                 | 0-shot |      15.8     |      43.0     |      54.5      |      60.3      |\n| [MBPP][mbpp]               | 3-shot |      35.2     |      63.2     |      73.0      |      74.4      |\n| [HumanEval][humaneval]     | 0-shot |      41.5     |      71.3     |      85.4      |      87.8      |\n| [Natural2Code][nat2code]   | 0-shot |      56.0     |      70.3     |      80.7      |      84.5      |\n| [GSM8K][gsm8k]             | 0-shot |      62.8     |      89.2     |      94.4      |      95.9      |\n\n| Benchmark                      | n-shot         | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:-------------:|:--------------:|:--------------:|\n| [MMLU][mmlu]                   | 5-shot         |      59.6     |      74.5      |      78.6      |\n| [MMLU][mmlu] (Pro COT)         | 5-shot         |      29.2     |      45.3      |      52.2      |\n| [AGIEval][agieval]             | 3-5-shot       |      42.1     |      57.4      |      66.2      |\n| [MATH][math]                   | 4-shot         |      24.2     |      43.3      |      50.0      |\n| [GSM8K][gsm8k]                 | 8-shot         |      38.4     |      71.0      |      82.6      |\n| [GPQA][gpqa]                   | 5-shot         |      15.0     |      25.4      |      24.3      |\n| [MBPP][mbpp]                   | 3-shot         |      46.0     |      60.4      |      65.6      |\n| [HumanEval][humaneval]         | 0-shot         |      36.0     |      45.7      |      48.8      |\n\n[mmlu]: https://arxiv.org/abs/2009.03300\n[agieval]: https://arxiv.org/abs/2304.06364\n[math]: https://arxiv.org/abs/2103.03874\n[gsm8k]: https://arxiv.org/abs/2110.14168\n[gpqa]: https://arxiv.org/abs/2311.12022\n[mbpp]: https://arxiv.org/abs/2108.07732\n[humaneval]: https://arxiv.org/abs/2107.03374\n[lcb]: https://arxiv.org/abs/2403.07974\n[bird-sql]: https://arxiv.org/abs/2305.03111\n[nat2code]: https://arxiv.org/abs/2405.04520\n\n#### Multilingual\n\n| Benchmark                            | n-shot | Gemma 3 IT 1B | Gemma 3 IT 4B | Gemma 3 IT 12B | Gemma 3 IT 27B |\n|--------------------------------------|--------|:-------------:|:-------------:|:--------------:|:--------------:|\n| [Global-MMLU-Lite][global-mmlu-lite] | 0-shot |      34.2     |      54.5     |      69.5      |      75.1      |\n| [ECLeKTic][eclektic]                 | 0-shot |      1.4      |      4.6      |      10.3      |      16.7      |\n| [WMT24++][wmt24pp]                   | 0-shot |      35.9     |      46.8     |      51.6      |      53.4      |\n\n| Benchmark                            | Gemma 3 PT 1B | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------------ |:-------------:|:-------------:|:--------------:|:--------------:|\n| [MGSM][mgsm]                         |      2.04     |      34.7     |      64.3     |      74.3     |\n| [Global-MMLU-Lite][global-mmlu-lite] |      24.9     |      57.0     |      69.4     |      75.7     |\n| [WMT24++][wmt24pp] (ChrF)            |      36.7     |      48.4     |      53.9     |      55.7     |\n| [FloRes][flores]                     |      29.5     |      39.2     |      46.0     |      48.8     |\n| [XQuAD][xquad] (all)                 |      43.9     |      68.0     |      74.5     |      76.8     |\n| [ECLeKTic][eclektic]                 |      4.69     |      11.0     |      17.2     |      24.4     |\n| [IndicGenBench][indicgenbench]       |      41.4     |      57.2     |      61.7     |      63.4     |\n\n[mgsm]: https://arxiv.org/abs/2210.03057\n[flores]: https://arxiv.org/abs/2106.03193\n[xquad]: https://arxiv.org/abs/1910.11856v3\n[global-mmlu-lite]: https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite\n[wmt24pp]: https://arxiv.org/abs/2502.12404v1\n[eclektic]: https://arxiv.org/abs/2502.21228\n[indicgenbench]: https://arxiv.org/abs/2404.16816\n\n##### Multimodal\n\n| Benchmark                         | Gemma 3 IT 4B | Gemma 3 IT 12B | Gemma 3 IT 27B |\n|-----------------------------------|:-------------:|:--------------:|:--------------:|\n| [MMMU][mmmu] (val)                |      48.8     |      59.6      |      64.9      |\n| [DocVQA][docvqa]                  |      75.8     |      87.1      |      86.6      |\n| [InfoVQA][info-vqa]               |      50.0     |      64.9      |      70.6      |\n| [TextVQA][textvqa]                |      57.8     |      67.7      |      65.1      |\n| [AI2D][ai2d]                      |      74.8     |      84.2      |      84.5      |\n| [ChartQA][chartqa]                |      68.8     |      75.7      |      78.0      |\n| [VQAv2][vqav2] (val)              |      62.4     |      71.6      |      71.0      |\n| [MathVista][mathvista] (testmini) |      50.0     |      62.9      |      67.6      |\n\n| Benchmark                      | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |:-------------:|:--------------:|:--------------:|\n| [COCOcap][coco-cap]            |      102      |      111       |      116       |\n| [DocVQA][docvqa] (val)         |      72.8     |      82.3      |      85.6      |\n| [InfoVQA][info-vqa] (val)      |      44.1     |      54.8      |      59.4      |\n| [MMMU][mmmu] (pt)              |      39.2     |      50.3      |      56.1      |\n| [TextVQA][textvqa] (val)       |      58.9     |      66.5      |      68.6      |\n| [RealWorldQA][realworldqa]     |      45.5     |      52.2      |      53.9      |\n| [ReMI][remi]                   |      27.3     |      38.5      |      44.8      |\n| [AI2D][ai2d]                   |      63.2     |      75.2      |      79.0      |\n| [ChartQA][chartqa]             |      63.6     |      74.7      |      76.3      |\n| [VQAv2][vqav2]                 |      63.9     |      71.2      |      72.9      |\n| [BLINK][blinkvqa]              |      38.0     |      35.9      |      39.6      |\n| [OKVQA][okvqa]                 |      51.0     |      58.7      |      60.2      |\n| [TallyQA][tallyqa]             |      42.5     |      51.8      |      54.3      |\n| [SpatialSense VQA][ss-vqa]     |      50.9     |      60.0      |      59.4      |\n| [CountBenchQA][countbenchqa]   |      26.1     |      17.8      |      68.0      |\n\n[coco-cap]: https://cocodataset.org/#home\n[docvqa]: https://www.docvqa.org/\n[info-vqa]: https://arxiv.org/abs/2104.12756\n[mmmu]: https://arxiv.org/abs/2311.16502\n[textvqa]: https://textvqa.org/\n[realworldqa]: https://paperswithcode.com/dataset/realworldqa\n[remi]: https://arxiv.org/html/2406.09175v1\n[ai2d]: https://allenai.org/data/diagrams\n[chartqa]: https://arxiv.org/abs/2203.10244\n[vqav2]: https://visualqa.org/index.html\n[blinkvqa]: https://arxiv.org/abs/2404.12390\n[okvqa]: https://okvqa.allenai.org/\n[tallyqa]: https://arxiv.org/abs/1810.12440\n[ss-vqa]: https://arxiv.org/abs/1908.02660\n[countbenchqa]: https://github.com/google-research/big_vision/blob/main/big_vision/datasets/countbenchqa/\n[mathvista]: https://arxiv.org/abs/2310.02255\n\n## Ethics and Safety\n\nEthics and safety evaluation approach and results.\n\n### Evaluation Approach\n\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\n\n-   **Child Safety**: Evaluation of text-to-text and image to text prompts\n    covering child safety policies, including child sexual abuse and\n    exploitation.\n-   **Content Safety:** Evaluation of text-to-text and image to text prompts\n    covering safety policies including, harassment, violence and gore, and hate\n    speech.\n-   **Representational Harms**: Evaluation of text-to-text and image to text\n    prompts covering safety policies including bias, stereotyping, and harmful\n    associations or inaccuracies.\n\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making.\nAssurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\n\n### Evaluation Results\n\nFor all areas of safety testing, we saw major improvements in the categories of\nchild safety, content safety, and representational harms relative to previous\nGemma models. All testing was conducted without safety filters to evaluate the\nmodel capabilities and behaviors. For both text-to-text and image-to-text, and\nacross all model sizes, the model produced minimal policy violations, and showed\nsignificant improvements over previous Gemma models' performance with respect\nto ungrounded inferences. A limitation of our evaluations was they included only\nEnglish language prompts.\n\n## Usage and Limitations\n\nThese models have certain limitations that users should be aware of.\n\n### Intended Usage\n\nOpen vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n-   Content Creation and Communication\n    -   Text Generation: These models can be used to generate creative text\n        formats such as poems, scripts, code, marketing copy, and email drafts.\n    -   Chatbots and Conversational AI: Power conversational interfaces\n        for customer service, virtual assistants, or interactive applications.\n    -   Text Summarization: Generate concise summaries of a text corpus,\n        research papers, or reports.\n    -   Image Data Extraction: These models can be used to extract,\n        interpret, and summarize visual data for text communications.\n-   Research and Education\n    -   Natural Language Processing (NLP) and VLM Research: These\n        models can serve as a foundation for researchers to experiment with VLM\n        and NLP techniques, develop algorithms, and contribute to the\n        advancement of the field.\n    -   Language Learning Tools: Support interactive language learning\n        experiences, aiding in grammar correction or providing writing practice.\n    -   Knowledge Exploration: Assist researchers in exploring large\n        bodies of text by generating summaries or answering questions about\n        specific topics.\n\n### Limitations\n\n-   Training Data\n    -   The quality and diversity of the training data significantly\n        influence the model's capabilities. Biases or gaps in the training data\n        can lead to limitations in the model's responses.\n    -   The scope of the training dataset determines the subject areas\n        the model can handle effectively.\n-   Context and Task Complexity\n    -   Models are better at tasks that can be framed with clear\n        prompts and instructions. Open-ended or highly complex tasks might be\n        challenging.\n    -   A model's performance can be influenced by the amount of context\n        provided (longer context generally leads to better outputs, up to a\n        certain point).\n-   Language Ambiguity and Nuance\n    -   Natural language is inherently complex. Models might struggle\n        to grasp subtle nuances, sarcasm, or figurative language.\n-   Factual Accuracy\n    -   Models generate responses based on information they learned\n        from their training datasets, but they are not knowledge bases. They\n        may generate incorrect or outdated factual statements.\n-   Common Sense\n    -   Models rely on statistical patterns in language. They might\n        lack the ability to apply common sense reasoning in certain situations.\n\n### Ethical Considerations and Risks\n\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\n\n-   Bias and Fairness\n    -   VLMs trained on large-scale, real-world text and image data can\n        reflect socio-cultural biases embedded in the training material. These\n        models underwent careful scrutiny, input data pre-processing described\n        and posterior evaluations reported in this card.\n-   Misinformation and Misuse\n    -   VLMs can be misused to generate text that is false, misleading,\n        or harmful.\n    -   Guidelines are provided for responsible use with the model, see the\n        [Responsible Generative AI Toolkit][rai-toolkit].\n-   Transparency and Accountability:\n    -   This model card summarizes details on the models' architecture,\n        capabilities, limitations, and evaluation processes.\n    -   A responsibly developed open model offers the opportunity to\n        share innovation by making VLM technology accessible to developers and\n        researchers across the AI ecosystem.\n\nRisks identified and mitigations:\n\n-   **Perpetuation of biases**: It's encouraged to perform continuous\n    monitoring (using evaluation metrics, human review) and the exploration of\n    de-biasing techniques during model training, fine-tuning, and other use\n    cases.\n-   **Generation of harmful content**: Mechanisms and guidelines for content\n    safety are essential. Developers are encouraged to exercise caution and\n    implement appropriate content safety safeguards based on their specific\n    product policies and application use cases.\n-   **Misuse for malicious purposes**: Technical limitations and developer\n    and end-user education can help mitigate against malicious applications of\n    VLMs. Educational resources and reporting mechanisms for users to flag\n    misuse are provided. Prohibited uses of Gemma models are outlined in the\n    [Gemma Prohibited Use Policy][prohibited-use].\n-   **Privacy violations**: Models were trained on data filtered for removal\n    of certain personal information and other sensitive data. Developers are\n    encouraged to adhere to privacy regulations with privacy-preserving\n    techniques.\n\n### Benefits\n\nAt the time of release, this family of models provides high-performance open\nvision-language model implementations designed from the ground up for\nresponsible AI development compared to similarly sized models.\n\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.\n\n[g3-tech-report]: https://arxiv.org/abs/2503.19786\n[rai-toolkit]: https://ai.google.dev/responsible\n[kaggle-gemma]: https://www.kaggle.com/models/google/gemma-3\n[vertex-mg-gemma3]: https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3\n[terms]: https://ai.google.dev/gemma/terms\n[safety-policies]: https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf\n[prohibited-use]: https://ai.google.dev/gemma/prohibited_use_policy\n[tpu]: https://cloud.google.com/tpu/docs/intro-to-tpu\n[sustainability]: https://sustainability.google/operating-sustainably/\n[jax]: https://github.com/jax-ml/jax\n[ml-pathways]: https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/\n[sustainability]: https://sustainability.google/operating-sustainably/\n[gemini-2-paper]: https://arxiv.org/abs/2312.11805\n", "files": [{"path": ".gitattributes", "size": 1570}, {"path": "README.md", "size": 28276}, {"path": "added_tokens.json", "size": 35}, {"path": "config.json", "size": 1352}, {"path": "generation_config.json", "size": 133}, {"path": "model.safetensors", "size": 536223056}, {"path": "special_tokens_map.json", "size": 662}, {"path": "tokenizer.json", "size": 33384570}, {"path": "tokenizer.model", "size": 4689074}, {"path": "tokenizer_config.json", "size": 1155375}], "size": 575484103, "datasets": [], "github_links": ["https://github.com/google-deepmind/bbeh", "https://github.com/google-research-datasets/natural-questions", "https://github.com/google-research/big_vision/blob/main/big_vision/datasets/countbenchqa/", "https://github.com/jax-ml/jax"], "_source": {"fetched_at": 1760028728.014226, "last_modified": "2025-08-14T07:35:01+00:00"}}, "fetched_at": 1760028728.014226}, "model:distilbert/distilbert-base-uncased": {"payload": {"url": "https://huggingface.co/distilbert/distilbert-base-uncased", "repo_id": "distilbert/distilbert-base-uncased", "repo_type": "model", "license": "apache-2.0", "downloads": 13911895, "likes": 779, "last_modified": "2024-05-06T13:44:53+00:00", "tags": ["transformers", "pytorch", "tf", "jax", "rust", "safetensors", "distilbert", "fill-mask", "exbert", "en", "dataset:bookcorpus", "dataset:wikipedia", "arxiv:1910.01108", "license:apache-2.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "pipeline_tag": "fill-mask", "card_yaml": "datasets:\n- bookcorpus\n- wikipedia\nlanguage: en\nlicense: apache-2.0\ntags:\n- exbert", "readme_text": "\n# DistilBERT base model (uncased)\n\nThis model is a distilled version of the [BERT base model](https://huggingface.co/bert-base-uncased). It was\nintroduced in [this paper](https://arxiv.org/abs/1910.01108). The code for the distillation process can be found\n[here](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation). This model is uncased: it does\nnot make a difference between english and English.\n\n## Model description\n\nDistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a\nself-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only,\nwith no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic\nprocess to generate inputs and labels from those texts using the BERT base model. More precisely, it was pretrained\nwith three objectives:\n\n- Distillation loss: the model was trained to return the same probabilities as the BERT base model.\n- Masked language modeling (MLM): this is part of the original training loss of the BERT base model. When taking a\n  sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the\n  model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that\n  usually see the words one after the other, or from autoregressive models like GPT which internally mask the future\n  tokens. It allows the model to learn a bidirectional representation of the sentence.\n- Cosine embedding loss: the model was also trained to generate hidden states as close as possible as the BERT base\n  model.\n\nThis way, the model learns the same inner representation of the English language than its teacher model, while being\nfaster for inference or downstream tasks.\n\n## Intended uses & limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=distilbert) to look for\nfine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.05292855575680733,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.03968575969338417,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a business model. [SEP]\",\n  'score': 0.034743521362543106,\n  'token': 2449,\n  'token_str': 'business'},\n {'sequence': \"[CLS] hello i'm a model model. [SEP]\",\n  'score': 0.03462274372577667,\n  'token': 2944,\n  'token_str': 'model'},\n {'sequence': \"[CLS] hello i'm a modeling model. [SEP]\",\n  'score': 0.018145186826586723,\n  'token': 11643,\n  'token_str': 'modeling'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import DistilBertTokenizer, DistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import DistilBertTokenizer, TFDistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions. It also inherits some of\n[the bias of its teacher model](https://huggingface.co/bert-base-uncased#limitations-and-bias).\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"The White man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the white man worked as a blacksmith. [SEP]',\n  'score': 0.1235365942120552,\n  'token': 20987,\n  'token_str': 'blacksmith'},\n {'sequence': '[CLS] the white man worked as a carpenter. [SEP]',\n  'score': 0.10142576694488525,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the white man worked as a farmer. [SEP]',\n  'score': 0.04985016956925392,\n  'token': 7500,\n  'token_str': 'farmer'},\n {'sequence': '[CLS] the white man worked as a miner. [SEP]',\n  'score': 0.03932540491223335,\n  'token': 18594,\n  'token_str': 'miner'},\n {'sequence': '[CLS] the white man worked as a butcher. [SEP]',\n  'score': 0.03351764753460884,\n  'token': 14998,\n  'token_str': 'butcher'}]\n\n>>> unmasker(\"The Black woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the black woman worked as a waitress. [SEP]',\n  'score': 0.13283951580524445,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the black woman worked as a nurse. [SEP]',\n  'score': 0.12586183845996857,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the black woman worked as a maid. [SEP]',\n  'score': 0.11708822101354599,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the black woman worked as a prostitute. [SEP]',\n  'score': 0.11499975621700287,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the black woman worked as a housekeeper. [SEP]',\n  'score': 0.04722772538661957,\n  'token': 22583,\n  'token_str': 'housekeeper'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nDistilBERT pretrained on the same data as BERT, which is [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset\nconsisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia)\n(excluding lists, tables and headers).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 8 16 GB V100 for 90 hours. See the\n[training code](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for all hyperparameters\ndetails.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |\n|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|\n|      | 82.2 | 88.5 | 89.2 | 91.3  | 51.3 | 85.8  | 87.5 | 59.9 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{Sanh2019DistilBERTAD,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1910.01108}\n}\n```\n\n<a href=\"https://huggingface.co/exbert/?model=distilbert-base-uncased\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n", "files": [{"path": ".gitattributes", "size": 445}, {"path": "LICENSE", "size": 11356}, {"path": "README.md", "size": 8577}, {"path": "config.json", "size": 483}, {"path": "flax_model.msgpack", "size": 267945836}, {"path": "model.safetensors", "size": 267954768}, {"path": "pytorch_model.bin", "size": 267967963}, {"path": "rust_model.ot", "size": 361732396}, {"path": "tf_model.h5", "size": 363423424}, {"path": "tokenizer.json", "size": 466062}, {"path": "tokenizer_config.json", "size": 48}, {"path": "vocab.txt", "size": 231508}], "size": 1529742866, "datasets": ["bookcorpus", "wikipedia"], "github_links": ["https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation"], "_source": {"fetched_at": 1761677185.897811, "last_modified": "2024-05-06T13:44:53+00:00"}}, "fetched_at": 1761677185.897811}, "model:roberta-base": {"payload": {"url": "https://huggingface.co/roberta-base", "repo_id": "roberta-base", "repo_type": "model", "license": "mit", "downloads": 15822394, "likes": 530, "last_modified": "2024-02-19T12:39:28+00:00", "tags": ["transformers", "pytorch", "tf", "jax", "rust", "safetensors", "roberta", "fill-mask", "exbert", "en", "dataset:bookcorpus", "dataset:wikipedia", "arxiv:1907.11692", "arxiv:1806.02847", "license:mit", "autotrain_compatible", "endpoints_compatible", "region:us"], "pipeline_tag": "fill-mask", "card_yaml": "datasets:\n- bookcorpus\n- wikipedia\nlanguage: en\nlicense: mit\ntags:\n- exbert", "readme_text": "\n# RoBERTa base model\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https://arxiv.org/abs/1907.11692) and first released in\n[this repository](https://github.com/pytorch/fairseq/tree/master/examples/roberta). This model is case-sensitive: it\nmakes a difference between english and English.\n\nDisclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nRoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means\nit was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. \n\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model\nrandomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict\nthe masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one\nafter the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to\nlearn a bidirectional representation of the sentence.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\n## Intended uses & limitations\n\nYou can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task.\nSee the [model hub](https://huggingface.co/models?filter=roberta) to look for fine-tuned versions on a task that\ninterests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at a model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='roberta-base')\n>>> unmasker(\"Hello I'm a <mask> model.\")\n\n[{'sequence': \"<s>Hello I'm a male model.</s>\",\n  'score': 0.3306540250778198,\n  'token': 2943,\n  'token_str': '\u0120male'},\n {'sequence': \"<s>Hello I'm a female model.</s>\",\n  'score': 0.04655390977859497,\n  'token': 2182,\n  'token_str': '\u0120female'},\n {'sequence': \"<s>Hello I'm a professional model.</s>\",\n  'score': 0.04232972860336304,\n  'token': 2038,\n  'token_str': '\u0120professional'},\n {'sequence': \"<s>Hello I'm a fashion model.</s>\",\n  'score': 0.037216778844594955,\n  'token': 2734,\n  'token_str': '\u0120fashion'},\n {'sequence': \"<s>Hello I'm a Russian model.</s>\",\n  'score': 0.03253649175167084,\n  'token': 1083,\n  'token_str': '\u0120Russian'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import RobertaTokenizer, RobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaModel.from_pretrained('roberta-base')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import RobertaTokenizer, TFRobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = TFRobertaModel.from_pretrained('roberta-base')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nThe training data used for this model contains a lot of unfiltered content from the internet, which is far from\nneutral. Therefore, the model can have biased predictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='roberta-base')\n>>> unmasker(\"The man worked as a <mask>.\")\n\n[{'sequence': '<s>The man worked as a mechanic.</s>',\n  'score': 0.08702439814805984,\n  'token': 25682,\n  'token_str': '\u0120mechanic'},\n {'sequence': '<s>The man worked as a waiter.</s>',\n  'score': 0.0819653645157814,\n  'token': 38233,\n  'token_str': '\u0120waiter'},\n {'sequence': '<s>The man worked as a butcher.</s>',\n  'score': 0.073323555290699,\n  'token': 32364,\n  'token_str': '\u0120butcher'},\n {'sequence': '<s>The man worked as a miner.</s>',\n  'score': 0.046322137117385864,\n  'token': 18678,\n  'token_str': '\u0120miner'},\n {'sequence': '<s>The man worked as a guard.</s>',\n  'score': 0.040150221437215805,\n  'token': 2510,\n  'token_str': '\u0120guard'}]\n\n>>> unmasker(\"The Black woman worked as a <mask>.\")\n\n[{'sequence': '<s>The Black woman worked as a waitress.</s>',\n  'score': 0.22177888453006744,\n  'token': 35698,\n  'token_str': '\u0120waitress'},\n {'sequence': '<s>The Black woman worked as a prostitute.</s>',\n  'score': 0.19288744032382965,\n  'token': 36289,\n  'token_str': '\u0120prostitute'},\n {'sequence': '<s>The Black woman worked as a maid.</s>',\n  'score': 0.06498628109693527,\n  'token': 29754,\n  'token_str': '\u0120maid'},\n {'sequence': '<s>The Black woman worked as a secretary.</s>',\n  'score': 0.05375480651855469,\n  'token': 2971,\n  'token_str': '\u0120secretary'},\n {'sequence': '<s>The Black woman worked as a nurse.</s>',\n  'score': 0.05245552211999893,\n  'token': 9008,\n  'token_str': '\u0120nurse'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe RoBERTa model was pretrained on the reunion of five datasets:\n- [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books;\n- [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers) ;\n- [CC-News](https://commoncrawl.org/2016/10/news-dataset-available/), a dataset containing 63 millions English news\n  articles crawled between September 2016 and February 2019.\n- [OpenWebText](https://github.com/jcpeterson/openwebtext), an opensource recreation of the WebText dataset used to\n  train GPT-2,\n- [Stories](https://arxiv.org/abs/1806.02847) a dataset containing a subset of CommonCrawl data filtered to match the\n  story-like style of Winograd schemas.\n\nTogether these datasets weigh 160GB of text.\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of\nthe model take pieces of 512 contiguous tokens that may span over documents. The beginning of a new document is marked\nwith `<s>` and the end of one by `</s>`\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `<mask>`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\nContrary to BERT, the masking is done dynamically during pretraining (e.g., it changes at each epoch and is not fixed).\n\n### Pretraining\n\nThe model was trained on 1024 V100 GPUs for 500K steps with a batch size of 8K and a sequence length of 512. The\noptimizer used is Adam with a learning rate of 6e-4, \\\\(\\beta_{1} = 0.9\\\\), \\\\(\\beta_{2} = 0.98\\\\) and\n\\\\(\\epsilon = 1e-6\\\\), a weight decay of 0.01, learning rate warmup for 24,000 steps and linear decay of the learning\nrate after.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |\n|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|\n|      | 87.6 | 91.9 | 92.8 | 94.8  | 63.6 | 91.2  | 90.2 | 78.7 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1907-11692,\n  author    = {Yinhan Liu and\n               Myle Ott and\n               Naman Goyal and\n               Jingfei Du and\n               Mandar Joshi and\n               Danqi Chen and\n               Omer Levy and\n               Mike Lewis and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\n  journal   = {CoRR},\n  volume    = {abs/1907.11692},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1907.11692},\n  archivePrefix = {arXiv},\n  eprint    = {1907.11692},\n  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n<a href=\"https://huggingface.co/exbert/?model=roberta-base\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n", "files": [{"path": ".gitattributes", "size": 445}, {"path": "README.md", "size": 9079}, {"path": "config.json", "size": 481}, {"path": "dict.txt", "size": 603290}, {"path": "flax_model.msgpack", "size": 498796983}, {"path": "merges.txt", "size": 456318}, {"path": "model.safetensors", "size": 498818054}, {"path": "pytorch_model.bin", "size": 501200538}, {"path": "rust_model.ot", "size": 655617312}, {"path": "tf_model.h5", "size": 657434796}, {"path": "tokenizer.json", "size": 1355863}, {"path": "tokenizer_config.json", "size": 25}, {"path": "vocab.json", "size": 898823}], "size": 2815192007, "datasets": ["bookcorpus", "wikipedia"], "github_links": ["https://github.com/jcpeterson/openwebtext", "https://github.com/pytorch/fairseq/tree/master/examples/roberta"], "_source": {"fetched_at": 1761677186.4064908, "last_modified": "2024-02-19T12:39:28+00:00"}}, "fetched_at": 1761677186.4064908}, "model:facebook/bart-base": {"payload": {"url": "https://huggingface.co/facebook/bart-base", "repo_id": "facebook/bart-base", "repo_type": "model", "license": "apache-2.0", "downloads": 1608491, "likes": 199, "last_modified": "2022-11-16T23:23:10+00:00", "tags": ["transformers", "pytorch", "tf", "jax", "safetensors", "bart", "feature-extraction", "en", "arxiv:1910.13461", "license:apache-2.0", "endpoints_compatible", "region:us"], "pipeline_tag": "feature-extraction", "card_yaml": "language: en\nlicense: apache-2.0", "readme_text": "\n# BART (base-sized model) \n\nBART model pre-trained on English language. It was introduced in the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Lewis et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart). \n\nDisclaimer: The team releasing BART did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nBART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n\nBART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).\n\n## Intended uses & limitations\n\nYou can use the raw model for text infilling. However, the model is mostly meant to be fine-tuned on a supervised dataset. See the [model hub](https://huggingface.co/models?search=bart) to look for fine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model in PyTorch:\n\n```python\nfrom transformers import BartTokenizer, BartModel\n\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\nmodel = BartModel.from_pretrained('facebook/bart-base')\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n```\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1910-13461,\n  author    = {Mike Lewis and\n               Yinhan Liu and\n               Naman Goyal and\n               Marjan Ghazvininejad and\n               Abdelrahman Mohamed and\n               Omer Levy and\n               Veselin Stoyanov and\n               Luke Zettlemoyer},\n  title     = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language\n               Generation, Translation, and Comprehension},\n  journal   = {CoRR},\n  volume    = {abs/1910.13461},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1910.13461},\n  eprinttype = {arXiv},\n  eprint    = {1910.13461},\n  timestamp = {Thu, 31 Oct 2019 14:02:26 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```", "files": [{"path": ".gitattributes", "size": 445}, {"path": "README.md", "size": 2593}, {"path": "config.json", "size": 1716}, {"path": "flax_model.msgpack", "size": 557690804}, {"path": "merges.txt", "size": 456318}, {"path": "model.safetensors", "size": 557709915}, {"path": "pytorch_model.bin", "size": 557771387}, {"path": "tf_model.h5", "size": 557950280}, {"path": "tokenizer.json", "size": 1355863}, {"path": "vocab.json", "size": 898823}], "size": 2233838144, "datasets": [], "github_links": ["https://github.com/pytorch/fairseq/tree/master/examples/bart"], "_source": {"fetched_at": 1761677186.717352, "last_modified": "2022-11-16T23:23:10+00:00"}}, "fetched_at": 1761677186.717352}, "model:bert-base-uncased": {"payload": {"url": "https://huggingface.co/bert-base-uncased", "repo_id": "bert-base-uncased", "repo_type": "model", "license": "apache-2.0", "downloads": 55476414, "likes": 2462, "last_modified": "2024-02-19T11:06:12+00:00", "tags": ["transformers", "pytorch", "tf", "jax", "rust", "coreml", "onnx", "safetensors", "bert", "fill-mask", "exbert", "en", "dataset:bookcorpus", "dataset:wikipedia", "arxiv:1810.04805", "license:apache-2.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "pipeline_tag": "fill-mask", "card_yaml": "datasets:\n- bookcorpus\n- wikipedia\nlanguage: en\nlicense: apache-2.0\ntags:\n- exbert", "readme_text": "\n# BERT base model (uncased)\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https://arxiv.org/abs/1810.04805) and first released in\n[this repository](https://github.com/google-research/bert). This model is uncased: it does not make a difference\nbetween english and English.\n\nDisclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it\nwas pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\n\n- Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\n  the entire masked sentence through the model and has to predict the masked words. This is different from traditional\n  recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\n  GPT which internally masks the future tokens. It allows the model to learn a bidirectional representation of the\n  sentence.\n- Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\n  they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\n  predict if the two sentences were following each other or not.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences, for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\n## Model variations\n\nBERT has originally been released in base and large variations, for cased and uncased input text. The uncased models also strips out an accent markers.  \nChinese and multilingual uncased and cased versions followed shortly after.  \nModified preprocessing with whole word masking has replaced subpiece masking in a following work, with the release of two models.  \nOther 24 smaller models are released afterward.  \n\nThe detailed release history can be found on the [google-research/bert readme](https://github.com/google-research/bert/blob/master/README.md) on github.\n\n| Model | #params | Language |\n|------------------------|--------------------------------|-------|\n| [`bert-base-uncased`](https://huggingface.co/bert-base-uncased) | 110M   | English |\n| [`bert-large-uncased`](https://huggingface.co/bert-large-uncased)              | 340M    | English | sub \n| [`bert-base-cased`](https://huggingface.co/bert-base-cased)        | 110M    | English |\n| [`bert-large-cased`](https://huggingface.co/bert-large-cased) | 340M    |  English |\n| [`bert-base-chinese`](https://huggingface.co/bert-base-chinese) | 110M    | Chinese |\n| [`bert-base-multilingual-cased`](https://huggingface.co/bert-base-multilingual-cased) | 110M | Multiple |\n| [`bert-large-uncased-whole-word-masking`](https://huggingface.co/bert-large-uncased-whole-word-masking) | 340M | English |\n| [`bert-large-cased-whole-word-masking`](https://huggingface.co/bert-large-cased-whole-word-masking) | 340M | English |\n\n## Intended uses & limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for\nfine-tuned versions of a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.1073106899857521,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.08774490654468536,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a new model. [SEP]\",\n  'score': 0.05338378623127937,\n  'token': 2047,\n  'token_str': 'new'},\n {'sequence': \"[CLS] hello i'm a super model. [SEP]\",\n  'score': 0.04667217284440994,\n  'token': 3565,\n  'token_str': 'super'},\n {'sequence': \"[CLS] hello i'm a fine model. [SEP]\",\n  'score': 0.027095865458250046,\n  'token': 2986,\n  'token_str': 'fine'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the man worked as a carpenter. [SEP]',\n  'score': 0.09747550636529922,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the man worked as a waiter. [SEP]',\n  'score': 0.0523831807076931,\n  'token': 15610,\n  'token_str': 'waiter'},\n {'sequence': '[CLS] the man worked as a barber. [SEP]',\n  'score': 0.04962705448269844,\n  'token': 13362,\n  'token_str': 'barber'},\n {'sequence': '[CLS] the man worked as a mechanic. [SEP]',\n  'score': 0.03788609802722931,\n  'token': 15893,\n  'token_str': 'mechanic'},\n {'sequence': '[CLS] the man worked as a salesman. [SEP]',\n  'score': 0.037680890411138535,\n  'token': 18968,\n  'token_str': 'salesman'}]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the woman worked as a nurse. [SEP]',\n  'score': 0.21981462836265564,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the woman worked as a waitress. [SEP]',\n  'score': 0.1597415804862976,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the woman worked as a maid. [SEP]',\n  'score': 0.1154729500412941,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the woman worked as a prostitute. [SEP]',\n  'score': 0.037968918681144714,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the woman worked as a cook. [SEP]',\n  'score': 0.03042375110089779,\n  'token': 5660,\n  'token_str': 'cook'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038\nunpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and\nheaders).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus, and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size\nof 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer\nused is Adam with a learning rate of 1e-4, \\\\(\\beta_{1} = 0.9\\\\) and \\\\(\\beta_{2} = 0.999\\\\), a weight decay of 0.01,\nlearning rate warmup for 10,000 steps and linear decay of the learning rate after.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI-(m/mm) | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  | Average |\n|:----:|:-----------:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|:-------:|\n|      | 84.6/83.4   | 71.2 | 90.5 | 93.5  | 52.1 | 85.8  | 88.9 | 66.4 | 79.6    |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n<a href=\"https://huggingface.co/exbert/?model=bert-base-uncased\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n", "files": [{"path": ".gitattributes", "size": 491}, {"path": "LICENSE", "size": 11356}, {"path": "README.md", "size": 10517}, {"path": "config.json", "size": 570}, {"path": "coreml/fill-mask/float32_model.mlpackage/Data/com.apple.CoreML/model.mlmodel", "size": 164911}, {"path": "coreml/fill-mask/float32_model.mlpackage/Data/com.apple.CoreML/weights/weight.bin", "size": 531833856}, {"path": "coreml/fill-mask/float32_model.mlpackage/Manifest.json", "size": 617}, {"path": "flax_model.msgpack", "size": 438064459}, {"path": "model.onnx", "size": 532091246}, {"path": "model.safetensors", "size": 440449768}, {"path": "pytorch_model.bin", "size": 440473133}, {"path": "rust_model.ot", "size": 534240408}, {"path": "tf_model.h5", "size": 536063208}, {"path": "tokenizer.json", "size": 466062}, {"path": "tokenizer_config.json", "size": 48}, {"path": "vocab.txt", "size": 231508}], "size": 3454102158, "datasets": ["bookcorpus", "wikipedia"], "github_links": ["https://github.com/google-research/bert", "https://github.com/google-research/bert/blob/master/README.md"], "_source": {"fetched_at": 1762843093.991481, "last_modified": "2024-02-19T11:06:12+00:00"}}, "fetched_at": 1762843093.991481}, "model:google/gemma-2-2b": {"payload": {"url": "https://huggingface.co/google/gemma-2-2b", "repo_id": "google/gemma-2-2b", "repo_type": "model", "license": "gemma", "downloads": 144221, "likes": 601, "last_modified": "2024-08-07T18:25:49+00:00", "tags": ["transformers", "safetensors", "gemma2", "text-generation", "arxiv:2009.03300", "arxiv:1905.07830", "arxiv:1911.11641", "arxiv:1904.09728", "arxiv:1905.10044", "arxiv:1907.10641", "arxiv:1811.00937", "arxiv:1809.02789", "arxiv:1911.01547", "arxiv:1705.03551", "arxiv:2107.03374", "arxiv:2108.07732", "arxiv:2110.14168", "arxiv:2009.11462", "arxiv:2101.11718", "arxiv:2110.08193", "arxiv:1804.09301", "arxiv:2109.07958", "arxiv:1804.06876", "arxiv:2103.03874", "arxiv:2304.06364", "arxiv:1903.00161", "arxiv:2206.04615", "arxiv:2203.09509", "arxiv:2403.13793", "license:gemma", "autotrain_compatible", "text-generation-inference", "endpoints_compatible", "region:us"], "pipeline_tag": "text-generation", "card_yaml": "library_name: transformers\nlicense: gemma\npipeline_tag: text-generation\nextra_gated_heading: Access Gemma on Hugging Face\nextra_gated_prompt: To access Gemma on Hugging Face, you\u2019re required to review and\n  agree to Google\u2019s usage license. To do this, please ensure you\u2019re logged in to Hugging\n  Face and click below. Requests are processed immediately.\nextra_gated_button_content: Acknowledge license", "readme_text": "\n\n# Gemma 2 model card\n\n**Model Page**: [Gemma](https://ai.google.dev/gemma/docs/base)\n\n**Resources and Technical Documentation**:\n\n* [Responsible Generative AI Toolkit][rai-toolkit]\n* [Gemma on Kaggle][kaggle-gemma]\n* [Gemma on Vertex Model Garden][vertex-mg-gemma2]\n\n**Terms of Use**: [Terms][terms]\n\n**Authors**: Google\n\n## Model Information\n\nSummary description and brief definition of inputs and outputs.\n\n### Description\n\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nThey are text-to-text, decoder-only large language models, available in English,\nwith open weights for both pre-trained variants and instruction-tuned variants.\nGemma models are well-suited for a variety of text generation tasks, including\nquestion answering, summarization, and reasoning. Their relatively small size\nmakes it possible to deploy them in environments with limited resources such as\na laptop, desktop or your own cloud infrastructure, democratizing access to\nstate of the art AI models and helping foster innovation for everyone.\n\n### Usage\n\nBelow we share some code snippets on how to get quickly started with running the model. First, install the Transformers library with:\n```sh\npip install -U transformers\n```\n\nThen, copy the snippet from the section that is relevant for your usecase.\n\n#### Running with the `pipeline` API\n\n```python\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\n    \"text-generation\",\n    model=\"google/gemma-2-2b\",\n    device=\"cuda\",  # replace with \"mps\" to run on a Mac device\n)\n\ntext = \"Once upon a time,\"\noutputs = pipe(text, max_new_tokens=256)\nresponse = outputs[0][\"generated_text\"]\nprint(response)\n```\n\n#### Running the model on a single / multi GPU\n\n```python\n# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-2-2b\",\n    device_map=\"auto\",\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids, max_new_tokens=32)\nprint(tokenizer.decode(outputs[0]))\n```\n\n#### Running the model through a CLI\n\nThe [local-gemma](https://github.com/huggingface/local-gemma) repository contains a lightweight wrapper around Transformers\nfor running Gemma 2 through a command line interface, or CLI. Follow the [installation instructions](https://github.com/huggingface/local-gemma#cli-usage)\nfor getting started, then launch the CLI through the following command:\n\n```shell\nlocal-gemma --model \"google/gemma-2-2b\" --prompt \"What is the capital of Mexico?\"\n```\n\n#### Quantized Versions through `bitsandbytes`\n\n<details>\n  <summary>\n    Using 8-bit precision (int8)  \n  </summary>\n\n```python\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-2-2b\",\n    quantization_config=quantization_config,\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids, max_new_tokens=32)\nprint(tokenizer.decode(outputs[0]))\n```\n</details>\n\n<details>\n  <summary>\n    Using 4-bit precision  \n  </summary>\n\n```python\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-2-2b\",\n    quantization_config=quantization_config,\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids, max_new_tokens=32)\nprint(tokenizer.decode(outputs[0]))\n```\n</details>\n\n#### Advanced Usage\n\n<details>\n  <summary>\n    Torch compile  \n  </summary>\n\n[Torch compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) is a method for speeding-up the \ninference of PyTorch modules. The Gemma-2 2b model can be run up to 6x faster by leveraging torch compile.\n\nNote that two warm-up steps are required before the full inference speed is realised:\n\n```python\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nfrom transformers import AutoTokenizer, Gemma2ForCausalLM\nfrom transformers.cache_utils import HybridCache\nimport torch\n\ntorch.set_float32_matmul_precision(\"high\")\n\n# load the model + tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\nmodel = Gemma2ForCausalLM.from_pretrained(\"google/gemma-2-2b\", torch_dtype=torch.bfloat16)\nmodel.to(\"cuda\")\n\n# apply the torch compile transformation\nmodel.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n\n# pre-process inputs\ninput_text = \"The theory of special relativity states \"\nmodel_inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\nprompt_length = model_inputs.input_ids.shape[1]\n\n# set-up k/v cache\npast_key_values = HybridCache(\n    config=model.config,\n    max_batch_size=1,\n    max_cache_len=model.config.max_position_embeddings,\n    device=model.device,\n    dtype=model.dtype\n)\n\n# enable passing kv cache to generate\nmodel._supports_cache_class = True\nmodel.generation_config.cache_implementation = None\n\n# two warm-up steps\nfor idx in range(2):\n    outputs = model.generate(**model_inputs, past_key_values=past_key_values, do_sample=True, temperature=1.0, max_new_tokens=128)\n    past_key_values.reset()\n\n# fast run\noutputs = model.generate(**model_inputs, past_key_values=past_key_values, do_sample=True, temperature=1.0, max_new_tokens=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\nFor more details, refer to the [Transformers documentation](https://huggingface.co/docs/transformers/main/en/llm_optims?static-kv=basic+usage%3A+generation_config).\n\n</details>\n\n### Inputs and outputs\n\n*   **Input:** Text string, such as a question, a prompt, or a document to be\n    summarized.\n*   **Output:** Generated English-language text in response to the input, such\n    as an answer to a question, or a summary of a document.\n\n### Citation\n\n```none\n@article{gemma_2024,\n    title={Gemma},\n    url={https://www.kaggle.com/m/3301},\n    DOI={10.34740/KAGGLE/M/3301},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2024}\n}\n```\n\n## Model Data\n\nData used for model training and how the data was processed.\n\n### Training Dataset\n\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 13 trillion tokens, the 9B model was\ntrained with 8 trillion tokens, and 2B model was trained with 2 trillion tokens.\nHere are the key components:\n\n* Web Documents: A diverse collection of web text ensures the model is exposed\n  to a broad range of linguistic styles, topics, and vocabulary. Primarily\n  English-language content.\n* Code: Exposing the model to code helps it to learn the syntax and patterns of\n  programming languages, which improves its ability to generate code or\n  understand code-related questions.\n* Mathematics: Training on mathematical text helps the model learn logical\n  reasoning, symbolic representation, and to address mathematical queries.\n\nThe combination of these diverse data sources is crucial for training a powerful\nlanguage model that can handle a wide variety of different tasks and text\nformats.\n\n### Data Preprocessing\n\nHere are the key data cleaning and filtering methods applied to the training\ndata:\n\n* CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was\n  applied at multiple stages in the data preparation process to ensure the\n  exclusion of harmful and illegal content.\n* Sensitive Data Filtering: As part of making Gemma pre-trained models safe and\n  reliable, automated techniques were used to filter out certain personal\n  information and other sensitive data from training sets.\n* Additional methods: Filtering based on content quality and safety in line with\n  [our policies][safety-policies].\n\n## Implementation Information\n\nDetails about the model internals.\n\n### Hardware\n\nGemma was trained using the latest generation of\n[Tensor Processing Unit (TPU)][tpu] hardware (TPUv5p).\n\nTraining large language models requires significant computational power. TPUs,\ndesigned specifically for matrix operations common in machine learning, offer\nseveral advantages in this domain:\n\n* Performance: TPUs are specifically designed to handle the massive computations\n  involved in training LLMs. They can speed up training considerably compared to\n  CPUs.\n* Memory: TPUs often come with large amounts of high-bandwidth memory, allowing\n  for the handling of large models and batch sizes during training. This can\n  lead to better model quality.\n* Scalability: TPU Pods (large clusters of TPUs) provide a scalable solution for\n  handling the growing complexity of large foundation models. You can distribute\n  training across multiple TPU devices for faster and more efficient processing.\n* Cost-effectiveness: In many scenarios, TPUs can provide a more cost-effective\n  solution for training large models compared to CPU-based infrastructure,\n  especially when considering the time and resources saved due to faster\n  training.\n* These advantages are aligned with\n  [Google's commitments to operate sustainably][sustainability].\n\n### Software\n\nTraining was done using [JAX][jax] and [ML Pathways][ml-pathways].\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\n\nML Pathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\n[foundation models][foundation-models], including large language models like\nthese ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models][gemini-2-paper]; \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\n\n## Evaluation\n\nModel evaluation metrics and results.\n\n### Benchmark Results\n\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\n\n| Benchmark                      | Metric        | Gemma 2 PT 2B | Gemma 2 PT 9B | Gemma 2 PT 27B |\n| ------------------------------ | ------------- | ------------- | ------------- | -------------- |\n| [MMLU][mmlu]                   | 5-shot, top-1 | 51.3          | 71.3          | 75.2           |\n| [HellaSwag][hellaswag]         | 10-shot       | 73.0          | 81.9          | 86.4           |\n| [PIQA][piqa]                   | 0-shot        | 77.8          | 81.7          | 83.2           |\n| [SocialIQA][socialiqa]         | 0-shot        | 51.9          | 53.4          | 53.7           |\n| [BoolQ][boolq]                 | 0-shot        | 72.5          | 84.2          | 84.8           |\n| [WinoGrande][winogrande]       | partial score | 70.9          | 80.6          | 83.7           |\n| [ARC-e][arc]                   | 0-shot        | 80.1          | 88.0          | 88.6           |\n| [ARC-c][arc]                   | 25-shot       | 55.4          | 68.4          | 71.4           |\n| [TriviaQA][triviaqa]           | 5-shot        | 59.4          | 76.6          | 83.7           |\n| [Natural Questions][naturalq]  | 5-shot        | 16.7          | 29.2          | 34.5           |\n| [HumanEval][humaneval]         | pass@1        | 17.7          | 40.2          | 51.8           |\n| [MBPP][mbpp]                   | 3-shot        | 29.6          | 52.4          | 62.6           |\n| [GSM8K][gsm8k]                 | 5-shot, maj@1 | 23.9          | 68.6          | 74.0           |\n| [MATH][math]                   | 4-shot        | 15.0          | 36.6          | 42.3           |\n| [AGIEval][agieval]             | 3-5-shot      | 30.6          | 52.8          | 55.1           |\n| [DROP][drop]                   | 3-shot, F1    | 52.0          | 69.4          | 72.2           |\n| [BIG-Bench][big-bench]         | 3-shot, CoT   | 41.9          | 68.2          | 74.9           |\n\n## Ethics and Safety\n\nEthics and safety evaluation approach and results.\n\n### Evaluation Approach\n\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\n\n* Text-to-Text Content Safety: Human evaluation on prompts covering safety\n  policies including child sexual abuse and exploitation, harassment, violence\n  and gore, and hate speech.\n* Text-to-Text Representational Harms: Benchmark against relevant academic\n  datasets such as [WinoBias][winobias] and [BBQ Dataset][bbq].\n* Memorization: Automated evaluation of memorization of training data, including\n  the risk of personally identifiable information exposure.\n* Large-scale harm: Tests for \"dangerous capabilities,\" such as chemical,\n  biological, radiological, and nuclear (CBRN) risks.\n\n### Evaluation Results\n\nThe results of ethics and safety evaluations are within acceptable thresholds\nfor meeting [internal policies][safety-policies] for categories such as child\nsafety, content safety, representational harms, memorization, large-scale harms.\nOn top of robust internal evaluations, the results of well-known safety\nbenchmarks like BBQ, BOLD, Winogender, Winobias, RealToxicity, and TruthfulQA\nare shown here.\n\n#### Gemma 2.0\n\n| Benchmark                | Metric        | Gemma 2 IT 2B | Gemma 2 IT 9B | Gemma 2 IT 27B |\n| ------------------------ | ------------- | ------------- | ------------- | -------------- |\n| [RealToxicity][realtox]  | average       |  8.16         |  8.25         |  8.84          |\n| [CrowS-Pairs][crows]     | top-1         | 37.67         | 37.47         | 36.67          |\n| [BBQ Ambig][bbq]         | 1-shot, top-1 | 83.20         | 88.58         | 85.99          |\n| [BBQ Disambig][bbq]      | top-1         | 69.31         | 82.67         | 86.94          |\n| [Winogender][winogender] | top-1         | 52.91         | 79.17         | 77.22          |\n| [TruthfulQA][truthfulqa] |               | 43.72         | 50.27         | 51.60          |\n| [Winobias 1_2][winobias] |               | 59.28         | 78.09         | 81.94          |\n| [Winobias 2_2][winobias] |               | 88.57         | 95.32         | 97.22          |\n| [Toxigen][toxigen]       |               | 48.32         | 39.30         | 38.42          |\n\n## Dangerous Capability Evaluations\n\n### Evaluation Approach\n\nWe evaluated a range of dangerous capabilities:\n\n-   **Offensive cybersecurity:** To assess the model's potential for misuse in\n    cybersecurity contexts, we utilized both publicly available\n    Capture-the-Flag (CTF) platforms like InterCode-CTF and Hack the Box, as\n    well as internally developed CTF challenges. These evaluations measure the\n    model's ability to exploit vulnerabilities and gain unauthorized access in\n    simulated environments.\n-   **Self-proliferation:** We evaluated the model's capacity for\n    self-proliferation by designing tasks that involve resource acquisition, code\n    execution, and interaction with remote systems. These evaluations assess\n    the model's ability to independently replicate and spread.\n-   **Persuasion:** To evaluate the model's capacity for persuasion and\n    deception, we conducted human persuasion studies. These studies involved\n    scenarios that measure the model's ability to build rapport, influence\n    beliefs, and elicit specific actions from human participants.\n\n### Evaluation Results\n\nAll evaluations are described in detail in\n[Evaluating Frontier Models for Dangerous Capabilities][eval-danger]\nand in brief in the\n[Gemma 2 technical report][tech-report].\n\n<table>\n  <thead>\n    <tr>\n      <th>Evaluation</th>\n      <th>Capability</th>\n      <th>Gemma 2 IT 27B</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>InterCode-CTF</td>\n      <td>Offensive cybersecurity</td>\n      <td>34/76 challenges</td>\n    </tr>\n    <tr>\n      <td>Internal CTF</td>\n      <td>Offensive cybersecurity</td>\n      <td>1/13 challenges</td>\n    </tr>\n    <tr>\n      <td>Hack the Box</td>\n      <td>Offensive cybersecurity</td>\n      <td>0/13 challenges</td>\n    </tr>\n    <tr>\n      <td>Self-proliferation early warning</td>\n      <td>Self-proliferation</td>\n      <td>1/10 challenges</td>\n    </tr>\n    <tr>\n      <td>Charm offensive</td>\n      <td>Persuasion</td>\n      <td>Percent of participants agreeing:\n        81% interesting,\n        75% would speak again,\n        80% made personal connection</td>\n    </tr>\n    <tr>\n      <td>Click Links</td>\n      <td>Persuasion</td>\n      <td>34% of participants</td>\n    </tr>\n    <tr>\n      <td>Find Info</td>\n      <td>Persuasion</td>\n      <td>9% of participants</td>\n    </tr>\n    <tr>\n      <td>Run Code</td>\n      <td>Persuasion</td>\n      <td>11% of participants</td>\n    </tr>\n    <tr>\n      <td>Money talks</td>\n      <td>Persuasion</td>\n      <td>\u00a33.72 mean donation</td>\n    </tr>\n    <tr>\n      <td>Web of Lies</td>\n      <td>Persuasion</td>\n      <td>18% mean shift towards correct belief, 1% mean shift towards\nincorrect belief</td>\n    </tr>\n  </tbody>\n</table>\n\n## Usage and Limitations\n\nThese models have certain limitations that users should be aware of.\n\n### Intended Usage\n\nOpen Large Language Models (LLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n* Content Creation and Communication\n  * Text Generation: These models can be used to generate creative text formats\n    such as poems, scripts, code, marketing copy, and email drafts.\n  * Chatbots and Conversational AI: Power conversational interfaces for customer\n    service, virtual assistants, or interactive applications.\n  * Text Summarization: Generate concise summaries of a text corpus, research\n    papers, or reports.\n* Research and Education\n  * Natural Language Processing (NLP) Research: These models can serve as a\n    foundation for researchers to experiment with NLP techniques, develop\n    algorithms, and contribute to the advancement of the field.\n  * Language Learning Tools: Support interactive language learning experiences,\n    aiding in grammar correction or providing writing practice.\n  * Knowledge Exploration: Assist researchers in exploring large bodies of text\n    by generating summaries or answering questions about specific topics.\n\n### Limitations\n\n* Training Data\n  * The quality and diversity of the training data significantly influence the\n    model's capabilities. Biases or gaps in the training data can lead to\n    limitations in the model's responses.\n  * The scope of the training dataset determines the subject areas the model can\n    handle effectively.\n* Context and Task Complexity\n  * LLMs are better at tasks that can be framed with clear prompts and\n    instructions. Open-ended or highly complex tasks might be challenging.\n  * A model's performance can be influenced by the amount of context provided\n    (longer context generally leads to better outputs, up to a certain point).\n* Language Ambiguity and Nuance\n  * Natural language is inherently complex. LLMs might struggle to grasp subtle\n    nuances, sarcasm, or figurative language.\n* Factual Accuracy\n  * LLMs generate responses based on information they learned from their\n    training datasets, but they are not knowledge bases. They may generate\n    incorrect or outdated factual statements.\n* Common Sense\n  * LLMs rely on statistical patterns in language. They might lack the ability\n    to apply common sense reasoning in certain situations.\n\n### Ethical Considerations and Risks\n\nThe development of large language models (LLMs) raises several ethical concerns.\nIn creating an open model, we have carefully considered the following:\n\n* Bias and Fairness\n  * LLMs trained on large-scale, real-world text data can reflect socio-cultural\n    biases embedded in the training material. These models underwent careful\n    scrutiny, input data pre-processing described and posterior evaluations\n    reported in this card.\n* Misinformation and Misuse\n  * LLMs can be misused to generate text that is false, misleading, or harmful.\n  * Guidelines are provided for responsible use with the model, see the\n    [Responsible Generative AI Toolkit][rai-toolkit].\n* Transparency and Accountability:\n  * This model card summarizes details on the models' architecture,\n    capabilities, limitations, and evaluation processes.\n  * A responsibly developed open model offers the opportunity to share\n    innovation by making LLM technology accessible to developers and researchers\n    across the AI ecosystem.\n\nRisks identified and mitigations:\n\n* Perpetuation of biases: It's encouraged to perform continuous monitoring\n  (using evaluation metrics, human review) and the exploration of de-biasing\n  techniques during model training, fine-tuning, and other use cases.\n* Generation of harmful content: Mechanisms and guidelines for content safety\n  are essential. Developers are encouraged to exercise caution and implement\n  appropriate content safety safeguards based on their specific product policies\n  and application use cases.\n* Misuse for malicious purposes: Technical limitations and developer and\n  end-user education can help mitigate against malicious applications of LLMs.\n  Educational resources and reporting mechanisms for users to flag misuse are\n  provided. Prohibited uses of Gemma models are outlined in the\n  [Gemma Prohibited Use Policy][prohibited-use].\n* Privacy violations: Models were trained on data filtered for removal of PII\n  (Personally Identifiable Information). Developers are encouraged to adhere to\n  privacy regulations with privacy-preserving techniques.\n\n### Benefits\n\nAt the time of release, this family of models provides high-performance open\nlarge language model implementations designed from the ground up for Responsible\nAI development compared to similarly sized models.\n\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.\n\n[tech-report]: https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf\n[rai-toolkit]: https://ai.google.dev/responsible\n[kaggle-gemma]: https://www.kaggle.com/models/google/gemma-2\n[terms]: https://ai.google.dev/gemma/terms\n[vertex-mg-gemma2]: https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma2\n[sensitive-info]: https://cloud.google.com/dlp/docs/high-sensitivity-infotypes-reference\n[safety-policies]: https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_Google_AI_Principles_Progress_Update.pdf#page=11\n[prohibited-use]: https://ai.google.dev/gemma/prohibited_use_policy\n[tpu]: https://cloud.google.com/tpu/docs/intro-to-tpu\n[sustainability]: https://sustainability.google/operating-sustainably/\n[jax]: https://github.com/google/jax\n[ml-pathways]: https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/\n[sustainability]: https://sustainability.google/operating-sustainably/\n[foundation-models]: https://ai.google/discover/foundation-models/\n[gemini-2-paper]: https://goo.gle/gemma2report\n[mmlu]: https://arxiv.org/abs/2009.03300\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[piqa]: https://arxiv.org/abs/1911.11641\n[socialiqa]: https://arxiv.org/abs/1904.09728\n[boolq]: https://arxiv.org/abs/1905.10044\n[winogrande]: https://arxiv.org/abs/1907.10641\n[commonsenseqa]: https://arxiv.org/abs/1811.00937\n[openbookqa]: https://arxiv.org/abs/1809.02789\n[arc]: https://arxiv.org/abs/1911.01547\n[triviaqa]: https://arxiv.org/abs/1705.03551\n[naturalq]: https://github.com/google-research-datasets/natural-questions\n[humaneval]: https://arxiv.org/abs/2107.03374\n[mbpp]: https://arxiv.org/abs/2108.07732\n[gsm8k]: https://arxiv.org/abs/2110.14168\n[realtox]: https://arxiv.org/abs/2009.11462\n[bold]: https://arxiv.org/abs/2101.11718\n[crows]: https://aclanthology.org/2020.emnlp-main.154/\n[bbq]: https://arxiv.org/abs/2110.08193v2\n[winogender]: https://arxiv.org/abs/1804.09301\n[truthfulqa]: https://arxiv.org/abs/2109.07958\n[winobias]: https://arxiv.org/abs/1804.06876\n[math]: https://arxiv.org/abs/2103.03874\n[agieval]: https://arxiv.org/abs/2304.06364\n[drop]: https://arxiv.org/abs/1903.00161\n[big-bench]: https://arxiv.org/abs/2206.04615\n[toxigen]: https://arxiv.org/abs/2203.09509\n[eval-danger]: https://arxiv.org/abs/2403.13793\n", "files": [{"path": ".gitattributes", "size": 1570}, {"path": "README.md", "size": 25752}, {"path": "config.json", "size": 818}, {"path": "generation_config.json", "size": 168}, {"path": "model-00001-of-00003.safetensors", "size": 4992576136}, {"path": "model-00002-of-00003.safetensors", "size": 4983443424}, {"path": "model-00003-of-00003.safetensors", "size": 481381384}, {"path": "model.safetensors.index.json", "size": 24224}, {"path": "special_tokens_map.json", "size": 636}, {"path": "tokenizer.json", "size": 17525357}, {"path": "tokenizer.model", "size": 4241003}, {"path": "tokenizer_config.json", "size": 46379}], "size": 10479266851, "datasets": [], "github_links": ["https://github.com/google-research-datasets/natural-questions", "https://github.com/google/jax", "https://github.com/huggingface/local-gemma"], "_source": {"fetched_at": 1762387335.192626, "last_modified": "2024-08-07T18:25:49+00:00"}}, "fetched_at": 1762387335.192626}, "model:FacebookAI/roberta-base": {"payload": {"url": "https://huggingface.co/FacebookAI/roberta-base", "repo_id": "FacebookAI/roberta-base", "repo_type": "model", "license": "mit", "downloads": 12932911, "likes": 531, "last_modified": "2024-02-19T12:39:28+00:00", "tags": ["transformers", "pytorch", "tf", "jax", "rust", "safetensors", "roberta", "fill-mask", "exbert", "en", "dataset:bookcorpus", "dataset:wikipedia", "arxiv:1907.11692", "arxiv:1806.02847", "license:mit", "autotrain_compatible", "endpoints_compatible", "region:us"], "pipeline_tag": "fill-mask", "card_yaml": "datasets:\n- bookcorpus\n- wikipedia\nlanguage: en\nlicense: mit\ntags:\n- exbert", "readme_text": "\n# RoBERTa base model\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https://arxiv.org/abs/1907.11692) and first released in\n[this repository](https://github.com/pytorch/fairseq/tree/master/examples/roberta). This model is case-sensitive: it\nmakes a difference between english and English.\n\nDisclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nRoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means\nit was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. \n\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model\nrandomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict\nthe masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one\nafter the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to\nlearn a bidirectional representation of the sentence.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\n## Intended uses & limitations\n\nYou can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task.\nSee the [model hub](https://huggingface.co/models?filter=roberta) to look for fine-tuned versions on a task that\ninterests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at a model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='roberta-base')\n>>> unmasker(\"Hello I'm a <mask> model.\")\n\n[{'sequence': \"<s>Hello I'm a male model.</s>\",\n  'score': 0.3306540250778198,\n  'token': 2943,\n  'token_str': '\u0120male'},\n {'sequence': \"<s>Hello I'm a female model.</s>\",\n  'score': 0.04655390977859497,\n  'token': 2182,\n  'token_str': '\u0120female'},\n {'sequence': \"<s>Hello I'm a professional model.</s>\",\n  'score': 0.04232972860336304,\n  'token': 2038,\n  'token_str': '\u0120professional'},\n {'sequence': \"<s>Hello I'm a fashion model.</s>\",\n  'score': 0.037216778844594955,\n  'token': 2734,\n  'token_str': '\u0120fashion'},\n {'sequence': \"<s>Hello I'm a Russian model.</s>\",\n  'score': 0.03253649175167084,\n  'token': 1083,\n  'token_str': '\u0120Russian'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import RobertaTokenizer, RobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaModel.from_pretrained('roberta-base')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import RobertaTokenizer, TFRobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = TFRobertaModel.from_pretrained('roberta-base')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nThe training data used for this model contains a lot of unfiltered content from the internet, which is far from\nneutral. Therefore, the model can have biased predictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='roberta-base')\n>>> unmasker(\"The man worked as a <mask>.\")\n\n[{'sequence': '<s>The man worked as a mechanic.</s>',\n  'score': 0.08702439814805984,\n  'token': 25682,\n  'token_str': '\u0120mechanic'},\n {'sequence': '<s>The man worked as a waiter.</s>',\n  'score': 0.0819653645157814,\n  'token': 38233,\n  'token_str': '\u0120waiter'},\n {'sequence': '<s>The man worked as a butcher.</s>',\n  'score': 0.073323555290699,\n  'token': 32364,\n  'token_str': '\u0120butcher'},\n {'sequence': '<s>The man worked as a miner.</s>',\n  'score': 0.046322137117385864,\n  'token': 18678,\n  'token_str': '\u0120miner'},\n {'sequence': '<s>The man worked as a guard.</s>',\n  'score': 0.040150221437215805,\n  'token': 2510,\n  'token_str': '\u0120guard'}]\n\n>>> unmasker(\"The Black woman worked as a <mask>.\")\n\n[{'sequence': '<s>The Black woman worked as a waitress.</s>',\n  'score': 0.22177888453006744,\n  'token': 35698,\n  'token_str': '\u0120waitress'},\n {'sequence': '<s>The Black woman worked as a prostitute.</s>',\n  'score': 0.19288744032382965,\n  'token': 36289,\n  'token_str': '\u0120prostitute'},\n {'sequence': '<s>The Black woman worked as a maid.</s>',\n  'score': 0.06498628109693527,\n  'token': 29754,\n  'token_str': '\u0120maid'},\n {'sequence': '<s>The Black woman worked as a secretary.</s>',\n  'score': 0.05375480651855469,\n  'token': 2971,\n  'token_str': '\u0120secretary'},\n {'sequence': '<s>The Black woman worked as a nurse.</s>',\n  'score': 0.05245552211999893,\n  'token': 9008,\n  'token_str': '\u0120nurse'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe RoBERTa model was pretrained on the reunion of five datasets:\n- [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books;\n- [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers) ;\n- [CC-News](https://commoncrawl.org/2016/10/news-dataset-available/), a dataset containing 63 millions English news\n  articles crawled between September 2016 and February 2019.\n- [OpenWebText](https://github.com/jcpeterson/openwebtext), an opensource recreation of the WebText dataset used to\n  train GPT-2,\n- [Stories](https://arxiv.org/abs/1806.02847) a dataset containing a subset of CommonCrawl data filtered to match the\n  story-like style of Winograd schemas.\n\nTogether these datasets weigh 160GB of text.\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of\nthe model take pieces of 512 contiguous tokens that may span over documents. The beginning of a new document is marked\nwith `<s>` and the end of one by `</s>`\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `<mask>`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\nContrary to BERT, the masking is done dynamically during pretraining (e.g., it changes at each epoch and is not fixed).\n\n### Pretraining\n\nThe model was trained on 1024 V100 GPUs for 500K steps with a batch size of 8K and a sequence length of 512. The\noptimizer used is Adam with a learning rate of 6e-4, \\\\(\\beta_{1} = 0.9\\\\), \\\\(\\beta_{2} = 0.98\\\\) and\n\\\\(\\epsilon = 1e-6\\\\), a weight decay of 0.01, learning rate warmup for 24,000 steps and linear decay of the learning\nrate after.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |\n|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|\n|      | 87.6 | 91.9 | 92.8 | 94.8  | 63.6 | 91.2  | 90.2 | 78.7 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1907-11692,\n  author    = {Yinhan Liu and\n               Myle Ott and\n               Naman Goyal and\n               Jingfei Du and\n               Mandar Joshi and\n               Danqi Chen and\n               Omer Levy and\n               Mike Lewis and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\n  journal   = {CoRR},\n  volume    = {abs/1907.11692},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1907.11692},\n  archivePrefix = {arXiv},\n  eprint    = {1907.11692},\n  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n<a href=\"https://huggingface.co/exbert/?model=roberta-base\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n", "files": [{"path": ".gitattributes", "size": 445}, {"path": "README.md", "size": 9079}, {"path": "config.json", "size": 481}, {"path": "dict.txt", "size": 603290}, {"path": "flax_model.msgpack", "size": 498796983}, {"path": "merges.txt", "size": 456318}, {"path": "model.safetensors", "size": 498818054}, {"path": "pytorch_model.bin", "size": 501200538}, {"path": "rust_model.ot", "size": 655617312}, {"path": "tf_model.h5", "size": 657434796}, {"path": "tokenizer.json", "size": 1355863}, {"path": "tokenizer_config.json", "size": 25}, {"path": "vocab.json", "size": 898823}], "size": 2815192007, "datasets": ["bookcorpus", "wikipedia"], "github_links": ["https://github.com/jcpeterson/openwebtext", "https://github.com/pytorch/fairseq/tree/master/examples/roberta"], "_source": {"fetched_at": 1762540226.3263009, "last_modified": "2024-02-19T12:39:28+00:00"}}, "fetched_at": 1762540226.3263009}, "model:distilbert-base-uncased": {"payload": {"url": "https://huggingface.co/distilbert-base-uncased", "repo_id": "distilbert-base-uncased", "repo_type": "model", "license": "apache-2.0", "downloads": 15100202, "likes": 786, "last_modified": "2024-05-06T13:44:53+00:00", "tags": ["transformers", "pytorch", "tf", "jax", "rust", "safetensors", "distilbert", "fill-mask", "exbert", "en", "dataset:bookcorpus", "dataset:wikipedia", "arxiv:1910.01108", "license:apache-2.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "pipeline_tag": "fill-mask", "card_yaml": "datasets:\n- bookcorpus\n- wikipedia\nlanguage: en\nlicense: apache-2.0\ntags:\n- exbert", "readme_text": "\n# DistilBERT base model (uncased)\n\nThis model is a distilled version of the [BERT base model](https://huggingface.co/bert-base-uncased). It was\nintroduced in [this paper](https://arxiv.org/abs/1910.01108). The code for the distillation process can be found\n[here](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation). This model is uncased: it does\nnot make a difference between english and English.\n\n## Model description\n\nDistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a\nself-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only,\nwith no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic\nprocess to generate inputs and labels from those texts using the BERT base model. More precisely, it was pretrained\nwith three objectives:\n\n- Distillation loss: the model was trained to return the same probabilities as the BERT base model.\n- Masked language modeling (MLM): this is part of the original training loss of the BERT base model. When taking a\n  sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the\n  model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that\n  usually see the words one after the other, or from autoregressive models like GPT which internally mask the future\n  tokens. It allows the model to learn a bidirectional representation of the sentence.\n- Cosine embedding loss: the model was also trained to generate hidden states as close as possible as the BERT base\n  model.\n\nThis way, the model learns the same inner representation of the English language than its teacher model, while being\nfaster for inference or downstream tasks.\n\n## Intended uses & limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=distilbert) to look for\nfine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.05292855575680733,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.03968575969338417,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a business model. [SEP]\",\n  'score': 0.034743521362543106,\n  'token': 2449,\n  'token_str': 'business'},\n {'sequence': \"[CLS] hello i'm a model model. [SEP]\",\n  'score': 0.03462274372577667,\n  'token': 2944,\n  'token_str': 'model'},\n {'sequence': \"[CLS] hello i'm a modeling model. [SEP]\",\n  'score': 0.018145186826586723,\n  'token': 11643,\n  'token_str': 'modeling'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import DistilBertTokenizer, DistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import DistilBertTokenizer, TFDistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions. It also inherits some of\n[the bias of its teacher model](https://huggingface.co/bert-base-uncased#limitations-and-bias).\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"The White man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the white man worked as a blacksmith. [SEP]',\n  'score': 0.1235365942120552,\n  'token': 20987,\n  'token_str': 'blacksmith'},\n {'sequence': '[CLS] the white man worked as a carpenter. [SEP]',\n  'score': 0.10142576694488525,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the white man worked as a farmer. [SEP]',\n  'score': 0.04985016956925392,\n  'token': 7500,\n  'token_str': 'farmer'},\n {'sequence': '[CLS] the white man worked as a miner. [SEP]',\n  'score': 0.03932540491223335,\n  'token': 18594,\n  'token_str': 'miner'},\n {'sequence': '[CLS] the white man worked as a butcher. [SEP]',\n  'score': 0.03351764753460884,\n  'token': 14998,\n  'token_str': 'butcher'}]\n\n>>> unmasker(\"The Black woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the black woman worked as a waitress. [SEP]',\n  'score': 0.13283951580524445,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the black woman worked as a nurse. [SEP]',\n  'score': 0.12586183845996857,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the black woman worked as a maid. [SEP]',\n  'score': 0.11708822101354599,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the black woman worked as a prostitute. [SEP]',\n  'score': 0.11499975621700287,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the black woman worked as a housekeeper. [SEP]',\n  'score': 0.04722772538661957,\n  'token': 22583,\n  'token_str': 'housekeeper'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nDistilBERT pretrained on the same data as BERT, which is [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset\nconsisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia)\n(excluding lists, tables and headers).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 8 16 GB V100 for 90 hours. See the\n[training code](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for all hyperparameters\ndetails.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |\n|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|\n|      | 82.2 | 88.5 | 89.2 | 91.3  | 51.3 | 85.8  | 87.5 | 59.9 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{Sanh2019DistilBERTAD,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1910.01108}\n}\n```\n\n<a href=\"https://huggingface.co/exbert/?model=distilbert-base-uncased\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n", "files": [{"path": ".gitattributes", "size": 445}, {"path": "LICENSE", "size": 11356}, {"path": "README.md", "size": 8577}, {"path": "config.json", "size": 483}, {"path": "flax_model.msgpack", "size": 267945836}, {"path": "model.safetensors", "size": 267954768}, {"path": "pytorch_model.bin", "size": 267967963}, {"path": "rust_model.ot", "size": 361732396}, {"path": "tf_model.h5", "size": 363423424}, {"path": "tokenizer.json", "size": 466062}, {"path": "tokenizer_config.json", "size": 48}, {"path": "vocab.txt", "size": 231508}], "size": 1529742866, "datasets": ["bookcorpus", "wikipedia"], "github_links": ["https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation"], "_source": {"fetched_at": 1762842868.343068, "last_modified": "2024-05-06T13:44:53+00:00"}}, "fetched_at": 1762842868.343068}, "model:google/flan-t5-small": {"payload": {"url": "https://huggingface.co/google/flan-t5-small", "repo_id": "google/flan-t5-small", "repo_type": "model", "license": "apache-2.0", "downloads": 489923, "likes": 444, "last_modified": "2023-10-10T18:01:54+00:00", "tags": ["transformers", "pytorch", "tf", "jax", "safetensors", "t5", "text2text-generation", "en", "fr", "ro", "de", "multilingual", "dataset:svakulenk0/qrecc", "dataset:taskmaster2", "dataset:djaym7/wiki_dialog", "dataset:deepmind/code_contests", "dataset:lambada", "dataset:gsm8k", "dataset:aqua_rat", "dataset:esnli", "dataset:quasc", "dataset:qed", "arxiv:2210.11416", "arxiv:1910.09700", "license:apache-2.0", "text-generation-inference", "endpoints_compatible", "region:us"], "pipeline_tag": null, "card_yaml": "datasets:\n- svakulenk0/qrecc\n- taskmaster2\n- djaym7/wiki_dialog\n- deepmind/code_contests\n- lambada\n- gsm8k\n- aqua_rat\n- esnli\n- quasc\n- qed\nlanguage:\n- en\n- fr\n- ro\n- de\n- multilingual\nlicense: apache-2.0\ntags:\n- text2text-generation\nwidget:\n- text: 'Translate to German:  My name is Arthur'\n  example_title: Translation\n- text: Please answer to the following question. Who is going to be the next Ballon\n    d'or?\n  example_title: Question Answering\n- text: 'Q: Can Geoffrey Hinton have a conversation with George Washington? Give the\n    rationale before answering.'\n  example_title: Logical reasoning\n- text: Please answer the following question. What is the boiling point of Nitrogen?\n  example_title: Scientific knowledge\n- text: Answer the following yes/no question. Can you write a whole Haiku in a single\n    tweet?\n  example_title: Yes/no question\n- text: Answer the following yes/no question by reasoning step-by-step. Can you write\n    a whole Haiku in a single tweet?\n  example_title: Reasoning task\n- text: 'Q: ( False or not False or False ) is? A: Let''s think step by step'\n  example_title: Boolean Expressions\n- text: The square root of x is the cube root of y. What is y to the power of 2, if\n    x = 4?\n  example_title: Math reasoning\n- text: 'Premise:  At my age you will probably have learnt one lesson. Hypothesis:  It''s\n    not certain how many lessons you''ll learn by your thirties. Does the premise\n    entail the hypothesis?'\n  example_title: Premise and hypothesis", "readme_text": "\n# Model Card for FLAN-T5 small\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/flan2_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n#  Table of Contents\n\n0. [TL;DR](#TL;DR)\n1. [Model Details](#model-details)\n2. [Usage](#usage)\n3. [Uses](#uses)\n4. [Bias, Risks, and Limitations](#bias-risks-and-limitations)\n5. [Training Details](#training-details)\n6. [Evaluation](#evaluation)\n7. [Environmental Impact](#environmental-impact)\n8. [Citation](#citation)\n9. [Model Card Authors](#model-card-authors)\n\n# TL;DR\n\nIf you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. \nAs mentioned in the first few lines of the abstract : \n>  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.\n\n**Disclaimer**: Content from **this** model card has been written by the Hugging Face team, and parts of it were copy pasted from the [T5 model card](https://huggingface.co/t5-large).\n\n# Model Details\n\n## Model Description\n\n\n- **Model type:** Language model\n- **Language(s) (NLP):** English, Spanish, Japanese, Persian, Hindi, French, Chinese, Bengali, Gujarati, German, Telugu, Italian, Arabic, Polish, Tamil, Marathi, Malayalam, Oriya, Panjabi, Portuguese, Urdu, Galician, Hebrew, Korean, Catalan, Thai, Dutch, Indonesian, Vietnamese, Bulgarian, Filipino, Central Khmer, Lao, Turkish, Russian, Croatian, Swedish, Yoruba, Kurdish, Burmese, Malay, Czech, Finnish, Somali, Tagalog, Swahili, Sinhala, Kannada, Zhuang, Igbo, Xhosa, Romanian, Haitian, Estonian, Slovak, Lithuanian, Greek, Nepali, Assamese, Norwegian\n- **License:** Apache 2.0\n- **Related Models:** [All FLAN-T5 Checkpoints](https://huggingface.co/models?search=flan-t5)\n- **Original Checkpoints:** [All Original FLAN-T5 Checkpoints](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)\n- **Resources for more information:**\n  - [Research paper](https://arxiv.org/pdf/2210.11416.pdf)\n  - [GitHub Repo](https://github.com/google-research/t5x)\n  - [Hugging Face FLAN-T5 Docs (Similar to T5) ](https://huggingface.co/docs/transformers/model_doc/t5)\n\n# Usage\n\nFind below some example scripts on how to use the model in `transformers`:\n\n## Using the Pytorch model\n\n### Running the model on a CPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n### Running the model on a GPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", device_map=\"auto\")\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n### Running the model on a GPU using different precisions\n\n#### FP16\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install accelerate\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", device_map=\"auto\", torch_dtype=torch.float16)\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n#### INT8\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install bitsandbytes accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", device_map=\"auto\", load_in_8bit=True)\n\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n# Uses\n\n## Direct Use and Downstream Use\n\nThe authors write in [the original paper's model card](https://arxiv.org/pdf/2210.11416.pdf) that: \n\n> The primary use is research on language models, including: research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering; advancing fairness and safety research, and understanding limitations of current large language models\n\nSee the [research paper](https://arxiv.org/pdf/2210.11416.pdf) for further details.\n\n## Out-of-Scope Use\n\nMore information needed.\n\n# Bias, Risks, and Limitations\n\nThe information below in this section are copied from the model's [official model card](https://arxiv.org/pdf/2210.11416.pdf):\n\n> Language models, including Flan-T5, can potentially be used for language generation in a harmful way, according to Rae et al. (2021). Flan-T5 should not be used directly in any application, without a prior assessment of safety and fairness concerns specific to the application.\n\n## Ethical considerations and risks\n\n> Flan-T5 is fine-tuned on a large corpus of text data that was not filtered for explicit content or assessed for existing biases. As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.\n\n## Known Limitations\n\n> Flan-T5 has not been tested in real world applications.\n\n## Sensitive Use:\n\n> Flan-T5 should not be applied for any unacceptable use cases, e.g., generation of abusive speech.\n\n# Training Details\n\n## Training Data\n\nThe model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2):\n\n![table.png](https://s3.amazonaws.com/moonup/production/uploads/1666363265279-62441d1d9fdefb55a0b7d12c.png)\n\n\n## Training Procedure\n\nAccording to the model card from the [original paper](https://arxiv.org/pdf/2210.11416.pdf):\n\n> These models are based on pretrained T5 (Raffel et al., 2020) and fine-tuned with instructions for better zero-shot and few-shot performance. There is one fine-tuned Flan model per T5 model size.\n\nThe model has been trained on TPU v3 or TPU v4 pods, using [`t5x`](https://github.com/google-research/t5x) codebase together with [`jax`](https://github.com/google/jax).\n\n\n# Evaluation\n\n## Testing Data, Factors & Metrics\n\nThe authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation:\n![image.png](https://s3.amazonaws.com/moonup/production/uploads/1668072995230-62441d1d9fdefb55a0b7d12c.png)\nFor full details, please check the [research paper](https://arxiv.org/pdf/2210.11416.pdf).\n\n## Results \n\nFor full results for FLAN-T5-Small, see the [research paper](https://arxiv.org/pdf/2210.11416.pdf), Table 3.\n\n# Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** Google Cloud TPU Pods - TPU v3 or TPU v4  | Number of chips \u2265 4.\n- **Hours used:** More information needed\n- **Cloud Provider:** GCP\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n\n# Citation\n\n**BibTeX:**\n\n```bibtex\n@misc{https://doi.org/10.48550/arxiv.2210.11416,\n  doi = {10.48550/ARXIV.2210.11416},\n  \n  url = {https://arxiv.org/abs/2210.11416},\n  \n  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\n  \n  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {Scaling Instruction-Finetuned Language Models},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}\n```", "files": [{"path": ".gitattributes", "size": 1434}, {"path": "README.md", "size": 10820}, {"path": "config.json", "size": 1401}, {"path": "flax_model.msgpack", "size": 307852839}, {"path": "generation_config.json", "size": 147}, {"path": "model.safetensors", "size": 307867048}, {"path": "pytorch_model.bin", "size": 307905733}, {"path": "special_tokens_map.json", "size": 2201}, {"path": "spiece.model", "size": 791656}, {"path": "tf_model.h5", "size": 439831352}, {"path": "tokenizer.json", "size": 2424064}, {"path": "tokenizer_config.json", "size": 2539}], "size": 1366691234, "datasets": ["svakulenk0/qrecc", "taskmaster2", "djaym7/wiki_dialog", "deepmind/code_contests", "lambada", "gsm8k", "aqua_rat", "esnli", "quasc", "qed"], "github_links": ["https://github.com/google-research/t5x", "https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints", "https://github.com/google/jax"], "_source": {"fetched_at": 1762842898.784414, "last_modified": "2023-10-10T18:01:54+00:00"}}, "fetched_at": 1762842898.784414}, "model:distilbert-base-uncased-distilled-squad": {"payload": {"url": "https://huggingface.co/distilbert-base-uncased-distilled-squad", "repo_id": "distilbert-base-uncased-distilled-squad", "repo_type": "model", "license": "apache-2.0", "downloads": 103421, "likes": 118, "last_modified": "2024-05-06T13:46:39+00:00", "tags": ["transformers", "pytorch", "tf", "tflite", "coreml", "safetensors", "distilbert", "question-answering", "en", "dataset:squad", "arxiv:1910.01108", "arxiv:1910.09700", "license:apache-2.0", "endpoints_compatible", "region:us"], "pipeline_tag": "question-answering", "card_yaml": "datasets:\n- squad\nlanguage: en\nlicense: apache-2.0\nwidget:\n- text: Which name is also used to describe the Amazon rainforest in English?\n  context: 'The Amazon rainforest (Portuguese: Floresta Amaz\u00f4nica or Amaz\u00f4nia; Spanish:\n    Selva Amaz\u00f3nica, Amazon\u00eda or usually Amazonia; French: For\u00eat amazonienne; Dutch:\n    Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is\n    a moist broadleaf forest that covers most of the Amazon basin of South America.\n    This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which\n    5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This\n    region includes territory belonging to nine nations. The majority of the forest\n    is contained within Brazil, with 60% of the rainforest, followed by Peru with\n    13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia,\n    Guyana, Suriname and French Guiana. States or departments in four nations contain\n    \"Amazonas\" in their names. The Amazon represents over half of the planet''s remaining\n    rainforests, and comprises the largest and most biodiverse tract of tropical rainforest\n    in the world, with an estimated 390 billion individual trees divided into 16,000\n    species.'\n- text: How many square kilometers of rainforest is covered in the basin?\n  context: 'The Amazon rainforest (Portuguese: Floresta Amaz\u00f4nica or Amaz\u00f4nia; Spanish:\n    Selva Amaz\u00f3nica, Amazon\u00eda or usually Amazonia; French: For\u00eat amazonienne; Dutch:\n    Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is\n    a moist broadleaf forest that covers most of the Amazon basin of South America.\n    This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which\n    5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This\n    region includes territory belonging to nine nations. The majority of the forest\n    is contained within Brazil, with 60% of the rainforest, followed by Peru with\n    13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia,\n    Guyana, Suriname and French Guiana. States or departments in four nations contain\n    \"Amazonas\" in their names. The Amazon represents over half of the planet''s remaining\n    rainforests, and comprises the largest and most biodiverse tract of tropical rainforest\n    in the world, with an estimated 390 billion individual trees divided into 16,000\n    species.'", "readme_text": "\n# DistilBERT base uncased distilled SQuAD\n\n## Table of Contents\n- [Model Details](#model-details)\n- [How To Get Started With the Model](#how-to-get-started-with-the-model)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n- [Evaluation](#evaluation)\n- [Environmental Impact](#environmental-impact)\n- [Technical Specifications](#technical-specifications)\n- [Citation Information](#citation-information)\n- [Model Card Authors](#model-card-authors)\n\n## Model Details\n\n**Model Description:** The DistilBERT model was proposed in the blog post [Smaller, faster, cheaper, lighter: Introducing DistilBERT, adistilled version of BERT](https://medium.com/huggingface/distilbert-8cf3380435b5), and the paper [DistilBERT, adistilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108). DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than *bert-base-uncased*, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.\n\nThis model is a fine-tune checkpoint of [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased), fine-tuned using (a second step of) knowledge distillation on [SQuAD v1.1](https://huggingface.co/datasets/squad). \n\n- **Developed by:** Hugging Face\n- **Model Type:** Transformer-based language model\n- **Language(s):** English \n- **License:** Apache 2.0\n- **Related Models:** [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased)\n- **Resources for more information:**\n  - See [this repository](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for more about Distil\\* (a class of compressed models including this model)\n  - See [Sanh et al. (2019)](https://arxiv.org/abs/1910.01108) for more information about knowledge distillation and the training procedure\n\n## How to Get Started with the Model \n\nUse the code below to get started with the model. \n\n```python\n>>> from transformers import pipeline\n>>> question_answerer = pipeline(\"question-answering\", model='distilbert-base-uncased-distilled-squad')\n\n>>> context = r\"\"\"\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example     of a\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n... \"\"\"\n\n>>> result = question_answerer(question=\"What is a good example of a question answering dataset?\",     context=context)\n>>> print(\n... f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n...)\n\nAnswer: 'SQuAD dataset', score: 0.4704, start: 147, end: 160\n```\n\nHere is how to use this model in PyTorch:\n\n```python\nfrom transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\nimport torch\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')\nmodel = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\n\nquestion, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\ninputs = tokenizer(question, text, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nanswer_start_index = torch.argmax(outputs.start_logits)\nanswer_end_index = torch.argmax(outputs.end_logits)\n\npredict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\ntokenizer.decode(predict_answer_tokens)\n```\n\nAnd in TensorFlow: \n\n```python\nfrom transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering\nimport tensorflow as tf\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\nmodel = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n\nquestion, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\ninputs = tokenizer(question, text, return_tensors=\"tf\")\noutputs = model(**inputs)\n\nanswer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\nanswer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n\npredict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\ntokenizer.decode(predict_answer_tokens)\n```\n\n## Uses\n\nThis model can be used for question answering.\n\n#### Misuse and Out-of-scope Use\n\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n## Risks, Limitations and Biases\n\n**CONTENT WARNING: Readers should be aware that language generated by this model can be disturbing or offensive to some and can propagate historical and current stereotypes.**\n\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. For example:\n\n\n```python\n>>> from transformers import pipeline\n>>> question_answerer = pipeline(\"question-answering\", model='distilbert-base-uncased-distilled-squad')\n\n>>> context = r\"\"\"\n... Alice is sitting on the bench. Bob is sitting next to her.\n... \"\"\"\n\n>>> result = question_answerer(question=\"Who is the CEO?\", context=context)\n>>> print(\n... f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n...)\n\nAnswer: 'Bob', score: 0.4183, start: 32, end: 35\n```\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\n\n## Training\n\n#### Training Data\n\nThe [distilbert-base-uncased model](https://huggingface.co/distilbert-base-uncased) model describes it's training data as: \n\n> DistilBERT pretrained on the same data as BERT, which is [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers).\n\nTo learn more about the SQuAD v1.1 dataset, see the [SQuAD v1.1 data card](https://huggingface.co/datasets/squad).\n\n#### Training Procedure\n\n##### Preprocessing\n\nSee the [distilbert-base-uncased model card](https://huggingface.co/distilbert-base-uncased) for further details.\n\n##### Pretraining\n\nSee the [distilbert-base-uncased model card](https://huggingface.co/distilbert-base-uncased) for further details. \n\n## Evaluation\n\nAs discussed in the [model repository](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/README.md)\n\n> This model reaches a F1 score of 86.9 on the [SQuAD v1.1] dev set (for comparison, Bert bert-base-uncased version reaches a F1 score of 88.5).\n\n## Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). We present the hardware type and hours used based on the [associated paper](https://arxiv.org/pdf/1910.01108.pdf). Note that these details are just for training DistilBERT, not including the fine-tuning with SQuAD.\n\n- **Hardware Type:** 8 16GB V100 GPUs\n- **Hours used:** 90 hours\n- **Cloud Provider:** Unknown\n- **Compute Region:** Unknown\n- **Carbon Emitted:** Unknown\n\n## Technical Specifications\n\nSee the [associated paper](https://arxiv.org/abs/1910.01108) for details on the modeling architecture, objective, compute infrastructure, and training details.\n\n## Citation Information\n\n```bibtex\n@inproceedings{sanh2019distilbert,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},\n  booktitle={NeurIPS EMC^2 Workshop},\n  year={2019}\n}\n```\n\nAPA: \n- Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.\n\n## Model Card Authors\n\nThis model card was written by the Hugging Face team. \n", "files": [{"path": ".gitattributes", "size": 445}, {"path": "384-8bits.tflite", "size": 67610976}, {"path": "384-fp16.tflite", "size": 132825972}, {"path": "384.tflite", "size": 265337392}, {"path": "README.md", "size": 10965}, {"path": "config.json", "size": 451}, {"path": "coreml_model.mlmodel", "size": 265486673}, {"path": "coreml_model_fp16.mlmodel", "size": 180548849}, {"path": "model.safetensors", "size": 265470036}, {"path": "pytorch_model.bin", "size": 265481570}, {"path": "saved_model.tar.gz", "size": 244849235}, {"path": "tf_model.h5", "size": 265582824}, {"path": "tokenizer.json", "size": 466062}, {"path": "tokenizer_config.json", "size": 48}, {"path": "vocab.txt", "size": 231508}], "size": 1953903006, "datasets": ["squad"], "github_links": ["https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/README.md", "https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation"], "_source": {"fetched_at": 1762843094.9482381, "last_modified": "2024-05-06T13:46:39+00:00"}}, "fetched_at": 1762843094.9482381}, "model:facebook/bart-large": {"payload": {"url": "https://huggingface.co/facebook/bart-large", "repo_id": "facebook/bart-large", "repo_type": "model", "license": "apache-2.0", "downloads": 45920, "likes": 198, "last_modified": "2022-06-03T10:00:20+00:00", "tags": ["transformers", "pytorch", "tf", "jax", "rust", "bart", "feature-extraction", "en", "arxiv:1910.13461", "license:apache-2.0", "endpoints_compatible", "region:us"], "pipeline_tag": "feature-extraction", "card_yaml": "language: en\nlicense: apache-2.0", "readme_text": "\n# BART (large-sized model) \n\nBART model pre-trained on English language. It was introduced in the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Lewis et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart). \n\nDisclaimer: The team releasing BART did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nBART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n\nBART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).\n\n## Intended uses & limitations\n\nYou can use the raw model for text infilling. However, the model is mostly meant to be fine-tuned on a supervised dataset. See the [model hub](https://huggingface.co/models?search=bart) to look for fine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model in PyTorch:\n\n```python\nfrom transformers import BartTokenizer, BartModel\n\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\nmodel = BartModel.from_pretrained('facebook/bart-large')\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n```\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1910-13461,\n  author    = {Mike Lewis and\n               Yinhan Liu and\n               Naman Goyal and\n               Marjan Ghazvininejad and\n               Abdelrahman Mohamed and\n               Omer Levy and\n               Veselin Stoyanov and\n               Luke Zettlemoyer},\n  title     = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language\n               Generation, Translation, and Comprehension},\n  journal   = {CoRR},\n  volume    = {abs/1910.13461},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1910.13461},\n  eprinttype = {arXiv},\n  eprint    = {1910.13461},\n  timestamp = {Thu, 31 Oct 2019 14:02:26 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```", "files": [{"path": ".gitattributes", "size": 391}, {"path": "README.md", "size": 2596}, {"path": "config.json", "size": 1628}, {"path": "flax_model.msgpack", "size": 812600864}, {"path": "merges.txt", "size": 456318}, {"path": "pytorch_model.bin", "size": 1018571383}, {"path": "rust_model.ot", "size": 2037059720}, {"path": "tf_model.h5", "size": 1625696184}, {"path": "tokenizer.json", "size": 1355863}, {"path": "tokenizer_config.json", "size": 26}, {"path": "vocab.json", "size": 898822}], "size": 5496643795, "datasets": [], "github_links": ["https://github.com/pytorch/fairseq/tree/master/examples/bart"], "_source": {"fetched_at": 1762843095.492095, "last_modified": "2022-06-03T10:00:20+00:00"}}, "fetched_at": 1762843095.492095}, "model:t5-small": {"payload": {"url": "https://huggingface.co/t5-small", "repo_id": "t5-small", "repo_type": "model", "license": "apache-2.0", "downloads": 2508475, "likes": 503, "last_modified": "2023-06-30T02:31:26+00:00", "tags": ["transformers", "pytorch", "tf", "jax", "rust", "onnx", "safetensors", "t5", "text2text-generation", "summarization", "translation", "en", "fr", "ro", "de", "multilingual", "dataset:c4", "arxiv:1805.12471", "arxiv:1708.00055", "arxiv:1704.05426", "arxiv:1606.05250", "arxiv:1808.09121", "arxiv:1810.12885", "arxiv:1905.10044", "arxiv:1910.09700", "license:apache-2.0", "text-generation-inference", "endpoints_compatible", "region:us"], "pipeline_tag": "translation", "card_yaml": "datasets:\n- c4\nlanguage:\n- en\n- fr\n- ro\n- de\n- multilingual\nlicense: apache-2.0\ntags:\n- summarization\n- translation", "readme_text": "\n# Model Card for T5 Small\n\n![model image](https://camo.githubusercontent.com/623b4dea0b653f2ad3f36c71ebfe749a677ac0a1/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f343030362f312a44304a31674e51663876727255704b657944387750412e706e67)\n\n#  Table of Contents\n\n1. [Model Details](#model-details)\n2. [Uses](#uses)\n3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)\n4. [Training Details](#training-details)\n5. [Evaluation](#evaluation)\n6. [Environmental Impact](#environmental-impact)\n7. [Citation](#citation)\n8. [Model Card Authors](#model-card-authors)\n9. [How To Get Started With the Model](#how-to-get-started-with-the-model)\n\n# Model Details\n\n## Model Description\n\nThe developers of the Text-To-Text Transfer Transformer (T5) [write](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html): \n\n> With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.\n\nT5-Small is the checkpoint with 60 million parameters. \n\n- **Developed by:** Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. See [associated paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) and [GitHub repo](https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints)\n- **Model type:** Language model\n- **Language(s) (NLP):** English, French, Romanian, German\n- **License:** Apache 2.0\n- **Related Models:** [All T5 Checkpoints](https://huggingface.co/models?search=t5)\n- **Resources for more information:**\n  - [Research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf)\n  - [Google's T5 Blog Post](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) \n  - [GitHub Repo](https://github.com/google-research/text-to-text-transfer-transformer)\n  - [Hugging Face T5 Docs](https://huggingface.co/docs/transformers/model_doc/t5)\n  \n# Uses\n\n## Direct Use and Downstream Use\n\nThe developers write in a [blog post](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) that the model: \n\n> Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task, including machine translation, document summarization, question answering, and classification tasks (e.g., sentiment analysis). We can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself.\n\nSee the [blog post](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) and [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) for further details.\n\n## Out-of-Scope Use\n\nMore information needed.\n\n# Bias, Risks, and Limitations\n\nMore information needed.\n\n## Recommendations\n\nMore information needed.\n\n# Training Details\n\n## Training Data\n\nThe model is pre-trained on the [Colossal Clean Crawled Corpus (C4)](https://www.tensorflow.org/datasets/catalog/c4), which was developed and released in the context of the same [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) as T5.\n\nThe model was pre-trained on a on a **multi-task mixture of unsupervised (1.) and supervised tasks (2.)**.\nThereby, the following datasets were being used for (1.) and (2.):\n\n1. **Datasets used for Unsupervised denoising objective**:\n\n- [C4](https://huggingface.co/datasets/c4)\n- [Wiki-DPR](https://huggingface.co/datasets/wiki_dpr)\n\n\n2. **Datasets used for Supervised text-to-text language modeling objective**\n\n- Sentence acceptability judgment\n  - CoLA [Warstadt et al., 2018](https://arxiv.org/abs/1805.12471)\n- Sentiment analysis \n  - SST-2 [Socher et al., 2013](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n- Paraphrasing/sentence similarity\n  - MRPC [Dolan and Brockett, 2005](https://aclanthology.org/I05-5002)\n  - STS-B [Ceret al., 2017](https://arxiv.org/abs/1708.00055)\n  - QQP [Iyer et al., 2017](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs)\n- Natural language inference\n  - MNLI [Williams et al., 2017](https://arxiv.org/abs/1704.05426)\n  - QNLI [Rajpurkar et al.,2016](https://arxiv.org/abs/1606.05250)\n  - RTE [Dagan et al., 2005](https://link.springer.com/chapter/10.1007/11736790_9) \n  - CB [De Marneff et al., 2019](https://semanticsarchive.net/Archive/Tg3ZGI2M/Marneffe.pdf)\n- Sentence completion\n  - COPA [Roemmele et al., 2011](https://www.researchgate.net/publication/221251392_Choice_of_Plausible_Alternatives_An_Evaluation_of_Commonsense_Causal_Reasoning)\n- Word sense disambiguation\n  - WIC [Pilehvar and Camacho-Collados, 2018](https://arxiv.org/abs/1808.09121)\n- Question answering\n  - MultiRC [Khashabi et al., 2018](https://aclanthology.org/N18-1023)\n  - ReCoRD [Zhang et al., 2018](https://arxiv.org/abs/1810.12885)\n  - BoolQ [Clark et al., 2019](https://arxiv.org/abs/1905.10044)\n\n## Training Procedure\n\nIn their [abstract](https://jmlr.org/papers/volume21/20-074/20-074.pdf), the model developers write: \n\n> In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. \n\nThe framework introduced, the T5 framework, involves a training procedure that brings together the approaches studied in the paper. See the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) for further details.\n\n# Evaluation\n\n## Testing Data, Factors & Metrics\n\nThe developers evaluated the model on 24 tasks, see the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) for full details.\n\n## Results \n\nFor full results for T5-small, see the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf), Table 14.\n\n# Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** Google Cloud TPU Pods\n- **Hours used:** More information needed\n- **Cloud Provider:** GCP\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n\n# Citation\n\n**BibTeX:**\n\n```bibtex\n@article{2020t5,\n  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\n  journal = {Journal of Machine Learning Research},\n  year    = {2020},\n  volume  = {21},\n  number  = {140},\n  pages   = {1-67},\n  url     = {http://jmlr.org/papers/v21/20-074.html}\n}\n```\n\n**APA:**\n- Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140), 1-67.\n\n# Model Card Authors\n\nThis model card was written by the team at Hugging Face.\n\n# How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nfrom transformers import T5Tokenizer, T5Model\n\ntokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\nmodel = T5Model.from_pretrained(\"t5-small\")\n\ninput_ids = tokenizer(\n    \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n).input_ids  # Batch size 1\ndecoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n\n# forward pass\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\nlast_hidden_states = outputs.last_hidden_state\n```\n\nSee the [Hugging Face T5](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Model) docs and a [Colab Notebook](https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb) created by the model developers for more examples.\n</details>\n\n", "files": [{"path": ".gitattributes", "size": 537}, {"path": "README.md", "size": 8473}, {"path": "config.json", "size": 1206}, {"path": "flax_model.msgpack", "size": 242032202}, {"path": "generation_config.json", "size": 147}, {"path": "model.safetensors", "size": 242043056}, {"path": "onnx/decoder_model.onnx", "size": 232468834}, {"path": "onnx/decoder_model_merged.onnx", "size": 232631325}, {"path": "onnx/decoder_model_merged_quantized.onnx", "size": 58682273}, {"path": "onnx/decoder_model_quantized.onnx", "size": 58405018}, {"path": "onnx/decoder_with_past_model.onnx", "size": 219874970}, {"path": "onnx/decoder_with_past_model_quantized.onnx", "size": 55233251}, {"path": "onnx/encoder_model.onnx", "size": 141404302}, {"path": "onnx/encoder_model_quantized.onnx", "size": 35501599}, {"path": "pytorch_model.bin", "size": 242065649}, {"path": "rust_model.ot", "size": 242133508}, {"path": "spiece.model", "size": 791656}, {"path": "tf_model.h5", "size": 242303832}, {"path": "tokenizer.json", "size": 1389353}, {"path": "tokenizer_config.json", "size": 2324}], "size": 2246973515, "datasets": ["c4"], "github_links": ["https://github.com/google-research/text-to-text-transfer-transformer"], "_source": {"fetched_at": 1762843101.1083388, "last_modified": "2023-06-30T02:31:26+00:00"}}, "fetched_at": 1762843101.1083388}, "model:gpt2": {"payload": {"url": "https://huggingface.co/gpt2", "repo_id": "gpt2", "repo_type": "model", "license": "mit", "downloads": 11477729, "likes": 3013, "last_modified": "2024-02-19T10:57:45+00:00", "tags": ["transformers", "pytorch", "tf", "jax", "tflite", "rust", "onnx", "safetensors", "gpt2", "text-generation", "exbert", "en", "doi:10.57967/hf/0039", "license:mit", "autotrain_compatible", "text-generation-inference", "endpoints_compatible", "region:us"], "pipeline_tag": "text-generation", "card_yaml": "language: en\nlicense: mit\ntags:\n- exbert", "readme_text": "\n\n# GPT-2\n\nTest the whole generation capabilities here: https://transformer.huggingface.co/doc/gpt2-large\n\nPretrained model on English language using a causal language modeling (CLM) objective. It was introduced in\n[this paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\nand first released at [this page](https://openai.com/blog/better-language-models/).\n\nDisclaimer: The team releasing GPT-2 also wrote a\n[model card](https://github.com/openai/gpt-2/blob/master/model_card.md) for their model. Content from this model card\nhas been written by the Hugging Face team to complete the information they provided and give specific examples of bias.\n\n## Model description\n\nGPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This\nmeans it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots\nof publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely,\nit was trained to guess the next word in sentences.\n\nMore precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence,\nshifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the\npredictions for the token `i` only uses the inputs from `1` to `i` but not the future tokens.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a\nprompt.\n\nThis is the **smallest** version of GPT-2, with 124M parameters. \n\n**Related Models:** [GPT-Large](https://huggingface.co/gpt2-large), [GPT-Medium](https://huggingface.co/gpt2-medium) and [GPT-XL](https://huggingface.co/gpt2-xl)\n\n## Intended uses & limitations\n\nYou can use the raw model for text generation or fine-tune it to a downstream task. See the\n[model hub](https://huggingface.co/models?filter=gpt2) to look for fine-tuned versions on a task that interests you.\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we\nset a seed for reproducibility:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2')\n>>> set_seed(42)\n>>> generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n[{'generated_text': \"Hello, I'm a language model, a language for thinking, a language for expressing thoughts.\"},\n {'generated_text': \"Hello, I'm a language model, a compiler, a compiler library, I just want to know how I build this kind of stuff. I don\"},\n {'generated_text': \"Hello, I'm a language model, and also have more than a few of your own, but I understand that they're going to need some help\"},\n {'generated_text': \"Hello, I'm a language model, a system model. I want to know my language so that it might be more interesting, more user-friendly\"},\n {'generated_text': 'Hello, I\\'m a language model, not a language model\"\\n\\nThe concept of \"no-tricks\" comes in handy later with new'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2Model.from_pretrained('gpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = TFGPT2Model.from_pretrained('gpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nThe training data used for this model has not been released as a dataset one can browse. We know it contains a lot of\nunfiltered content from the internet, which is far from neutral. As the openAI team themselves point out in their\n[model card](https://github.com/openai/gpt-2/blob/master/model_card.md#out-of-scope-use-cases):\n\n> Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don\u2019t support use-cases\n> that require the generated text to be true.\n>\n> Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do\n> not recommend that they be deployed into systems that interact with humans > unless the deployers first carry out a\n> study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race,\n> and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar\n> levels of caution around use cases that are sensitive to biases around human attributes.\n\nHere's an example of how the model can have biased predictions:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2')\n>>> set_seed(42)\n>>> generator(\"The White man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The White man worked as a mannequin for'},\n {'generated_text': 'The White man worked as a maniser of the'},\n {'generated_text': 'The White man worked as a bus conductor by day'},\n {'generated_text': 'The White man worked as a plumber at the'},\n {'generated_text': 'The White man worked as a journalist. He had'}]\n\n>>> set_seed(42)\n>>> generator(\"The Black man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The Black man worked as a man at a restaurant'},\n {'generated_text': 'The Black man worked as a car salesman in a'},\n {'generated_text': 'The Black man worked as a police sergeant at the'},\n {'generated_text': 'The Black man worked as a man-eating monster'},\n {'generated_text': 'The Black man worked as a slave, and was'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe OpenAI team wanted to train this model on a corpus as large as possible. To build it, they scraped all the web\npages from outbound links on Reddit which received at least 3 karma. Note that all Wikipedia pages were removed from\nthis dataset, so the model was not trained on any part of Wikipedia. The resulting dataset (called WebText) weights\n40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText\n[here](https://github.com/openai/gpt-2/blob/master/domains.txt).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a\nvocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.\n\nThe larger model was trained on 256 cloud TPU v3 cores. The training duration was not disclosed, nor were the exact\ndetails of training.\n\n## Evaluation results\n\nThe model achieves the following results without any fine-tuning (zero-shot):\n\n| Dataset  | LAMBADA | LAMBADA | CBT-CN | CBT-NE | WikiText2 | PTB    | enwiki8 | text8  | WikiText103 | 1BW   |\n|:--------:|:-------:|:-------:|:------:|:------:|:---------:|:------:|:-------:|:------:|:-----------:|:-----:|\n| (metric) | (PPL)   | (ACC)   | (ACC)  | (ACC)  | (PPL)     | (PPL)  | (BPB)   | (BPC)  | (PPL)       | (PPL) |\n|          | 35.13   | 45.99   | 87.65  | 83.4   | 29.41     | 65.85  | 1.16    | 1,17   | 37.50       | 75.20 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{radford2019language,\n  title={Language Models are Unsupervised Multitask Learners},\n  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},\n  year={2019}\n}\n```\n\n<a href=\"https://huggingface.co/exbert/?model=gpt2\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n", "files": [{"path": ".gitattributes", "size": 445}, {"path": "64-8bits.tflite", "size": 125162496}, {"path": "64-fp16.tflite", "size": 248269688}, {"path": "64.tflite", "size": 495791932}, {"path": "README.md", "size": 8092}, {"path": "config.json", "size": 665}, {"path": "flax_model.msgpack", "size": 497764120}, {"path": "generation_config.json", "size": 124}, {"path": "merges.txt", "size": 456318}, {"path": "model.safetensors", "size": 548105171}, {"path": "onnx/config.json", "size": 879}, {"path": "onnx/decoder_model.onnx", "size": 653665842}, {"path": "onnx/decoder_model_merged.onnx", "size": 655189339}, {"path": "onnx/decoder_with_past_model.onnx", "size": 653672649}, {"path": "onnx/generation_config.json", "size": 119}, {"path": "onnx/merges.txt", "size": 456318}, {"path": "onnx/special_tokens_map.json", "size": 99}, {"path": "onnx/tokenizer.json", "size": 2107653}, {"path": "onnx/tokenizer_config.json", "size": 234}, {"path": "onnx/vocab.json", "size": 798156}, {"path": "pytorch_model.bin", "size": 548118077}, {"path": "rust_model.ot", "size": 702517648}, {"path": "tf_model.h5", "size": 497933648}, {"path": "tokenizer.json", "size": 1355256}, {"path": "tokenizer_config.json", "size": 26}, {"path": "vocab.json", "size": 1042301}], "size": 5632417295, "datasets": [], "github_links": ["https://github.com/openai/gpt-2/blob/master/domains.txt", "https://github.com/openai/gpt-2/blob/master/model_card.md", "https://github.com/openai/gpt-2/blob/master/model_card.md#out-of-scope-use-cases"], "_source": {"fetched_at": 1762843104.4978728, "last_modified": "2024-02-19T10:57:45+00:00"}}, "fetched_at": 1762843104.4978728}}