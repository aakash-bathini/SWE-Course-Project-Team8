{"google-research-datasets/natural-questions": {"payload": {"url": "https://github.com/google-research-datasets/natural-questions", "repo_id": "google-research-datasets/natural-questions", "repo_type": "code", "name": "natural-questions", "full_name": "google-research-datasets/natural-questions", "description": "Natural Questions (NQ) contains real user questions issued to Google search, and answers found from Wikipedia by annotators. NQ is designed for the training and evaluation of automatic question answering systems.", "homepage": "", "default_branch": "master", "topics": [], "language": "Python", "archived": false, "disabled": false, "fork": false, "created_at": "2019-01-22T18:37:59Z", "updated_at": "2025-09-25T23:26:38Z", "pushed_at": "2021-07-30T00:22:08Z", "stars": 1046, "forks": 156, "open_issues": 16, "watchers": 34, "license_spdx": "Apache-2.0", "readme_text": "# Natural Questions\n\nNatural Questions (NQ) contains real user questions issued to Google search, and\nanswers found from Wikipedia by annotators.\nNQ is designed for the training and evaluation of automatic question answering\nsystems.\n\nPlease see\n[http://ai.google.com/research/NaturalQuestions](http://ai.google.com/research/NaturalQuestions)\nto get the data and view the leaderboard.\nFor more details on the design and content of the dataset, please see\nthe paper\n[Natural Questions: a Benchmark for Question Answering Research](https://ai.google/research/pubs/pub47761).\nTo help you get started on this task we have provided some\n[baseline systems](https://github.com/google-research/language/tree/master/language/question_answering)\nthat can be branched.\n\n\n# Data Description\nNQ contains 307,372 training examples, 7,830 examples for development, and we\nwithold a further 7,842 examples for testing. In the paper, we demonstrate a\nhuman upper bound of 87% F1 on the long answer selection task, and 76% on the\nshort answer selection task.\n\nTo run on the hidden test set, you will have to upload a Docker image containing\nyour system to the\n[NQ competition site](http://ai.google.com/research/NaturalQuestions/competition).\nInstructions on building the Docker image are given [here](competition.md).\n\n\n## Data Format\nEach example in the original NQ format contains the rendered HTML of an entire\nWikipedia page, as well as a tokenized representation of the text on the page.\n\nThis section will go on to define the full NQ data format, but we recognize\nthat most users will only want a version of the data in which the text has\nalready been extracted. We have supplied a\n[simplified version of the training set](https://storage.cloud.google.com/natural_questions/v1.0-simplified/simplified-nq-train.jsonl.gz)\nand we have also supplied a `simplify_nq_example` function\nin [data_utils.py](data_utils.py) which maps from the original format to the\nsimplified format. Only the original format is provided by our\n[competition site](https://ai.google.com/research/NaturalQuestions/competition).\nIf you use the simplified data, you should call `simplify_nq_example` on each\nexample seen during evaluation and you should provide predictions using the\n`start_token` and `end_token` offsets that correspond to the whitespace\nseparated tokens in the document text.\n\nAs well as recognizing predictions according to token offsets, the evaluation\nscript also recognizes predictions as byte offsets into the original HTML. This\nallows users to define their own text extraction and tokenization schemes.\n\nTo help you explore the data, this repository also contains a simple\n[data browser](nq_browser.py) that you can run on your own machine, and modify\nas you see fit. We also have provided extra preprocessing utilities and\ntensorflow dataset code in\n[the repository containing the baseline systems presented in our paper](https://github.com/google-research/language/tree/master/language/question_answering).\nThe rest of this section describes the data format thouroughly in reference to\na [toy example](toy_example.md).\n\nEach example contains a single question, a tokenized representation of the question,\na timestamped Wikipedia URL, and the HTML representation of that Wikipedia page.\n\n```json\n\"question_text\": \"who founded google\",\n\"question_tokens\": [\"who\", \"founded\", \"google\"],\n\"document_url\": \"http://www.wikipedia.org/Google\",\n\"document_html\": \"<html><body><h1>Google</h1><p>Google was founded in 1998 by ...\"\n```\n\nWe release the raw HTML, since this is what was seen by our annotators, and we\nwould like to support approaches that make use of the document structure.\nHowever, we expect most initial efforts will prefer to use a tokenized\nrepresentation of the page.\n\n```json\n\"document_tokens\":[\n  { \"token\": \"<h1>\", \"start_byte\": 12, \"end_byte\": 16, \"html_token\": true },\n  { \"token\": \"Google\", \"start_byte\": 16, \"end_byte\": 22, \"html_token\": false },\n  { \"token\": \"inc\", \"start_byte\": 23, \"end_byte\": 26, \"html_token\": false },\n  { \"token\": \".\", \"start_byte\": 26, \"end_byte\": 27, \"html_token\": false },\n  { \"token\": \"</h1>\", \"start_byte\": 27, \"end_byte\": 32, \"html_token\": true },\n  { \"token\": \"<p>\", \"start_byte\": 32, \"end_byte\": 35, \"html_token\": true },\n  { \"token\": \"Google\", \"start_byte\": 35, \"end_byte\": 41, \"html_token\": false },\n  { \"token\": \"was\", \"start_byte\": 42, \"end_byte\": 45, \"html_token\": false },\n  { \"token\": \"founded\", \"start_byte\": 46, \"end_byte\": 53, \"html_token\": false },\n  { \"Token\": \"in\", \"start_byte\": 54, \"end_byte\": 56, \"html_token\": false },\n  { \"token\": \"1998\", \"start_byte\": 57, \"end_byte\": 61, \"html_token\": false },\n  { \"token\": \"by\", \"start_byte\": 62, \"end_byte\": 64, \"html_token\": false },\n```\n\nEach token is either a word or a HTML tag that defines a heading, paragraph,\ntable, or list. HTML tags are marked as such using the boolean field `html_token`.\nEach token also has an inclusive `start_byte` and exclusive `end_byte` that\nidentifies the token's position within the example's UTF-8 indexed HTML string.\n\n### Long Answer Candidates\nThe first task in Natural Questions is to identify the *smallest* HTML bounding\nbox that contains all of the information required to infer the answer to a\nquestion.\nThese long answers can be paragraphs, lists, list items, tables, or table rows.\nWhile the candidates can be inferred directly from the HTML or token sequence, we\nalso include a list of long answer candidates for convenience.\nEach candidate is defined in terms of offsets into both the HTML and the\ndocument tokens.\nAs with all other annotations, start offsets are inclusive and end offsets are\nexclusive.\n\n```json\n\"long_answer_candidates\": [\n  { \"start_byte\": 32, \"end_byte\": 106, \"start_token\": 5, \"end_token\": 22, \"top_level\": true },\n  { \"start_byte\": 65, \"end_byte\": 102, \"start_token\": 13, \"end_token\": 21, \"top_level\": false },\n```\n\nIn this example, you can see that the second long answer candidate is contained\nwithin the first. We do not disallow nested long answer candidates, we just ask\nannotators to find the *smallest candidate containing all of the information\nrequired to infer the answer to the question*. However, we do observe that 95%\nof all long answers (including all paragraph answers) are not nested below any\nother candidates.\nSince we believe that some users may want to start by only considering\nnon-overlapping candidates, we include a boolean flag `top_level` that\nidentifies whether a candidate is nested below another (`top_level = False`) or\nnot (`top_level = True`). Please be aware that this flag is only included for\nconvenience and it is not related to the task definition in any way.\nFor more information about the distribution of long answer types, please\nsee the data statistics section below.\n\n### Annotations\nThe NQ training data has a single annotation with each example and the evaluation\ndata has five. Each annotation defines a \"long_answer\" span, a list of\n`short_answers`, and a `yes_no_answer`. \u00a0If the annotator has marked a long\nanswer, then the long answer dictionary identifies this long answer using byte\noffsets, token offsets, and an index into the list of long answer candidates. If\nthe annotator has marked that no long answer is available, all of the fields in\nthe long answer dictionary are set to -1.\n\n```json\n\"annotations\": [{\n  \"long_answer\": { \"start_byte\": 32, \"end_byte\": 106, \"start_token\": 5, \"end_token\": 22, \"candidate_index\": 0 },\n  \"short_answers\": [\n    {\"start_byte\": 73, \"end_byte\": 78, \"start_token\": 15, \"end_token\": 16},\n    {\"start_byte\": 87, \"end_byte\": 92, \"start_token\": 18, \"end_token\": 19}\n  ],\n  \"yes_no_answer\": \"NONE\"\n}]\n```\n\nEach of the short answers is also identified using both byte offsets and token\nindices. There is no limit to the number of short answers. There is also often\nno short answer, since some questions such as \"describe google's founding\" do\nnot have a succinct extractive answer. When this is the case, the long answer is\ngiven but the \"short_answers\" list is empty.\n\nFinally, if no short answer is given, it is possible that there is a\n`yes_no_answer` for questions such as \"did larry co-found google\". The values\nfor this field `YES`, or `NO` if a yes/no answer is given. The default value is\n`NONE` when no yes/no answer is given. For statistics on long answers, short\nanswers, and yes/no answers, please see the data statistics section below.\n\n### Data Statistics\nThe NQ training data contains 307,373 examples. 152,148 have a long answer\nand 110,724 have a short answer. Short answers can be sets of spans in the document\n(106,926), or yes or no (3,798). Long answers are HTML bounding boxes, and the\ndistribution of NQ long answer types is as follows:\n\n| HTML tags | Percent of long answers |\n|-----------|-------------------------|\n| `<P>`     | 72.9%                   |\n| `<Table>` | 19.0%                   |\n| `<Tr>`    | 1.5%                    |\n| `<Ul>`, `<Ol>`, `<Dl>` | 3.2%       |\n| `<Li>`, `<Dd>`, `<Dt>` | 3.4%       |\n\nWhile we allow any paragraph, table, or list element to be a long answer,\nwe find that 95% of the long answers are not contained by any other\nlong answer candidate. We mark these `top level` candidates in the data,\nas described above.\n\nShort answers may contain more than one span, if the question is asking\nfor a list of answers (e.g. who made it to stage 3 in american ninja warrior season 9).\nHowever, almost all short answers (90%) only contain a single span of text.\nAll short answers are contained by the long answer given in the same annotation.\n\n# Prediction Format\nPlease see the [evaluation script](nq_eval.py) for a description of the prediction\nformat that your model should output.\n\n# Contact us\nIf you have a technical question regarding the dataset, code or publication, please\ncreate an issue in this repository. This is the fastest way to reach us.\n\nIf you would like to share feedback or report concerns, please email us at <natural-questions@google.com>.\n", "doc_texts": {"CONTRIBUTING.md": "# How to Contribute\n\nWe'd love to accept your patches and contributions to this project. There are\njust a few small guidelines you need to follow.\n\n## Contributor License Agreement\n\nContributions to this project must be accompanied by a Contributor License\nAgreement. You (or your employer) retain the copyright to your contribution;\nthis simply gives us permission to use and redistribute your contributions as\npart of the project. Head over to <https://cla.developers.google.com/> to see\nyour current agreements on file or to sign a new one.\n\nYou generally only need to submit a CLA once, so if you've already submitted one\n(even if it was for a different project), you probably don't need to do it\nagain.\n\n## Code reviews\n\nAll submissions, including submissions by project members, require review. We\nuse GitHub pull requests for this purpose. Consult\n[GitHub Help](https://help.github.com/articles/about-pull-requests/) for more\ninformation on using pull requests.\n\n## Community Guidelines\n\nThis project follows [Google's Open Source Community\nGuidelines](https://opensource.google.com/conduct/).\n", "LICENSE": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n", "README.md": "# Natural Questions\n\nNatural Questions (NQ) contains real user questions issued to Google search, and\nanswers found from Wikipedia by annotators.\nNQ is designed for the training and evaluation of automatic question answering\nsystems.\n\nPlease see\n[http://ai.google.com/research/NaturalQuestions](http://ai.google.com/research/NaturalQuestions)\nto get the data and view the leaderboard.\nFor more details on the design and content of the dataset, please see\nthe paper\n[Natural Questions: a Benchmark for Question Answering Research](https://ai.google/research/pubs/pub47761).\nTo help you get started on this task we have provided some\n[baseline systems](https://github.com/google-research/language/tree/master/language/question_answering)\nthat can be branched.\n\n\n# Data Description\nNQ contains 307,372 training examples, 7,830 examples for development, and we\nwithold a further 7,842 examples for testing. In the paper, we demonstrate a\nhuman upper bound of 87% F1 on the long answer selection task, and 76% on the\nshort answer selection task.\n\nTo run on the hidden test set, you will have to upload a Docker image containing\nyour system to the\n[NQ competition site](http://ai.google.com/research/NaturalQuestions/competition).\nInstructions on building the Docker image are given [here](competition.md).\n\n\n## Data Format\nEach example in the original NQ format contains the rendered HTML of an entire\nWikipedia page, as well as a tokenized representation of the text on the page.\n\nThis section will go on to define the full NQ data format, but we recognize\nthat most users will only want a version of the data in which the text has\nalready been extracted. We have supplied a\n[simplified version of the training set](https://storage.cloud.google.com/natural_questions/v1.0-simplified/simplified-nq-train.jsonl.gz)\nand we have also supplied a `simplify_nq_example` function\nin [data_utils.py](data_utils.py) which maps from the original format to the\nsimplified format. Only the original format is provided by our\n[competition site](https://ai.google.com/research/NaturalQuestions/competition).\nIf you use the simplified data, you should call `simplify_nq_example` on each\nexample seen during evaluation and you should provide predictions using the\n`start_token` and `end_token` offsets that correspond to the whitespace\nseparated tokens in the document text.\n\nAs well as recognizing predictions according to token offsets, the evaluation\nscript also recognizes predictions as byte offsets into the original HTML. This\nallows users to define their own text extraction and tokenization schemes.\n\nTo help you explore the data, this repository also contains a simple\n[data browser](nq_browser.py) that you can run on your own machine, and modify\nas you see fit. We also have provided extra preprocessing utilities and\ntensorflow dataset code in\n[the repository containing the baseline systems presented in our paper](https://github.com/google-research/language/tree/master/language/question_answering).\nThe rest of this section describes the data format thouroughly in reference to\na [toy example](toy_example.md).\n\nEach example contains a single question, a tokenized representation of the question,\na timestamped Wikipedia URL, and the HTML representation of that Wikipedia page.\n\n```json\n\"question_text\": \"who founded google\",\n\"question_tokens\": [\"who\", \"founded\", \"google\"],\n\"document_url\": \"http://www.wikipedia.org/Google\",\n\"document_html\": \"<html><body><h1>Google</h1><p>Google was founded in 1998 by ...\"\n```\n\nWe release the raw HTML, since this is what was seen by our annotators, and we\nwould like to support approaches that make use of the document structure.\nHowever, we expect most initial efforts will prefer to use a tokenized\nrepresentation of the page.\n\n```json\n\"document_tokens\":[\n  { \"token\": \"<h1>\", \"start_byte\": 12, \"end_byte\": 16, \"html_token\": true },\n  { \"token\": \"Google\", \"start_byte\": 16, \"end_byte\": 22, \"html_token\": false },\n  { \"token\": \"inc\", \"start_byte\": 23, \"end_byte\": 26, \"html_token\": false },\n  { \"token\": \".\", \"start_byte\": 26, \"end_byte\": 27, \"html_token\": false },\n  { \"token\": \"</h1>\", \"start_byte\": 27, \"end_byte\": 32, \"html_token\": true },\n  { \"token\": \"<p>\", \"start_byte\": 32, \"end_byte\": 35, \"html_token\": true },\n  { \"token\": \"Google\", \"start_byte\": 35, \"end_byte\": 41, \"html_token\": false },\n  { \"token\": \"was\", \"start_byte\": 42, \"end_byte\": 45, \"html_token\": false },\n  { \"token\": \"founded\", \"start_byte\": 46, \"end_byte\": 53, \"html_token\": false },\n  { \"Token\": \"in\", \"start_byte\": 54, \"end_byte\": 56, \"html_token\": false },\n  { \"token\": \"1998\", \"start_byte\": 57, \"end_byte\": 61, \"html_token\": false },\n  { \"token\": \"by\", \"start_byte\": 62, \"end_byte\": 64, \"html_token\": false },\n```\n\nEach token is either a word or a HTML tag that defines a heading, paragraph,\ntable, or list. HTML tags are marked as such using the boolean field `html_token`.\nEach token also has an inclusive `start_byte` and exclusive `end_byte` that\nidentifies the token's position within the example's UTF-8 indexed HTML string.\n\n### Long Answer Candidates\nThe first task in Natural Questions is to identify the *smallest* HTML bounding\nbox that contains all of the information required to infer the answer to a\nquestion.\nThese long answers can be paragraphs, lists, list items, tables, or table rows.\nWhile the candidates can be inferred directly from the HTML or token sequence, we\nalso include a list of long answer candidates for convenience.\nEach candidate is defined in terms of offsets into both the HTML and the\ndocument tokens.\nAs with all other annotations, start offsets are inclusive and end offsets are\nexclusive.\n\n```json\n\"long_answer_candidates\": [\n  { \"start_byte\": 32, \"end_byte\": 106, \"start_token\": 5, \"end_token\": 22, \"top_level\": true },\n  { \"start_byte\": 65, \"end_byte\": 102, \"start_token\": 13, \"end_token\": 21, \"top_level\": false },\n```\n\nIn this example, you can see that the second long answer candidate is contained\nwithin the first. We do not disallow nested long answer candidates, we just ask\nannotators to find the *smallest candidate containing all of the information\nrequired to infer the answer to the question*. However, we do observe that 95%\nof all long answers (including all paragraph answers) are not nested below any\nother candidates.\nSince we believe that some users may want to start by only considering\nnon-overlapping candidates, we include a boolean flag `top_level` that\nidentifies whether a candidate is nested below another (`top_level = False`) or\nnot (`top_level = True`). Please be aware that this flag is only included for\nconvenience and it is not related to the task definition in any way.\nFor more information about the distribution of long answer types, please\nsee the data statistics section below.\n\n### Annotations\nThe NQ training data has a single annotation with each example and the evaluation\ndata has five. Each annotation defines a \"long_answer\" span, a list of\n`short_answers`, and a `yes_no_answer`. \u00a0If the annotator has marked a long\nanswer, then the long answer dictionary identifies this long answer using byte\noffsets, token offsets, and an index into the list of long answer candidates. If\nthe annotator has marked that no long answer is available, all of the fields in\nthe long answer dictionary are set to -1.\n\n```json\n\"annotations\": [{\n  \"long_answer\": { \"start_byte\": 32, \"end_byte\": 106, \"start_token\": 5, \"end_token\": 22, \"candidate_index\": 0 },\n  \"short_answers\": [\n    {\"start_byte\": 73, \"end_byte\": 78, \"start_token\": 15, \"end_token\": 16},\n    {\"start_byte\": 87, \"end_byte\": 92, \"start_token\": 18, \"end_token\": 19}\n  ],\n  \"yes_no_answer\": \"NONE\"\n}]\n```\n\nEach of the short answers is also identified using both byte offsets and token\nindices. There is no limit to the number of short answers. There is also often\nno short answer, since some questions such as \"describe google's founding\" do\nnot have a succinct extractive answer. When this is the case, the long answer is\ngiven but the \"short_answers\" list is empty.\n\nFinally, if no short answer is given, it is possible that there is a\n`yes_no_answer` for questions such as \"did larry co-found google\". The values\nfor this field `YES`, or `NO` if a yes/no answer is given. The default value is\n`NONE` when no yes/no answer is given. For statistics on long answers, short\nanswers, and yes/no answers, please see the data statistics section below.\n\n### Data Statistics\nThe NQ training data contains 307,373 examples. 152,148 have a long answer\nand 110,724 have a short answer. Short answers can be sets of spans in the document\n(106,926), or yes or no (3,798). Long answers are HTML bounding boxes, and the\ndistribution of NQ long answer types is as follows:\n\n| HTML tags | Percent of long answers |\n|-----------|-------------------------|\n| `<P>`     | 72.9%                   |\n| `<Table>` | 19.0%                   |\n| `<Tr>`    | 1.5%                    |\n| `<Ul>`, `<Ol>`, `<Dl>` | 3.2%       |\n| `<Li>`, `<Dd>`, `<Dt>` | 3.4%       |\n\nWhile we allow any paragraph, table, or list element to be a long answer,\nwe find that 95% of the long answers are not contained by any other\nlong answer candidate. We mark these `top level` candidates in the data,\nas described above.\n\nShort answers may contain more than one span, if the question is asking\nfor a list of answers (e.g. who made it to stage 3 in american ninja warrior season 9).\nHowever, almost all short answers (90%) only contain a single span of text.\nAll short answers are contained by the long answer given in the same annotation.\n\n# Prediction Format\nPlease see the [evaluation script](nq_eval.py) for a description of the prediction\nformat that your model should output.\n\n# Contact us\nIf you have a technical question regarding the dataset, code or publication, please\ncreate an issue in this repository. This is the fastest way to reach us.\n\nIf you would like to share feedback or report concerns, please email us at <natural-questions@google.com>.\n", "competition.md": "|WARNING: Once you have created your Docker image and uploaded it to the NQ competition site, you must  grant our service account read access. Otherwise your Docker images will be private and we won't be able to run them. |\n| :--- |\n| 1. Go to the storage tab in the gcloud console. |\n| 2. Locate the artifacts bucket. It should have a name like `artifacts.<project-name >.appspot.com` |\n| 3. Click the dropdown for the bucket and select \"Edit Bucket Permissions\". |\n| 4. Grant Storage Object Viewer permissions to the following user: `mljam-compute@mljam-205019.iam.gserviceaccount.com` |\n\n# Building a Docker Image for the Natural Questions Competition\nFirst, make sure that you have set up a profile as instructed on the\n[Natural Questions competition site](http://ai.google.com/research/NaturalQuestions/competition).\n\nYou must submit your model as a Docker image. You are allowed to use whatever\nsoftware dependencies you want, but those dependencies must be included in your\nDocker image. Let's say your model is a Tensorflow model. For this, you can use\nthe official tensorflow Docker container as the base container image:\n\n```dockerfile\nFROM tensorflow/tensorflow:latest\nADD nq_model /nq_model/\n```\n\nThe first line of this Dockerfile says to use the official Tensorflow Docker\nimage as the starting point of the image. The second line says to add the\ncontents of a directory called `nq_model` to a folder called\n`/nq_model` inside the image. Read the Docker manual for more details\non how to use Dockerfiles.\n\nThe folder `/nq_model` is expected to contain a script called\n`submission.sh`. The NQ test set is in a number of gzipped jsonl files\nwith exactly the same format as the released development set. During\nevaluation, the `/nq_model/submission.sh` script contained in\nyour Docker image will be called with an argument `input_path` that\nmatches the files containing the test set. Another argument `output_path`\ntells your code where to write predictions for each of the input examples.\nFor a complete description of the prediction format, please see the\n[evaluation script](nq_eval.py).\n\nBelow, we give an example `submission.sh` that works with the\n[`nq_export_scorer.py`](https://github.com/google-research/language/tree/master/language/question_answering/experiments/nq_export_scorer.py)\n executable released along with the NQ baselines.\n\n```shell\n#!/bin/bash\n#\n# submission.sh: The script to be launched in the Docker image.\n#\n# Usage: submission.sh <input_path> <output_path>\n#   input_path: File pattern (e.g. <input dir>/nq-test-??.jsonl.gz).\n#   output_path: Path to JSON file containing predictions (e.g. predictions.json).\n#\n# Sample usage:\n#   submission.sh input_path output_path\n\nINPUT_PATH=$1\nOUTPUT_PATH=$2\n\n# YOUR CODE HERE!\n#\n# For example, to run the baseline system from:\n#  https://github.com/google-research/language/tree/master/language/question_answering/experiments/nq_export_scorer.py)\n\npython -m language.question_answering.experiments.nq_export_scorer\n  --input_data_pattern=${INPUT_PATH} \\\n  --output_path=${OUTPUT_PATH} \\\n  --context_export_dir=<path_to_exported_long_answer_model_within_docker_image> \\\n  --entity_export_dir=<path_to_exported_short_answer_model_within_docker_image>\n```\n\nWhen you upload your Docker image to the\n[NQ competition site](http://ai.google.com/research/NaturalQuestions/competition),\n`/nq_model/submission.sh` will be called with `input_path` and\n`output_path` arguments that point to the test data input, and the output\nfile that will be fed to the evaluation script, respectively.\n\nRemember that each team is only allowed to make one submission per week to the\nNQ leaderboard. But you are allowed to run as many times as you like on\nthe 200 item sample that we provide so that you can test your uploaded Docker\nimage.\n", "nq_open/README.md": "# NQ-Open\n\nThe NQ-Open task, introduced by\n[Lee et.al. 2019](https://www.aclweb.org/anthology/P19-1612/), is an open domain\nquestion answering benchmark that is derived from\n[Natural Questions](https://ai.google.com/research/NaturalQuestions).\nThe goal is to predict an English answer string for an input English question.\nAll questions can be answered using the contents of English Wikipedia.\n\nThe NQ-Open task format was also used as part of the\n[EfficientQA competition](https://efficientqa.github.io/) at NeurIPS 2020. Results\nfrom the EfficientQA competition are reported in\n[Min et.al. 2021](https://arxiv.org/pdf/2101.00133.pdf).\n\nThe EfficientQA competition used *different dev and test splits from the original\nNQ-open task*. This repository contains both the original NQ-open data, as well\nas the EfficientQA data. Users should take care to ensure they are reporting\nmetrics on the correct splits. All work preceeding the EfficientQA competition,\nin December 2020, reports results on the NQ-open Dev split.\n\nThe different splits have all been created from Natural Questions data using\n[this conversion script](https://github.com/google-research/language/blob/master/language/orqa/preprocessing/convert_to_nq_open.py).\nSplit statistics are given below. More details on the data format, including\nthe various `answer` fields present in the EfficientQA test set, are given\nin the [Data Format](#data-format) section of this page.\n\n| Split               | Size   | Filename                           |\n|---------------------|--------|------------------------------------|\n| Train               | 87,925 | NQ-open.train.jsonl                |\n| Original Dev        | 3,610  | NQ-open.dev.jsonl                  |\n| EfficientQA Dev     | 1,800  | NQ-open.efficientqa.dev.1.1.jsonl  |\n| EfficientQA Test    | 1,769  | NQ-open.efficientqa.test.1.1.jsonl |\n\nAll of the Natural Questions data is released under the\n[CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/) license.\n\n## Data format\nAll of the data splits, apart from `NQ-open.efficientqa.test.1.1.jsonl`, contain\nthe following fields:\n\n```\nquestion: 'who signed the sugauli treaty on behalf of nepal'\nanswer: ['Raj Guru Gajaraj Mishra']\n```\n\nand predictions should be compared to the contents of the `answer` field\nusing the\n[NQ-open evaluation script](https://github.com/google-research/language/blob/master/language/orqa/evaluation/evaluate_predictions.py).\n\n### Answer fields in EfficientQA test\nAs part of the EfficientQA competition, predictions from the top performing\nsubmission were sent for further evaluation. The details of this evaluation\nare provided in [Min et.al. 2021](https://arxiv.org/pdf/2101.00133.pdf).\n\nInstead of a single `answer` field, the EfficientQA test set has the following\nfields containing reference answer strings.\n\n1. `answer`: contains the answers from the original NQ annotations,\n2. `def_correct_predictions`: contains predictions from top performing\nsubmissions that were determined to be definitely correct by annotators,\n3. `poss_correct_predictions`: contains preditions from top performing\nsubmissions that were determined to be possibly correct given some interpretation\nof the question.\n4. `answer_and_def_correct_predictions`: contains the union of\n`answer` and `def_correct_predictions`.\n\nWe include `poss_correct_predictions` in this release for completeness. However,\nwe *do not suggest that these are used for evaluation of any systems* since they\nrarely provide a properly satisfactory answer to the question. You can evaluate\nyour predictions on the standard `answer` references or the expanded\n`answer_and_def_correct_predictions` using the\n[evaluation_code](https://github.com/google-research/language/blob/master/language/orqa/evaluation/evaluate_predictions.py)\nwith the appropriate `answer_field` flag.\n\n\n\n\nFor further discussion of the data, as well as our recommendations for robust\nevaluation, please see [Min et.al. 2021](https://arxiv.org/pdf/2101.00133.pdf).\nAlso, please remember that almost all work to date has reported accuracy on the\noriginal NQ-open dev set described above. Any work that uses the EfficientQA\ntest set should describe this choice explicitly.\n\nDue to a bug in the post-competition labeling process, there may be small\ndiscrepancies between results calculated using the 1,769 examples released as\npart of the EfficientQA rated test data and the 1,800 examples used in the\noriginal EfficientQA leaderboard. Refer to\n[Min et.al. 2021](https://arxiv.org/pdf/2101.00133.pdf) for official\nresults.\n\n## Baselines\n\n| Method                                                                         | Original Dev | EfficientQa Dev | EfficientQa test |\n|--------------------------------------------------------------------------------|--------------|-----------------|------------------|\n| [TFIDF Nearest Question](https://arxiv.org/abs/2008.02637)                     | 22%          | 17%             | 16%              |\n| [REALM](https://github.com/google-research/language/tree/master/language/realm)| 40%          | 36%             | 35%              |\n| [T5XXL](https://efficientqa.github.io/getting_started.html)                    | 37%          | 32%             | 32%              |\n| [DPR](https://efficientqa.github.io/getting_started.html)                      | 41%          | 37%             | 36%              |\n| [DPR subset](https://efficientqa.github.io/getting_started.html)               | 35%          | 30%             | 30%              |\n\n## Citation\nIf you use this data, please cite\n[Kwiatkowski et.al. 2019](https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00276)\nand [Lee et.al. 2019](https://www.aclweb.org/anthology/P19-1612/).\n", "toy_example.md": "```json\n\"example_id\": 797803103760793766,\n\"question_text\": \"who founded google\",\n\"question_tokens\": [\"who\", \"founded\", \"google\"],\n\"document_url\": \"http://www.wikipedia.org/Google\",\n\"document_html\": \"<html><body><h1>Google Inc.</h1><p>Google was founded in 1998 By:<ul><li>Larry</li><li>Sergey</li></ul></p></body></html>\",\n\"document_tokens\":[\n \u00a0{ \"token\": \"<h1>\", \"start_byte\": 12, \"end_byte\": 16, \"html_token\": True },\n \u00a0{ \"token\": \"Google\", \"start_byte\": 16, \"end_byte\": 22, \"html_token\": False },\n \u00a0{ \"token\": \"inc\", \"start_byte\": 23, \"end_byte\": 26, \"html_token\": False },\n \u00a0{ \"token\": \".\", \"start_byte\": 26, \"end_byte\": 27, \"html_token\": False },\n \u00a0{ \"token\": \"</h1>\", \"start_byte\": 27, \"end_byte\": 32, \"html_token\": True },\n \u00a0{ \"token\": \"<p>\", \"start_byte\": 32, \"end_byte\": 35, \"html_token\": True },\n \u00a0{ \"token\": \"Google\", \"start_byte\": 35, \"end_byte\": 41, \"html_token\": False },\n \u00a0{ \"token\": \"was\", \"start_byte\": 42, \"end_byte\": 45, \"html_token\": False },\n \u00a0{ \"token\": \"founded\", \"start_byte\": 46, \"end_byte\": 53, \"html_token\": False },\n \u00a0{ \"Token\": \"in\", \"start_byte\": 54, \"end_byte\": 56, \"html_token\": False },\n \u00a0{ \"token\": \"1998\", \"start_byte\": 57, \"end_byte\": 61, \"html_token\": False },\n \u00a0{ \"token\": \"by\", \"start_byte\": 62, \"end_byte\": 64, \"html_token\": False },\n \u00a0{ \"token\": \":\", \"start_byte\": 64, \"end_byte\": 65, \"html_token\": False },\n \u00a0{ \"token\": \"<ul>\", \"start_byte\": 65, \"end_byte\": 69, \"html_token\": True },\n \u00a0{ \"token\": \"<li>\", \"start_byte\": 69, \"end_byte\": 73, \"html_token\": True },\n \u00a0{ \"token\": \"Larry\", \"start_byte\": 73, \"end_byte\": 78, \"html_token\": False },\n \u00a0{ \"token\": \"</li>\", \"start_byte\": 78, \"end_byte\": 83, \"html_token\": True },\n \u00a0{ \"token\": \"<li>\", \"start_byte\": 83, \"end_byte\": 87, \"html_token\": True },\n \u00a0{ \"token\": \"Sergey\", \"start_byte\": 87, \"end_byte\": 92, \"html_token\": False },\n \u00a0{ \"token\": \"</li>\", \"start_byte\": 92, \"end_byte\": 97, \"html_token\": True },\n \u00a0{ \"token\": \"</ul>\", \"start_byte\": 97, \"end_byte\": 102, \"html_token\": True },\n \u00a0{ \"token\": \"</p>\", \"start_byte\": 102, \"end_byte\": 106, \"html_token\": True }\n],\n\"long_answer_candidates\": [\n \u00a0{ \"start_byte\": 32, \"end_byte\": 106, \"start_token\": 5, \"end_token\": 22, \"top_level\": True },\n \u00a0{ \"start_byte\": 65, \"end_byte\": 102, \"start_token\": 13, \"end_token\": 21, \"top_level\": False },\n \u00a0{ \"start_byte\": 69, \"end_byte\": 83, \"start_token\": 14, \"end_token\": 17, \"top_level\": False },\n \u00a0{ \"start_byte\": 83, \"end_byte\": 92, \"start_token\": 17, \"end_token\": 20 , \"top_level\": False }\n],\n\"annotations\": [{\n \u00a0\"long_answer\": { \"start_byte\": 32, \"end_byte\": 106, \"start_token\": 5, \"end_token\": 22, \"candidate_index\": 0 },\n \u00a0\"short_answers\": [\n \u00a0\u00a0\u00a0{\"start_byte\": 73, \"end_byte\": 78, \"start_token\": 15, \"end_token\": 16},\n \u00a0\u00a0\u00a0{\"start_byte\": 87, \"end_byte\": 92, \"start_token\": 18, \"end_token\": 19}\n \u00a0],\n \u00a0\"yes_no_answer\": \"NONE\"\n}]\n```\n"}, "files_index": [{"path": "CONTRIBUTING.md", "type": "blob", "size": 1101}, {"path": "LICENSE", "type": "blob", "size": 11358}, {"path": "README.md", "type": "blob", "size": 9957}, {"path": "__init__.py", "type": "blob", "size": 616}, {"path": "competition.md", "type": "blob", "size": 3792}, {"path": "eval_utils.py", "type": "blob", "size": 10966}, {"path": "eval_utils_test.py", "type": "blob", "size": 2351}, {"path": "make_test_data.py", "type": "blob", "size": 4273}, {"path": "nq_browser.py", "type": "blob", "size": 11611}, {"path": "nq_eval.py", "type": "blob", "size": 16288}, {"path": "nq_eval_test.py", "type": "blob", "size": 7164}, {"path": "nq_open", "type": "tree", "size": null}, {"path": "nq_open/NQ-open.dev.jsonl", "type": "blob", "size": 391316}, {"path": "nq_open/NQ-open.efficientqa.dev.1.1.jsonl", "type": "blob", "size": 194998}, {"path": "nq_open/NQ-open.efficientqa.dev.1.1.no-annotations.jsonl", "type": "blob", "size": 116642}, {"path": "nq_open/NQ-open.efficientqa.dev.1.1.no-annotations.sample.jsonl", "type": "blob", "size": 6672}, {"path": "nq_open/NQ-open.efficientqa.dev.1.1.sample.jsonl", "type": "blob", "size": 11066}, {"path": "nq_open/NQ-open.efficientqa.test.1.1.jsonl", "type": "blob", "size": 595322}, {"path": "nq_open/NQ-open.efficientqa.test.1.1.no-annotations.jsonl", "type": "blob", "size": 113800}, {"path": "nq_open/NQ-open.train.jsonl", "type": "blob", "size": 8522298}, {"path": "nq_open/README.md", "type": "blob", "size": 5712}, {"path": "simplify_nq_data.py", "type": "blob", "size": 2567}, {"path": "static", "type": "tree", "size": null}, {"path": "static/nq.css", "type": "blob", "size": 202}, {"path": "templates", "type": "tree", "size": null}, {"path": "templates/features.html", "type": "blob", "size": 2046}, {"path": "templates/index.html", "type": "blob", "size": 1572}, {"path": "test_docker.sh", "type": "blob", "size": 1152}, {"path": "text_utils.py", "type": "blob", "size": 5174}, {"path": "toy_example.md", "type": "blob", "size": 2847}], "contributors": {"anon:tomkwiat": 11, "chrisgorgo": 5, "daphnelg": 2}, "_source": {"fetched_at": 1758915537.4444375, "api_base": "https://api.github.com/repos/google-research-datasets/natural-questions", "limits": {"max_files": 25, "max_total_bytes": 1000000}}}, "fetched_at": 1758915537.4444375}, "google/jax": {"payload": {"url": "https://github.com/jax-ml/jax", "repo_id": "google/jax", "repo_type": "code", "name": "jax", "full_name": "jax-ml/jax", "description": "Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more", "homepage": "https://docs.jax.dev", "default_branch": "main", "topics": ["jax"], "language": "Python", "archived": false, "disabled": false, "fork": false, "created_at": "2018-10-25T21:25:02Z", "updated_at": "2025-09-26T19:13:52Z", "pushed_at": "2025-09-26T19:32:30Z", "stars": 33542, "forks": 3185, "open_issues": 2166, "watchers": 326, "license_spdx": "Apache-2.0", "readme_text": "<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/jax-ml/jax/main/images/jax_logo_250px.png\" alt=\"logo\"></img>\n</div>\n\n# Transformable numerical computing at scale\n\n[![Continuous integration](https://github.com/jax-ml/jax/actions/workflows/ci-build.yaml/badge.svg)](https://github.com/jax-ml/jax/actions/workflows/ci-build.yaml)\n[![PyPI version](https://img.shields.io/pypi/v/jax)](https://pypi.org/project/jax/)\n\n[**Transformations**](#transformations)\n| [**Scaling**](#scaling)\n| [**Install guide**](#installation)\n| [**Change logs**](https://docs.jax.dev/en/latest/changelog.html)\n| [**Reference docs**](https://docs.jax.dev/en/latest/)\n\n\n## What is JAX?\n\nJAX is a Python library for accelerator-oriented array computation and program transformation,\ndesigned for high-performance numerical computing and large-scale machine learning.\n\nJAX can automatically differentiate native\nPython and NumPy functions. It can differentiate through loops, branches,\nrecursion, and closures, and it can take derivatives of derivatives of\nderivatives. It supports reverse-mode differentiation (a.k.a. backpropagation)\nvia [`jax.grad`](#automatic-differentiation-with-grad) as well as forward-mode differentiation,\nand the two can be composed arbitrarily to any order.\n\nJAX uses [XLA](https://www.openxla.org/xla)\nto compile and scale your NumPy programs on TPUs, GPUs, and other hardware accelerators.\nYou can compile your own pure functions with [`jax.jit`](#compilation-with-jit).\nCompilation and automatic differentiation can be composed arbitrarily.\n\nDig a little deeper, and you'll see that JAX is really an extensible system for\n[composable function transformations](#transformations) at [scale](#scaling).\n\nThis is a research project, not an official Google product. Expect\n[sharp edges](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html).\nPlease help by trying it out, [reporting bugs](https://github.com/jax-ml/jax/issues),\nand letting us know what you think!\n\n```python\nimport jax\nimport jax.numpy as jnp\n\ndef predict(params, inputs):\n  for W, b in params:\n    outputs = jnp.dot(inputs, W) + b\n    inputs = jnp.tanh(outputs)  # inputs to the next layer\n  return outputs                # no activation on last layer\n\ndef loss(params, inputs, targets):\n  preds = predict(params, inputs)\n  return jnp.sum((preds - targets)**2)\n\ngrad_loss = jax.jit(jax.grad(loss))  # compiled gradient evaluation function\nperex_grads = jax.jit(jax.vmap(grad_loss, in_axes=(None, 0, 0)))  # fast per-example grads\n```\n\n### Contents\n* [Transformations](#transformations)\n* [Scaling](#scaling)\n* [Current gotchas](#gotchas-and-sharp-bits)\n* [Installation](#installation)\n* [Citing JAX](#citing-jax)\n* [Reference documentation](#reference-documentation)\n\n## Transformations\n\nAt its core, JAX is an extensible system for transforming numerical functions.\nHere are three: `jax.grad`, `jax.jit`, and `jax.vmap`.\n\n### Automatic differentiation with `grad`\n\nUse [`jax.grad`](https://docs.jax.dev/en/latest/jax.html#jax.grad)\nto efficiently compute reverse-mode gradients:\n\n```python\nimport jax\nimport jax.numpy as jnp\n\ndef tanh(x):\n  y = jnp.exp(-2.0 * x)\n  return (1.0 - y) / (1.0 + y)\n\ngrad_tanh = jax.grad(tanh)\nprint(grad_tanh(1.0))\n# prints 0.4199743\n```\n\nYou can differentiate to any order with `grad`:\n\n```python\nprint(jax.grad(jax.grad(jax.grad(tanh)))(1.0))\n# prints 0.62162673\n```\n\nYou're free to use differentiation with Python control flow:\n\n```python\ndef abs_val(x):\n  if x > 0:\n    return x\n  else:\n    return -x\n\nabs_val_grad = jax.grad(abs_val)\nprint(abs_val_grad(1.0))   # prints 1.0\nprint(abs_val_grad(-1.0))  # prints -1.0 (abs_val is re-evaluated)\n```\n\nSee the [JAX Autodiff\nCookbook](https://docs.jax.dev/en/latest/notebooks/autodiff_cookbook.html)\nand the [reference docs on automatic\ndifferentiation](https://docs.jax.dev/en/latest/jax.html#automatic-differentiation)\nfor more.\n\n### Compilation with `jit`\n\nUse XLA to compile your functions end-to-end with\n[`jit`](https://docs.jax.dev/en/latest/jax.html#just-in-time-compilation-jit),\nused either as an `@jit` decorator or as a higher-order function.\n\n```python\nimport jax\nimport jax.numpy as jnp\n\ndef slow_f(x):\n  # Element-wise ops see a large benefit from fusion\n  return x * x + x * 2.0\n\nx = jnp.ones((5000, 5000))\nfast_f = jax.jit(slow_f)\n%timeit -n10 -r3 fast_f(x)\n%timeit -n10 -r3 slow_f(x)\n```\n\nUsing `jax.jit` constrains the kind of Python control flow\nthe function can use; see\nthe tutorial on [Control Flow and Logical Operators with JIT](https://docs.jax.dev/en/latest/control-flow.html)\nfor more.\n\n### Auto-vectorization with `vmap`\n\n[`vmap`](https://docs.jax.dev/en/latest/jax.html#vectorization-vmap) maps\na function along array axes.\nBut instead of just looping over function applications, it pushes the loop down\nonto the function\u2019s primitive operations, e.g. turning matrix-vector multiplies into\nmatrix-matrix multiplies for better performance.\n\nUsing `vmap` can save you from having to carry around batch dimensions in your\ncode:\n\n```python\nimport jax\nimport jax.numpy as jnp\n\ndef l1_distance(x, y):\n  assert x.ndim == y.ndim == 1  # only works on 1D inputs\n  return jnp.sum(jnp.abs(x - y))\n\ndef pairwise_distances(dist1D, xs):\n  return jax.vmap(jax.vmap(dist1D, (0, None)), (None, 0))(xs, xs)\n\nxs = jax.random.normal(jax.random.key(0), (100, 3))\ndists = pairwise_distances(l1_distance, xs)\ndists.shape  # (100, 100)\n```\n\nBy composing `jax.vmap` with `jax.grad` and `jax.jit`, we can get efficient\nJacobian matrices, or per-example gradients:\n\n```python\nper_example_grads = jax.jit(jax.vmap(jax.grad(loss), in_axes=(None, 0, 0)))\n```\n\n## Scaling\n\nTo scale your computations across thousands of devices, you can use any\ncomposition of these:\n* [**Compiler-based automatic parallelization**](https://docs.jax.dev/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html)\nwhere you program as if using a single global machine, and the compiler chooses\nhow to shard data and partition computation (with some user-provided constraints);\n* [**Explicit sharding and automatic partitioning**](https://docs.jax.dev/en/latest/notebooks/explicit-sharding.html)\nwhere you still have a global view but data shardings are\nexplicit in JAX types, inspectable using `jax.typeof`;\n* [**Manual per-device programming**](https://docs.jax.dev/en/latest/notebooks/shard_map.html)\nwhere you have a per-device view of data\nand computation, and can communicate with explicit collectives.\n\n| Mode | View? | Explicit sharding? | Explicit Collectives? |\n|---|---|---|---|\n| Auto | Global | \u274c | \u274c |\n| Explicit | Global | \u2705 | \u274c |\n| Manual | Per-device | \u2705 | \u2705 |\n\n```python\nfrom jax.sharding import set_mesh, AxisType, PartitionSpec as P\nmesh = jax.make_mesh((8,), ('data',), axis_types=(AxisType.Explicit,))\nset_mesh(mesh)\n\n# parameters are sharded for FSDP:\nfor W, b in params:\n  print(f'{jax.typeof(W)}')  # f32[512@data,512]\n  print(f'{jax.typeof(b)}')  # f32[512]\n\n# shard data for batch parallelism:\ninputs, targets = jax.device_put((inputs, targets), P('data'))\n\n# evaluate gradients, automatically parallelized!\ngradfun = jax.jit(jax.grad(loss))\nparam_grads = gradfun(params, (inputs, targets))\n```\n\nSee the [tutorial](https://docs.jax.dev/en/latest/sharded-computation.html) and\n[advanced guides](https://docs.jax.dev/en/latest/advanced_guide.html) for more.\n\n## Gotchas and sharp bits\n\nSee the [Gotchas\nNotebook](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html).\n\n## Installation\n\n### Supported platforms\n\n|            | Linux x86_64 | Linux aarch64 | Mac aarch64  | Windows x86_64 | Windows WSL2 x86_64 |\n|------------|--------------|---------------|--------------|----------------|---------------------|\n| CPU        | yes          | yes           | yes          | yes            | yes                 |\n| NVIDIA GPU | yes          | yes           | n/a          | no             | experimental        |\n| Google TPU | yes          | n/a           | n/a          | n/a            | n/a                 |\n| AMD GPU    | yes          | no            | n/a          | no             | experimental        |\n| Apple GPU  | n/a          | no            | experimental | n/a            | n/a                 |\n| Intel GPU  | experimental | n/a           | n/a          | no             | no                  |\n\n\n### Instructions\n\n| Platform        | Instructions                                                                                                    |\n|-----------------|-----------------------------------------------------------------------------------------------------------------|\n| CPU             | `pip install -U jax`                                                                                            |\n| NVIDIA GPU      | `pip install -U \"jax[cuda13]\"`                                                                                  |\n| Google TPU      | `pip install -U \"jax[tpu]\"`                                                                                     |\n| AMD GPU (Linux) | Follow [AMD's instructions](https://github.com/jax-ml/jax/blob/main/build/rocm/README.md).                      |\n| Mac GPU         | Follow [Apple's instructions](https://developer.apple.com/metal/jax/).                                          |\n| Intel GPU       | Follow [Intel's instructions](https://github.com/intel/intel-extension-for-openxla/blob/main/docs/acc_jax.md).  |\n\nSee [the documentation](https://docs.jax.dev/en/latest/installation.html)\nfor information on alternative installation strategies. These include compiling\nfrom source, installing with Docker, using other versions of CUDA, a\ncommunity-supported conda build, and answers to some frequently-asked questions.\n\n## Citing JAX\n\nTo cite this repository:\n\n```\n@software{jax2018github,\n  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},\n  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},\n  url = {http://github.com/jax-ml/jax},\n  version = {0.3.13},\n  year = {2018},\n}\n```\n\nIn the above bibtex entry, names are in alphabetical order, the version number\nis intended to be that from [jax/version.py](../main/jax/version.py), and\nthe year corresponds to the project's open-source release.\n\nA nascent version of JAX, supporting only automatic differentiation and\ncompilation to XLA, was described in a [paper that appeared at SysML\n2018](https://mlsys.org/Conferences/2019/doc/2018/146.pdf). We're currently working on\ncovering JAX's ideas and capabilities in a more comprehensive and up-to-date\npaper.\n\n## Reference documentation\n\nFor details about the JAX API, see the\n[reference documentation](https://docs.jax.dev/).\n\nFor getting started as a JAX developer, see the\n[developer documentation](https://docs.jax.dev/en/latest/developer.html).\n", "doc_texts": {".github/ISSUE_TEMPLATE/Feature_request.md": "---\nname: 'Feature Request'\nabout: 'Suggest a new idea or improvement for JAX'\nlabels: 'enhancement'\n---\n\nPlease:\n\n- [ ] Check for duplicate requests.\n- [ ] Describe your goal, and if possible provide a code snippet with a motivating example.\n", ".github/workflows/README.md": "# Github Actions workflows\n\nSee the Github documentation for more information on Github Actions in general.\n\n## Notes\n\n* <https://opensource.google/documentation/reference/github/services#actions>\n  mandates using a specific commit for non-Google actions. We use\n  [Ratchet](https://github.com/sethvargo/ratchet) to pin specific versions.  If\n  you'd like to update an action, you can write something like `uses:\n  'actions/checkout@v4'`, and then run `./ratchet pin workflow.yml` to convert\n  to a commit hash. See the Ratchet README for installation and more detailed\n  instructions.\n", ".github/workflows/self_hosted_runner_utils/README.md": "Configuration files for self-hosted Github Actions runners. We use self-hosted\nCloud TPU VM runners for TPU CI.\n\nGooglers, see go/jax-self-hosted-runners for more information.\n", "CHANGELOG.md": "# Change log\n\nBest viewed [here](https://docs.jax.dev/en/latest/changelog.html).\nFor the changes specific to the experimental Pallas APIs,\nsee {ref}`pallas-changelog`.\n\nJAX follows Effort-based versioning; for a discussion of this and JAX's API\ncompatibility policy, refer to {ref}`api-compatibility`. For the Python and\nNumPy version support policy, refer to {ref}`version-support-policy`.\n\n<!--\nRemember to align the itemized text with the first line of an item within a list.\n\nWhen releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.md.\n-->\n\n## Unreleased\n\n* Breaking changes:\n\n  * JAX no longer accepts `Array` values where a `dtype` value is expected. Call\n    `.dtype` on these values first.\n  * The deprecated function {func}`jax.interpreters.mlir.custom_call` was\n    removed.\n  * The `jax.util`, `jax.extend.ffi`, and `jax.experimental.host_callback`\n    modules have been removed. All public APIs within these modules were\n    deprecated and removed in v0.7.0 or earlier.\n  * The deprecated symbol {obj}`jax.custom_derivatives.custom_jvp_call_jaxpr_p`\n    was removed.\n  * from {mod}`jax.experimental.compilation_cache`, the deprecated symbols\n    `is_initialized` and `initialize_cache` were removed.\n  * The deprecated function {func}`jax.interpreters.xla.canonicalize_dtype`\n    was removed.\n  * {mod}`jaxlib.hlo_helpers` has been removed. Use {mod}`jax.ffi` instead.\n  * The option `jax_cpu_enable_gloo_collectives` has been removed. Use\n    `jax_cpu_collectives_implementation` instead.\n  * The previously-deprecated `interpolation` argument to\n    {func}`jax.numpy.percentile` and {func}`jax.numpy.quantile` has been\n    removed; use `method` instead.\n  * The JAX-internal `for_loop` primitive was removed. Its functionality,\n    reading from and writing to refs in the loop body, is now directly\n    supported by {func}`jax.lax.fori_loop`. If you need help updating your\n    code, please file a bug.\n  * {func}`jax.numpy.trimzeros` now errors for non-1D input.\n  * The `where` argument to {func}`jax.numpy.sum` and other reductions is now\n    required to be boolean. Non-boolean values have resulted in a\n    `DeprecationWarning` since JAX v0.5.0.\n  * The deprecated functions in {mod} `jax.dlpack`, {mod} `jax.errors`, {mod}\n    `jax.lib.xla_bridge`, {mod} `jax.lib.xla_client`, and {mod}\n    `jax.lib.xla_extension` were removed.\n\n* Changes\n  * `jax.grad` and `jax.vjp` will now round always primals to float32 if float64\n    mode is not enabled.\n\n\n## JAX 0.7.2 (September 16, 2025)\n\n* Breaking changes:\n\n  * {func}`jax.dlpack.from_dlpack` no longer accepts a DLPack capsule. This\n    behavior was deprecated and is now removed. The function must be called\n    with an array implementing `__dlpack__` and `__dlpack_device__`.\n\n* Changes\n  * The minimum supported NumPy version is now 2.0. Since SciPy 1.13 is required\n    for NumPy 2.0 support, the minimum supported SciPy version is now 1.13.\n\n  * JAX now represents constants in its internal jaxpr representation as a\n    `TypedNdArray`, which is a private JAX type that duck types as a\n    `numpy.ndarray`. This type may be exposed to users via `custom_jvp` rules,\n    for example, and may break code that uses `isinstance(x, np.ndarray)`. If\n    this breaks your code, you may convert these arrays to classic NumPy arrays\n    using `np.asarray(x)`.\n\n* Bug fixes\n  * `arr.view(dtype=None)` now returns the array unchanged, matching NumPy's\n    semantics. Previously it returned the array with a float dtype.\n  * `jax.random.randint` now produces a less-biased distribution for 8-bit and\n    16-bit integer types ({jax-issue}`#27742`). To restore the previous biased\n    behavior, you may temporarily set the `jax_safer_randint` configuration to\n    `False`, but note this is a temporary config that will be removed in a\n    future release.\n\n* Deprecations:\n  * The parameters `enable_xla` and `native_serialization` for `jax2tf.convert`\n    are deprecated and will be removed in a future version of JAX. These were\n    used for jax2tf with non-native serialization, which has been now removed.\n  * Setting the config state `jax_pmap_no_rank_reduction` to `False` is\n    deprecated. By default, `jax_pmap_no_rank_reduction` will be set to `True`\n    and `jax.pmap` shards will not have their rank reduced, keeping the same\n    rank as their enclosing array.\n\n## JAX 0.7.1 (August 20, 2025)\n\n* New features\n  * JAX now ships Python 3.14 and 3.14t wheels.\n  * JAX now ships Python 3.13t and 3.14t wheels on Mac. Previously we only\n    offered free-threading builds on Linux.\n\n* Changes\n  * Exposed `jax.set_mesh` which acts as a global setter and a context manager.\n    Removed `jax.sharding.use_mesh` in favor of `jax.set_mesh`.\n  * JAX is now built using CUDA 12.9. All versions of CUDA 12.1 or newer remain\n    supported.\n  * {func}`jax.lax.dot` now implements the general dot product via the optional\n    ``dimension_numbers`` argument.\n\n* Deprecations:\n\n  * {func}`jax.lax.zeros_like_array` is deprecated. Please use\n    {func}`jax.numpy.zeros_like` instead.\n  * Attempting to import {mod}`jax.experimental.host_callback` now results in\n    a `DeprecationWarning`, and will result in an `ImportError` starting in JAX\n    v0.8.0. Its APIs have raised `NotImplementedError` since JAX version 0.4.35.\n  * In {func}`jax.lax.dot`, passing the ``precision`` and ``preferred_element_type``\n    arguments by position is deprecated. Pass them by explicit keyword instead.\n  * Several dozen internal APIs have been deprecated from {mod}`jax.interpreters.ad`,\n    {mod}`jax.interpreters.batching`, and {mod}`jax.interpreters.partial_eval`; they\n    are used rarely if ever outside JAX itself, and most are deprecated without any\n    public replacement.\n\n\n## JAX 0.7.0 (July 22, 2025)\n\n* New features:\n  * Added `jax.P` which is an alias for `jax.sharding.PartitionSpec`.\n  * Added {func}`jax.tree.reduce_associative`.\n  * The {attr}`jax.numpy.ndarray.at` indexing methods now support a `wrap_negative_indices`\n    argument, which defaults to `True` to match the current behavior ({jax-issue}`#29434`).\n\n* Breaking changes:\n  * JAX is migrating from GSPMD to Shardy by default. See the\n    [migration guide](https://docs.jax.dev/en/latest/shardy_jax_migration.html)\n    for more information.\n  * JAX autodiff is switching to using direct linearization by default (instead of\n    implementing linearization via JVP and partial eval).\n    See [migration guide](https://docs.jax.dev/en/latest/direct_linearize_migration.html)\n    for more information.\n  * `jax.stages.OutInfo` has been replaced with `jax.ShapeDtypeStruct`.\n  * {func}`jax.jit` now requires `fun` to be passed by position, and additional\n    arguments to be passed by keyword. Doing otherwise will result in an error\n    starting in v0.7.x. This raised a DeprecationWarning in v0.6.x.\n  * The minimum Python version is now 3.11. 3.11 will remain the minimum\n    supported version until July 2026.\n  * Layout API renames:\n    * `Layout`, `.layout`, `.input_layouts` and `.output_layouts` have been\n      renamed to `Format`, `.format`, `.input_formats` and `.output_formats`\n    * `DeviceLocalLayout`, `.device_local_layout` have been renamed to `Layout`\n      and `.layout`\n  * `jax.experimental.shard` module has been deleted and all the APIs have been\n    moved to the `jax.sharding` endpoint. So use `jax.sharding.reshard`,\n    `jax.sharding.auto_axes` and `jax.sharding.explicit_axes` instead of their\n    experimental endpoints.\n  * `lax.infeed` and `lax.outfeed` were removed, after being deprecated in\n    JAX 0.6. The `transfer_to_infeed` and `transfer_from_outfeed` methods were\n    also removed the `Device` objects.\n  * The `jax.extend.core.primitives.pjit_p` primitive has been renamed to\n    `jit_p`, and its `name` attribute has changed from `\"pjit\"` to `\"jit\"`.\n    This affects the string representations of jaxprs. The same primitive is no\n    longer exported from the `jax.experimental.pjit` module.\n  * The (undocumented) function `jax.extend.backend.add_clear_backends_callback`\n    has been removed. Users should use `jax.extend.backend.register_backend_cache`\n    instead.\n  * `out_sharding` arg added to `x.at[y].set` and `x.at[y].add`. Previous\n    behavior propagating operand sharding removed. Please use\n    `x.at[y].set/add(z, out_sharding=jax.typeof(x).sharding)` to retain previous\n    behavior if scatter op requires collectives.\n\n* Deprecations:\n  * {obj}`jax.dlpack.SUPPORTED_DTYPES` is deprecated; please use the new\n    {func}`jax.dlpack.is_supported_dtype` function.\n  * {func}`jax.scipy.special.sph_harm` has been deprecated following a similar\n    deprecation in SciPy; use {func}`jax.scipy.special.sph_harm_y` instead.\n  * From {mod}`jax.interpreters.xla`, the previously deprecated symbols\n    `abstractify` and `pytype_aval_mappings` have been removed.\n  * {func}`jax.interpreters.xla.canonicalize_dtype` is deprecated. For\n    canonicalizing dtypes, prefer {func}`jax.dtypes.canonicalize_dtype`.\n    For checking whether an object is a valid jax input, prefer\n    {func}`jax.core.valid_jaxtype`.\n  * From {mod}`jax.core`, the previously deprecated symbols `AxisName`,\n    `ConcretizationTypeError`, `axis_frame`, `call_p`, `closed_call_p`,\n    `get_type`, `trace_state_clean`, `typematch`, and `typecheck` have been\n    removed.\n  * From {mod}`jax.lib.xla_client`, the previously deprecated symbols\n    `DeviceAssignment`, `get_topology_for_devices`, and `mlir_api_version`\n    have been removed.\n  * `jax.extend.ffi` was removed after being deprecated in v0.5.0.\n    Use {mod}`jax.ffi` instead.\n  * {func}`jax.lib.xla_bridge.get_compile_options` is deprecated, and replaced by\n    {func}`jax.extend.backend.get_compile_options`.\n\n## JAX 0.6.2 (June 17, 2025)\n\n* New features:\n  * Added {func}`jax.tree.broadcast` which implements a pytree prefix broadcasting helper.\n\n* Changes\n  * The minimum NumPy version is 1.26 and the minimum SciPy version is 1.12.\n\n## JAX 0.6.1 (May 21, 2025)\n\n* New features:\n  * Added {func}`jax.lax.axis_size` which returns the size of the mapped axis\n    given its name.\n\n* Changes\n  * Additional checking for the versions of CUDA package dependencies was\n    re-enabled, having been accidentally disabled in a previous release.\n  * JAX nightly packages are now published to artifact registry. To install\n    these packages, see the [JAX installation guide](https://docs.jax.dev/en/latest/installation.html#jax-nightly-installation).\n  * `jax.sharding.PartitionSpec` no longer inherits from a tuple.\n  * `jax.ShapeDtypeStruct` is immutable now. Please use `.update` method to\n    update your `ShapeDtypeStruct` instead of doing in-place updates.\n\n* Deprecations\n  * `jax.custom_derivatives.custom_jvp_call_jaxpr_p` is deprecated, and will be\n    removed in JAX v0.7.0.\n\n## JAX 0.6.0 (April 16, 2025)\n\n* Breaking changes\n\n  * {func}`jax.numpy.array` no longer accepts `None`. This behavior was\n    deprecated since November 2023 and is now removed.\n  * Removed the `config.jax_data_dependent_tracing_fallback` config option,\n    which was added temporarily in v0.4.36 to allow users to opt out of the\n    new \"stackless\" tracing machinery.\n  * Removed the `config.jax_eager_pmap` config option.\n  * Disallow the calling of `lower` and `trace` AOT APIs on the result\n    of `jax.jit` if there have been subsequent wrappers applied.\n    Previously this worked, but silently ignored the wrappers.\n    The workaround is to apply `jax.jit` last among the wrappers,\n    and similarly for `jax.pmap`.\n    See {jax-issue}`#27873`.\n  * The `cuda12_pip` extra for `jax` has been removed; use `pip install jax[cuda12]`\n    instead.\n\n* Changes\n  * The minimum CuDNN version is v9.8.\n  * JAX is now built using CUDA 12.8. All versions of CUDA 12.1 or newer remain\n    supported.\n  * JAX package extras are now updated to use dash instead of underscore to\n    align with PEP 685. For instance, if you were previously using `pip install jax[cuda12_local]`\n    to install JAX, run `pip install jax[cuda12-local]` instead.\n  * {func}`jax.jit` now requires `fun` to be passed by position, and additional\n    arguments to be passed by keyword. Doing otherwise will result in a\n    DeprecationWarning in v0.6.X, and an error in starting in v0.7.X.\n\n* Deprecations\n\n  * {func}`jax.tree_util.build_tree` is deprecated. Use {func}`jax.tree.unflatten`\n    instead.\n  * Implemented host callback handlers for CPU and GPU devices using XLA's FFI\n    and removed existing CPU/GPU handlers using XLA's custom call.\n  * All APIs in `jax.lib.xla_extension` are now deprecated.\n  * `jax.interpreters.mlir.hlo` and `jax.interpreters.mlir.func_dialect`,\n    which were accidental exports, have been removed. If needed, they are\n    available from `jax.extend.mlir`.\n  * `jax.interpreters.mlir.custom_call` is deprecated. The APIs provided by\n    {mod}`jax.ffi` should be used instead.\n  * The deprecated use of {func}`jax.ffi.ffi_call` with inline arguments is no\n    longer supported. {func}`~jax.ffi.ffi_call` now unconditionally returns a\n    callable.\n  * The following exports in `jax.lib.xla_client` are deprecated:\n    `get_topology_for_devices`, `heap_profile`, `mlir_api_version`, `Client`,\n    `CompileOptions`, `DeviceAssignment`, `Frame`, `HloSharding`, `OpSharding`,\n    `Traceback`.\n  * The following internal APIs in `jax.util` are deprecated:\n    `HashableFunction`, `as_hashable_function`, `cache`, `safe_map`, `safe_zip`,\n    `split_dict`, `split_list`, `split_list_checked`, `split_merge`, `subvals`,\n    `toposort`, `unzip2`, `wrap_name`, and `wraps`.\n  * `jax.dlpack.to_dlpack` has been deprecated. You can usually pass a JAX\n    `Array` directly to the `from_dlpack` function of another framework. If you\n    need the functionality of `to_dlpack`, use the `__dlpack__` attribute of an\n    array.\n  * `jax.lax.infeed`, `jax.lax.infeed_p`, `jax.lax.outfeed`, and\n    `jax.lax.outfeed_p` are deprecated and will be removed in JAX v0.7.0.\n  * Several previously-deprecated APIs have been removed, including:\n    * From `jax.lib.xla_client`: `ArrayImpl`, `FftType`, `PaddingType`,\n      `PrimitiveType`, `XlaBuilder`, `dtype_to_etype`,\n      `ops`, `register_custom_call_target`, `shape_from_pyval`, `Shape`,\n      `XlaComputation`.\n    * From `jax.lib.xla_extension`: `ArrayImpl`, `XlaRuntimeError`.\n    * From `jax`: `jax.treedef_is_leaf`, `jax.tree_flatten`, `jax.tree_map`,\n      `jax.tree_leaves`, `jax.tree_structure`, `jax.tree_transpose`, and\n      `jax.tree_unflatten`. Replacements can be found in {mod}`jax.tree` or\n      {mod}`jax.tree_util`.\n    * From `jax.core`: `AxisSize`, `ClosedJaxpr`, `EvalTrace`, `InDBIdx`, `InputType`,\n      `Jaxpr`, `JaxprEqn`, `Literal`, `MapPrimitive`, `OpaqueTraceState`, `OutDBIdx`,\n      `Primitive`, `Token`, `TRACER_LEAK_DEBUGGER_WARNING`, `Var`, `concrete_aval`,\n      `dedup_referents`, `escaped_tracer_error`, `extend_axis_env_nd`, `full_lower`,  `get_referent`, `jaxpr_as_fun`, `join_effects`, `lattice_join`,\n      `leaked_tracer_error`, `maybe_find_leaked_tracers`, `raise_to_shaped`,\n      `raise_to_shaped_mappings`, `reset_trace_state`, `str_eqn_compact`,\n      `substitute_vars_in_output_ty`, `typecompat`, and `used_axis_names_jaxpr`. Most\n      have no public replacement, though a few are available at {mod}`jax.extend.core`.\n    * The `vectorized` argument to {func}`~jax.pure_callback` and\n      {func}`~jax.ffi.ffi_call`. Use the `vmap_method` parameter instead.\n\n## jax 0.5.3 (Mar 19, 2025)\n\n* New Features\n\n  * Added a `allow_negative_indices` option to {func}`jax.lax.dynamic_slice`,\n    {func}`jax.lax.dynamic_update_slice` and related functions. The default is\n    true, matching the current behavior. If set to false, JAX does not need to\n    emit code clamping negative indices, which improves code size.\n  * Added a `replace` option to {func}`jax.random.categorical` to enable sampling\n    without replacement.\n\n## jax 0.5.2 (Mar 4, 2025)\n\nPatch release of 0.5.1\n\n* Bug fixes\n  * Fixes TPU metric logging and `tpu-info`, which was broken in 0.5.1\n\n## jax 0.5.1 (Feb 24, 2025)\n\n* Breaking changes\n  * The jit tracing cache now keys on input NamedShardings. Previously, the\n    tracing cache did not include sharding information at all\n    (although subsequent jit caches did like lowering and compilation caches),\n    so two equivalent shardings of different types would not retrace,\n    but now they do. For example:\n    ```python\n    @jax.jit\n    def f(x):\n      return x\n\n    # inp1.sharding is of type SingleDeviceSharding\n    inp1 = jnp.arange(8)\n    f(inp1)\n\n    mesh = jax.make_mesh((1,), ('x',))\n    # inp2.sharding is of type NamedSharding\n    inp2 = jax.device_put(jnp.arange(8), NamedSharding(mesh, P('x')))\n    f(inp2)  # tracing cache miss\n    ```\n    In the above example, calling `f(inp1)` and then `f(inp2)` will lead to a\n    tracing cache miss because the shardings have changed on the abstract values\n    while tracing.\n\n* New Features\n  * Added an experimental {func}`jax.experimental.custom_dce.custom_dce`\n    decorator to support customizing the behavior of opaque functions under\n    JAX-level dead code elimination (DCE). See {jax-issue}`#25956` for more\n    details.\n  * Added low-level reduction APIs in {mod}`jax.lax`: {func}`jax.lax.reduce_sum`,\n    {func}`jax.lax.reduce_prod`, {func}`jax.lax.reduce_max`, {func}`jax.lax.reduce_min`,\n    {func}`jax.lax.reduce_and`, {func}`jax.lax.reduce_or`, and {func}`jax.lax.reduce_xor`.\n  * {func}`jax.lax.linalg.qr`, and {func}`jax.scipy.linalg.qr`, now support\n    column-pivoting on CPU and GPU. See {jax-issue}`#20282` and\n  * Added {func}`jax.random.multinomial`.\n    {jax-issue}`#25955` for more details.\n\n* Changes\n  * `JAX_CPU_COLLECTIVES_IMPLEMENTATION` and `JAX_NUM_CPU_DEVICES` now work as\n    env vars. Before they could only be specified via jax.config or flags.\n  * `JAX_CPU_COLLECTIVES_IMPLEMENTATION` now defaults to `'gloo'`, meaning\n    multi-process CPU communication works out-of-the-box.\n  * The `jax[tpu]` TPU extra no longer depends on the `libtpu-nightly` package.\n    This package may safely be removed if it is present on your machine; JAX now\n    uses `libtpu` instead.\n\n* Deprecations\n  * The internal function `linear_util.wrap_init` and the constructor\n    `core.Jaxpr` now must take a non-empty `core.DebugInfo` kwarg. For\n    a limited time, a `DeprecationWarning` is printed if\n    `jax.extend.linear_util.wrap_init` is used without debugging info.\n    A downstream effect of this several other internal functions need debug\n    info. This change does not affect public APIs.\n    See https://github.com/jax-ml/jax/issues/26480 for more detail.\n  * In {func}`jax.numpy.ndim`, {func}`jax.numpy.shape`, and {func}`jax.numpy.size`,\n    non-arraylike inputs (such as lists, tuples, etc.) are now deprecated.\n\n* Bug fixes\n  * TPU runtime startup and shutdown time should be significantly improved on\n    TPU v5e and newer (from around 17s to around 8s). If not already set, you may\n    need to enable transparent hugepages in your VM image\n    (`sudo sh -c 'echo always > /sys/kernel/mm/transparent_hugepage/enabled'`).\n    We hope to improve this further in future releases.\n  * Persistent compilation cache no longer writes access time file if\n    JAX_COMPILATION_CACHE_MAX_SIZE is unset or set to -1, i.e. if the LRU\n    eviction policy isn't enabled. This should improve performance when using\n    the cache with large-scale network storage.\n\n## jax 0.5.0 (Jan 17, 2025)\n\nAs of this release, JAX now uses\n[effort-based versioning](https://docs.jax.dev/en/latest/jep/25516-effver.html).\nSince this release makes a breaking change to PRNG key semantics that\nmay require users to update their code, we are bumping the \"meso\" version of JAX\nto signify this.\n\n* Breaking changes\n  * Enable `jax_threefry_partitionable` by default (see\n    [the update note](https://github.com/jax-ml/jax/discussions/18480)).\n\n  * This release drops support for Mac x86 wheels. Mac ARM of course remains\n    supported. For a recent discussion, see\n    https://github.com/jax-ml/jax/discussions/22936.\n\n    Two key factors motivated this decision:\n    * The Mac x86 build (only) has a number of test failures and crashes. We\n      would prefer to ship no release than a broken release.\n    * Mac x86 hardware is end-of-life and cannot be easily obtained for\n      developers at this point. So it is difficult for us to fix this kind of\n      problem even if we wanted to.\n\n    We are open to re-adding support for Mac x86 if the community is willing\n    to help support that platform: in particular, we would need the JAX test\n    suite to pass cleanly on Mac x86 before we could ship releases again.\n\n* Changes:\n  * The minimum NumPy version is now 1.25. NumPy 1.25 will remain the minimum\n    supported version until June 2025.\n  * The minimum SciPy version is now 1.11. SciPy 1.11 will remain the minimum\n    supported version until June 2025.\n  * {func}`jax.numpy.einsum` now defaults to `optimize='auto'` rather than\n    `optimize='optimal'`. This avoids exponentially-scaling trace-time in\n    the case of many arguments ({jax-issue}`#25214`).\n  * {func}`jax.numpy.linalg.solve` no longer supports batched 1D arguments\n    on the right hand side. To recover the previous behavior in these cases,\n    use `solve(a, b[..., None]).squeeze(-1)`.\n\n* New Features\n  * {func}`jax.numpy.fft.fftn`, {func}`jax.numpy.fft.rfftn`,\n    {func}`jax.numpy.fft.ifftn`, and {func}`jax.numpy.fft.irfftn` now support\n    transforms in more than 3 dimensions, which was previously the limit. See\n    {jax-issue}`#25606` for more details.\n  * Support added for user defined state in the FFI via the new\n    {func}`jax.ffi.register_ffi_type_id` function.\n  * The AOT lowering `.as_text()` method now supports the `debug_info` option\n    to include debugging information, e.g., source location, in the output.\n\n* Deprecations\n  * From {mod}`jax.interpreters.xla`, `abstractify` and `pytype_aval_mappings`\n    are now deprecated, having been replaced by symbols of the same name\n    in {mod}`jax.core`.\n  * {func}`jax.scipy.special.lpmn` and {func}`jax.scipy.special.lpmn_values`\n    are deprecated, following their deprecation in SciPy v1.15.0. There are\n    no plans to replace these deprecated functions with new APIs.\n  * The {mod}`jax.extend.ffi` submodule was moved to {mod}`jax.ffi`, and the\n    previous import path is deprecated.\n\n* Deletions\n  * `jax_enable_memories` flag has been deleted and the behavior of that flag\n    is on by default.\n  * From `jax.lib.xla_client`, the previously-deprecated `Device` and\n    `XlaRuntimeError` symbols have been removed; instead use `jax.Device`\n    and `jax.errors.JaxRuntimeError` respectively.\n  * The `jax.experimental.array_api` module has been removed after being\n    deprecated in JAX v0.4.32. Since that release, {mod}`jax.numpy` supports\n    the array API directly.\n\n## jax 0.4.38 (Dec 17, 2024)\n\n* Breaking Changes\n  * `XlaExecutable.cost_analysis` now returns a `dict[str, float]` (instead of a\n    single-element `list[dict[str, float]]`).\n\n* Changes:\n  * `jax.tree.flatten_with_path` and `jax.tree.map_with_path` are added\n    as shortcuts of the corresponding `tree_util` functions.\n\n* Deprecations\n  * a number of APIs in the internal `jax.core` namespace have been deprecated.\n    Most were no-ops, were little-used, or can be replaced by APIs of the same\n    name in {mod}`jax.extend.core`; see the documentation for {mod}`jax.extend`\n    for information on the compatibility guarantees of these semi-public extensions.\n  * Several previously-deprecated APIs have been removed, including:\n    * from {mod}`jax.core`: `check_eqn`, `check_type`,  `check_valid_jaxtype`, and\n      `non_negative_dim`.\n    * from {mod}`jax.lib.xla_bridge`: `xla_client` and `default_backend`.\n    * from {mod}`jax.lib.xla_client`: `_xla` and `bfloat16`.\n    * from {mod}`jax.numpy`: `round_`.\n\n* New Features\n  * {func}`jax.export.export` can be used for device-polymorphic export with\n    shardings constructed with {func}`jax.sharding.AbstractMesh`.\n    See the [jax.export documentation](https://docs.jax.dev/en/latest/export/export.html#device-polymorphic-export).\n  * Added {func}`jax.lax.split`. This is a primitive version of\n    {func}`jax.numpy.split`, added because it yields a more compact\n    transpose during automatic differentiation.\n\n## jax 0.4.37 (Dec 9, 2024)\n\nThis is a patch release of jax 0.4.36. Only \"jax\" was released at this version.\n\n* Bug fixes\n  * Fixed a bug where `jit` would error if an argument was named `f` (#25329).\n  * Fix a bug that will throw `index out of range` error in\n    {func}`jax.lax.while_loop` if the user register pytree node class with\n    different aux data for the flatten and flatten_with_path.\n  * Pinned a new libtpu release (0.0.6) that fixes a compiler bug on TPU v6e.\n\n## jax 0.4.36 (Dec 5, 2024)\n\n* Breaking Changes\n  * This release lands \"stackless\", an internal change to JAX's tracing\n    machinery. We made trace dispatch purely a function of context rather than a\n    function of both context and data. This let us delete a lot of machinery for\n    managing data-dependent tracing: levels, sublevels, `post_process_call`,\n    `new_base_main`, `custom_bind`, and so on. The change should only affect\n    users that use JAX internals.\n\n    If you do use JAX internals then you may need to\n    update your code (see\n    https://github.com/jax-ml/jax/commit/c36e1f7c1ad4782060cbc8e8c596d85dfb83986f\n    for clues about how to do this). There might also be version skew\n    issues with JAX libraries that do this. If you find this change breaks your\n    non-JAX-internals-using code then try the\n    `config.jax_data_dependent_tracing_fallback` flag as a workaround, and if\n    you need help updating your code then please file a bug.\n  * {func}`jax.experimental.jax2tf.convert` with `native_serialization=False`\n    or with `enable_xla=False` have been deprecated since July 2024, with\n    JAX version 0.4.31. Now we removed support for these use cases. `jax2tf`\n    with native serialization will still be supported.\n  * In `jax.interpreters.xla`, the `xb`, `xc`, and `xe` symbols have been removed\n    after being deprecated in JAX v0.4.31. Instead use `xb = jax.lib.xla_bridge`,\n    `xc = jax.lib.xla_client`, and `xe = jax.lib.xla_extension`.\n  * The deprecated module `jax.experimental.export` has been removed. It was replaced\n    by {mod}`jax.export` in JAX v0.4.30. See the [migration guide](https://docs.jax.dev/en/latest/export/export.html#migration-guide-from-jax-experimental-export)\n    for information on migrating to the new API.\n  * The `initial` argument to {func}`jax.nn.softmax` and {func}`jax.nn.log_softmax`\n    has been removed, after being deprecated in v0.4.27.\n  * Calling `np.asarray` on typed PRNG keys (i.e. keys produced by {func}`jax.random.key`)\n    now raises an error. Previously, this returned a scalar object array.\n  * The following deprecated methods and functions in {mod}`jax.export` have\n    been removed:\n      * `jax.export.DisabledSafetyCheck.shape_assertions`: it had no effect\n        already.\n      * `jax.export.Exported.lowering_platforms`: use `platforms`.\n      * `jax.export.Exported.mlir_module_serialization_version`:\n        use `calling_convention_version`.\n      * `jax.export.Exported.uses_shape_polymorphism`:\n         use `uses_global_constants`.\n      * the `lowering_platforms` kwarg for {func}`jax.export.export`: use\n        `platforms` instead.\n  * The kwargs `symbolic_scope` and `symbolic_constraints` from\n    {func}`jax.export.symbolic_args_specs` have been removed. They were\n    deprecated in June 2024. Use `scope` and `constraints` instead.\n  * Hashing of tracers, which has been deprecated since version 0.4.30, now\n    results in a `TypeError`.\n  * Refactor: JAX build CLI (build/build.py) now uses a subcommand structure and\n    replaces previous build.py usage. Run `python build/build.py --help` for\n    more details. Brief overview of the new subcommand options:\n    * `build`: Builds JAX wheel packages. For e.g., `python build/build.py build --wheels=jaxlib,jax-cuda-pjrt`\n    * `requirements_update`: Updates requirements_lock.txt files.\n  * {func}`jax.scipy.linalg.toeplitz` now does implicit batching on multi-dimensional\n    inputs. To recover the previous behavior, you can call {func}`jax.numpy.ravel`\n    on the function inputs.\n  * {func}`jax.scipy.special.gamma` and {func}`jax.scipy.special.gammasgn` now\n    return NaN for negative integer inputs, to match the behavior of SciPy from\n    https://github.com/scipy/scipy/pull/21827.\n  * `jax.clear_backends` was removed after being deprecated in v0.4.26.\n  * We removed the custom call \"__gpu$xla.gpu.triton\" from the list of custom\n    call that we guarantee export stability. This is because this custom call\n    relies on Triton IR, which is not guaranteed to be stable. If you need\n    to export code that uses this custom call, you can use the `disabled_checks`\n    parameter. See more details in the [documentation](https://docs.jax.dev/en/latest/export/export.html#compatibility-guarantees-for-custom-calls).\n\n* New Features\n  * {func}`jax.jit` got a new `compiler_options: dict[str, Any]` argument, for\n    passing compilation options to XLA. For the moment it's undocumented and\n    may be in flux.\n  * {func}`jax.tree_util.register_dataclass` now allows metadata fields to be\n    declared inline via {func}`dataclasses.field`. See the function documentation\n    for examples.\n  * Added {func}`jax.numpy.put_along_axis`.\n  * {func}`jax.lax.linalg.eig` and the related `jax.numpy` functions\n    ({func}`jax.numpy.linalg.eig` and {func}`jax.numpy.linalg.eigvals`) are now\n    supported on GPU. See {jax-issue}`#24663` for more details.\n  * Added two new configuration flags, `jax_exec_time_optimization_effort` and `jax_memory_fitting_effort`, to control the amount of effort the compiler spends minimizing execution time and memory usage, respectively.  Valid values are between -1.0 and 1.0, default is 0.0.\n\n* Bug fixes\n  * Fixed a bug where the GPU implementations of LU and QR decomposition would\n    result in an indexing overflow for batch sizes close to int32 max. See\n    {jax-issue}`#24843` for more details.\n\n* Deprecations\n  * `jax.lib.xla_extension.ArrayImpl` and `jax.lib.xla_client.ArrayImpl` are deprecated;\n    use `jax.Array` instead.\n  * `jax.lib.xla_extension.XlaRuntimeError` is deprecated; use `jax.errors.JaxRuntimeError`\n    instead.\n\n## jax 0.4.35 (Oct 22, 2024)\n\n* Breaking Changes\n  * {func}`jax.numpy.isscalar` now returns True for any array-like object with\n    zero dimensions. Previously it only returned True for zero-dimensional\n    array-like objects with a weak dtype.\n  * `jax.experimental.host_callback` has been deprecated since March 2024, with\n    JAX version 0.4.26. Now we removed it.\n    See {jax-issue}`#20385` for a discussion of alternatives.\n\n* Changes:\n  * `jax.lax.FftType` was introduced as a public name for the enum of FFT\n    operations. The semi-public API `jax.lib.xla_client.FftType` has been\n    deprecated.\n  * TPU: JAX now installs TPU support from the `libtpu` package rather than\n    `libtpu-nightly`. For the next few releases JAX will pin an empty version of\n    `libtpu-nightly` as well as `libtpu` to ease the transition; that dependency\n    will be removed in Q1 2025.\n\n* Deprecations:\n  * The semi-public API `jax.lib.xla_client.PaddingType` has been deprecated.\n    No JAX APIs consume this type, so there is no replacement.\n  * The default behavior of {func}`jax.pure_callback` and\n    {func}`jax.extend.ffi.ffi_call` under `vmap` has been deprecated and so has\n    the `vectorized` parameter to those functions. The `vmap_method` parameter\n    should be used instead for better defined behavior. See the discussion in\n    {jax-issue}`#23881` for more details.\n  * The semi-public API `jax.lib.xla_client.register_custom_call_target` has\n    been deprecated. Use the JAX FFI instead.\n  * The semi-public APIs `jax.lib.xla_client.dtype_to_etype`,\n    `jax.lib.xla_client.ops`,\n    `jax.lib.xla_client.shape_from_pyval`, `jax.lib.xla_client.PrimitiveType`,\n    `jax.lib.xla_client.Shape`, `jax.lib.xla_client.XlaBuilder`, and\n    `jax.lib.xla_client.XlaComputation` have been deprecated. Use StableHLO\n    instead.\n\n## jax 0.4.34 (October 4, 2024)\n\n* New Functionality\n  * This release includes wheels for Python 3.13. Free-threading mode is not yet\n    supported.\n  * `jax.errors.JaxRuntimeError` has been added as a public alias for the\n    formerly private `XlaRuntimeError` type.\n\n* Breaking changes\n  * `jax_pmap_no_rank_reduction` flag is set to `True` by default.\n    * array[0] on a pmap result now introduces a reshape (use array[0:1]\n      instead).\n    * The per-shard shape (accessible via jax_array.addressable_shards or\n      jax_array.addressable_data(0)) now has a leading (1, ...). Update code\n      that directly accesses shards accordingly. The rank of the per-shard-shape\n      now matches that of the global shape which is the same behavior as jit.\n      This avoids costly reshapes when passing results from pmap into jit.\n  * `jax.experimental.host_callback` has been deprecated since March 2024, with\n    JAX version 0.4.26. Now we set the default value of the\n    `--jax_host_callback_legacy` configuration value to `True`, which means that\n    if your code uses `jax.experimental.host_callback` APIs, those API calls\n    will be implemented in terms of the new `jax.experimental.io_callback` API.\n    If this breaks your code, for a very limited time, you can set the\n    `--jax_host_callback_legacy` to `True`. Soon we will remove that\n    configuration option, so you should instead transition to using the\n    new JAX callback APIs. See {jax-issue}`#20385` for a discussion.\n\n* Deprecations\n  * In {func}`jax.numpy.trim_zeros`, non-arraylike arguments or arraylike\n    arguments with `ndim != 1` are now deprecated, and in the future will result\n    in an error.\n  * Internal pretty-printing tools `jax.core.pp_*` have been removed, after\n    being deprecated in JAX v0.4.30.\n  * `jax.lib.xla_client.Device` is deprecated; use `jax.Device` instead.\n  * `jax.lib.xla_client.XlaRuntimeError` has been deprecated. Use\n    `jax.errors.JaxRuntimeError` instead.\n\n* Deletion:\n  * `jax.xla_computation` is deleted. It's been 3 months since it's deprecation\n    in 0.4.30 JAX release.\n    Please use the AOT APIs to get the same functionality as `jax.xla_computation`.\n    * `jax.xla_computation(fn)(*args, **kwargs)` can be replaced with\n      `jax.jit(fn).lower(*args, **kwargs).compiler_ir('hlo')`.\n    * You can also use `.out_info` property of `jax.stages.Lowered` to get the\n      output information (like tree structure, shape and dtype).\n    * For cross-backend lowering, you can replace\n      `jax.xla_computation(fn, backend='tpu')(*args, **kwargs)` with\n      `jax.jit(fn).trace(*args, **kwargs).lower(lowering_platforms=('tpu',)).compiler_ir('hlo')`.\n  * {class}`jax.ShapeDtypeStruct` no longer accepts the `named_shape` argument.\n    The argument was only used by `xmap` which was removed in 0.4.31.\n  * `jax.tree.map(f, None, non-None)`, which previously emitted a\n    `DeprecationWarning`, now raises an error in a future version of jax. `None`\n    is only a tree-prefix of itself. To preserve the current behavior, you can\n    ask `jax.tree.map` to treat `None` as a leaf value by writing:\n    `jax.tree.map(lambda x, y: None if x is None else f(x, y), a, b, is_leaf=lambda x: x is None)`.\n  * `jax.sharding.XLACompatibleSharding` has been removed. Please use\n    `jax.sharding.Sharding`.\n\n* Bug fixes\n  * Fixed a bug where {func}`jax.numpy.cumsum` would produce incorrect outputs\n    if a non-boolean input was provided and `dtype=bool` was specified.\n  * Edit implementation of {func}`jax.numpy.ldexp` to get correct gradient.\n\n## jax 0.4.33 (September 16, 2024)\n\nThis is a patch release on top of jax 0.4.32, that fixes two bugs found in that\nrelease.\n\nA TPU-only data corruption bug was found in the version of libtpu pinned by\nJAX 0.4.32, which manifested only if multiple TPU slices were present in the\nsame job, for example, if training on multiple v5e slices.\nThis release fixes that issue by pinning a fixed version of `libtpu`.\n\nThis release fixes an inaccurate result for F64 tanh on CPU (#23590).\n\n## jax 0.4.32 (September 11, 2024)\n\nNote: This release was yanked from PyPi because of a data corruption bug on TPU.\nSee the 0.4.33 release notes for more details.\n\n* New Functionality\n  * Added {func}`jax.extend.ffi.ffi_call` and {func}`jax.extend.ffi.ffi_lowering`\n    to support the use of the new {ref}`ffi-tutorial` to interface with custom\n    C++ and CUDA code from JAX.\n\n* Changes\n  * `jax_enable_memories` flag is set to `True` by default.\n  * {mod}`jax.numpy` now supports v2023.12 of the Python Array API Standard.\n    See {ref}`python-array-api` for more information.\n  * Computations on the CPU backend may now be dispatched asynchronously in\n    more cases. Previously non-parallel computations were always dispatched\n    synchronously. You can recover the old behavior by setting\n    `jax.config.update('jax_cpu_enable_async_dispatch', False)`.\n  * Added new {func}`jax.process_indices` function to replace the\n    `jax.host_ids()` function that was deprecated in JAX v0.2.13.\n  * To align with the behavior of `numpy.fabs`, `jax.numpy.fabs` has been\n    modified to no longer support `complex dtypes`.\n  * ``jax.tree_util.register_dataclass`` now checks that ``data_fields``\n    and ``meta_fields`` includes all dataclass fields with ``init=True``\n    and only them, if ``nodetype`` is a dataclass.\n  * Several {mod}`jax.numpy` functions now have full {class}`~jax.numpy.ufunc`\n    interfaces, including {obj}`~jax.numpy.add`, {obj}`~jax.numpy.multiply`,\n    {obj}`~jax.numpy.bitwise_and`, {obj}`~jax.numpy.bitwise_or`,\n    {obj}`~jax.numpy.bitwise_xor`, {obj}`~jax.numpy.logical_and`,\n    {obj}`~jax.numpy.logical_and`, and {obj}`~jax.numpy.logical_and`.\n    In future releases we plan to expand these to other ufuncs.\n  * Added {func}`jax.lax.optimization_barrier`, which allows users to prevent\n    compiler optimizations such as common-subexpression elimination and to\n    control scheduling.\n\n* Breaking changes\n  * The MHLO MLIR dialect (`jax.extend.mlir.mhlo`) has been removed. Use the\n    `stablehlo` dialect instead.\n\n* Deprecations\n  * Complex inputs to {func}`jax.numpy.clip` and {func}`jax.numpy.hypot` are\n    no longer allowed, after being deprecated since JAX v0.4.27.\n  * Deprecated the following APIs:\n    * `jax.lib.xla_bridge.xla_client`: use {mod}`jax.lib.xla_client` directly.\n    * `jax.lib.xla_bridge.get_backend`: use {func}`jax.extend.backend.get_backend`.\n    * `jax.lib.xla_bridge.default_backend`: use {func}`jax.extend.backend.default_backend`.\n  * The `jax.experimental.array_api` module is deprecated, and importing it is no\n    longer required to use the Array API. `jax.numpy` supports the array API\n    directly; see {ref}`python-array-api` for more information.\n  * The internal utilities `jax.core.check_eqn`, `jax.core.check_type`, and\n    `jax.core.check_valid_jaxtype` are now deprecated, and will be removed in\n    the future.\n  * `jax.numpy.round_` has been deprecated, following removal of the corresponding\n    API in NumPy 2.0. Use {func}`jax.numpy.round` instead.\n  * Passing a DLPack capsule to {func}`jax.dlpack.from_dlpack` is deprecated.\n    The argument to {func}`jax.dlpack.from_dlpack` should be an array from\n    another framework that implements the ``__dlpack__`` protocol.\n\n## jaxlib 0.4.32 (September 11, 2024)\n\nNote: This release was yanked from PyPi because of a data corruption bug on TPU.\nSee the 0.4.33 release notes for more details.\n\n* Breaking changes\n  * This release of jaxlib switched to a new version of the CPU backend, which\n    should compile faster and leverage parallelism better. If you experience\n    any problems due to this change, you can temporarily enable the old CPU\n    backend by setting the environment variable\n    `XLA_FLAGS=--xla_cpu_use_thunk_runtime=false`. If you need to do this,\n    please file a JAX bug with instructions to reproduce.\n  * Hermetic CUDA support is added.\n    Hermetic CUDA uses a specific downloadable version of CUDA instead of the\n    user\u2019s locally installed CUDA. Bazel will download CUDA, CUDNN and NCCL\n    distributions, and then use CUDA libraries and tools as dependencies in\n    various Bazel targets. This enables more reproducible builds for JAX and its\n    supported CUDA versions.\n\n* Changes\n  * SparseCore profiling is added.\n    * JAX now supports profiling [SparseCore](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#sparsecore) on TPUv5p chips. These traces will be viewable in Tensorboard Profiler's [TraceViewer](https://www.tensorflow.org/guide/profiler#trace_viewer).\n\n## jax 0.4.31 (July 29, 2024)\n\n* Deletion\n  * xmap has been deleted. Please use {func}`shard_map` as the replacement.\n\n* Changes\n  * The minimum CuDNN version is v9.1. This was true in previous releases also,\n    but we now declare this version constraint formally.\n  * The minimum Python version is now 3.10. 3.10 will remain the minimum\n    supported version until July 2025.\n  * The minimum NumPy version is now 1.24. NumPy 1.24 will remain the minimum\n    supported version until December 2024.\n  * The minimum SciPy version is now 1.10. SciPy 1.10 will remain the minimum\n    supported version until January 2025.\n  * {func}`jax.numpy.ceil`, {func}`jax.numpy.floor` and {func}`jax.numpy.trunc` now return the output\n    of the same dtype as the input, i.e. no longer upcast integer or boolean inputs to floating point.\n  * `libdevice.10.bc` is no longer bundled with CUDA wheels. It must be\n    installed either as a part of local CUDA installation, or via NVIDIA's CUDA\n    pip wheels.\n  * {class}`jax.experimental.pallas.BlockSpec` now expects `block_shape` to\n    be passed *before* `index_map`. The old argument order is deprecated and\n    will be removed in a future release.\n  * Updated the repr of gpu devices to be more consistent\n    with TPUs/CPUs. For example, `cuda(id=0)` will now be `CudaDevice(id=0)`.\n  * Added the `device` property and `to_device` method to {class}`jax.Array`, as\n    part of JAX's [Array API](https://data-apis.org/array-api) support.\n* Deprecations\n  * Removed a number of previously-deprecated internal APIs related to\n    polymorphic shapes. From {mod}`jax.core`: removed `canonicalize_shape`,\n    `dimension_as_value`, `definitely_equal`, and `symbolic_equal_dim`.\n  * HLO lowering rules should no longer wrap singleton ir.Values in tuples.\n    Instead, return singleton ir.Values unwrapped. Support for wrapped values\n    will be removed in a future version of JAX.\n  * {func}`jax.experimental.jax2tf.convert` with `native_serialization=False`\n    or `enable_xla=False` is now deprecated and this support will be removed in\n    a future version.\n    Native serialization has been the default since JAX 0.4.16 (September 2023).\n  * The previously-deprecated function `jax.random.shuffle` has been removed;\n    instead use `jax.random.permutation` with `independent=True`.\n\n## jaxlib 0.4.31 (July 29, 2024)\n\n* Bug fixes\n  * Fixed a bug that meant that negative static_argnums to a jit were mishandled\n    by the jit dispatch fast path.\n  * Fixed a bug that meant triangular solves of batches of singular matrices\n    produce nonsensical finite values, instead of inf or nan (#3589, #15429).\n\n## jax 0.4.30 (June 18, 2024)\n\n* Changes\n  * JAX supports ml_dtypes >= 0.2. In 0.4.29 release, the ml_dtypes version was\n    bumped to 0.4.0 but this has been rolled back in this release to give users\n    of both TensorFlow and JAX more time to migrate to a newer TensorFlow\n    release.\n  * `jax.experimental.mesh_utils` can now create an efficient mesh for TPU v5e.\n  * jax now depends on jaxlib directly. This change was enabled by the CUDA\n    plugin switch: there are no longer multiple jaxlib variants. You can install\n    a CPU-only jax with `pip install jax`, no extras required.\n  * Added an API for exporting and serializing JAX functions. This used\n    to exist in `jax.experimental.export` (which is being deprecated),\n    and will now live in `jax.export`.\n    See the [documentation](https://docs.jax.dev/en/latest/export/index.html).\n\n* Deprecations\n  * Internal pretty-printing tools `jax.core.pp_*` are deprecated, and will be removed\n    in a future release.\n  * Hashing of tracers is deprecated, and will lead to a `TypeError` in a future JAX\n    release. This previously was the case, but there was an inadvertent regression in\n    the last several JAX releases.\n  * `jax.experimental.export` is deprecated. Use {mod}`jax.export` instead.\n    See the [migration guide](https://docs.jax.dev/en/latest/export/export.html#migration-guide-from-jax-experimental-export).\n  * Passing an array in place of a dtype is now deprecated in most cases; e.g. for arrays\n    `x` and `y`, `x.astype(y)` will raise a warning. To silence it use `x.astype(y.dtype)`.\n  * `jax.xla_computation` is deprecated and will be removed in a future release.\n    Please use the AOT APIs to get the same functionality as `jax.xla_computation`.\n    * `jax.xla_computation(fn)(*args, **kwargs)` can be replaced with\n      `jax.jit(fn).lower(*args, **kwargs).compiler_ir('hlo')`.\n    * You can also use `.out_info` property of `jax.stages.Lowered` to get the\n      output information (like tree structure, shape and dtype).\n    * For cross-backend lowering, you can replace\n      `jax.xla_computation(fn, backend='tpu')(*args, **kwargs)` with\n      `jax.jit(fn).trace(*args, **kwargs).lower(lowering_platforms=('tpu',)).compiler_ir('hlo')`.\n\n\n## jaxlib 0.4.30 (June 18, 2024)\n\n  * Support for monolithic CUDA jaxlibs has been dropped. You must use the\n    plugin-based installation (`pip install jax[cuda12]` or\n    `pip install jax[cuda12_local]`).\n\n## jax 0.4.29 (June 10, 2024)\n\n* Changes\n  * We anticipate that this will be the last release of JAX and jaxlib\n    supporting a monolithic CUDA jaxlib. Future releases will use the CUDA\n    plugin jaxlib (e.g. `pip install jax[cuda12]`).\n  * JAX now requires ml_dtypes version 0.4.0 or newer.\n  * Removed backwards-compatibility support for old usage of the\n    `jax.experimental.export` API. It is not possible anymore to use\n    `from jax.experimental.export import export`, and instead you should use\n    `from jax.experimental import export`.\n    The removed functionality has been deprecated since 0.4.24.\n  * Added `is_leaf` argument to {func}`jax.tree.all` & {func}`jax.tree_util.tree_all`.\n\n* Deprecations\n  * `jax.sharding.XLACompatibleSharding` is deprecated. Please use\n    `jax.sharding.Sharding`.\n  * `jax.experimental.Exported.in_shardings` has been renamed as\n    `jax.experimental.Exported.in_shardings_hlo`. Same for `out_shardings`.\n    The old names will be removed after 3 months.\n  * Removed a number of previously-deprecated APIs:\n    * from {mod}`jax.core`: `non_negative_dim`, `DimSize`, `Shape`\n    * from {mod}`jax.lax`: `tie_in`\n    * from {mod}`jax.nn`: `normalize`\n    * from {mod}`jax.interpreters.xla`: `backend_specific_translations`,\n      `translations`, `register_translation`, `xla_destructure`,\n      `TranslationRule`, `TranslationContext`, `XlaOp`.\n  * The ``tol`` argument of {func}`jax.numpy.linalg.matrix_rank` is being\n    deprecated and will soon be removed. Use `rtol` instead.\n  * The ``rcond`` argument of {func}`jax.numpy.linalg.pinv` is being\n    deprecated and will soon be removed. Use `rtol` instead.\n  * The deprecated `jax.config` submodule has been removed. To configure JAX\n    use `import jax` and then reference the config object via `jax.config`.\n  * {mod}`jax.random` APIs no longer accept batched keys, where previously\n    some did unintentionally. Going forward, we recommend explicit use of\n    {func}`jax.vmap` in such cases.\n  * In {func}`jax.scipy.special.beta`, the `x` and `y` parameters have been\n    renamed to `a` and `b` for consistency with other `beta` APIs.\n\n* New Functionality\n  * Added {func}`jax.experimental.Exported.in_shardings_jax` to construct\n    shardings that can be used with the JAX APIs from the HloShardings\n    that are stored in the `Exported` objects.\n\n## jaxlib 0.4.29 (June 10, 2024)\n\n* Bug fixes\n  * Fixed a bug where XLA sharded some concatenation operations incorrectly,\n    which manifested as an incorrect output for cumulative reductions (#21403).\n  * Fixed a bug where XLA:CPU miscompiled certain matmul fusions\n    (https://github.com/openxla/xla/pull/13301).\n  * Fixes a compiler crash on GPU (https://github.com/jax-ml/jax/issues/21396).\n\n* Deprecations\n  * `jax.tree.map(f, None, non-None)` now emits a `DeprecationWarning`, and will\n    raise an error in a future version of jax. `None` is only a tree-prefix of\n    itself. To preserve the current behavior, you can ask `jax.tree.map` to\n    treat `None` as a leaf value by writing:\n    `jax.tree.map(lambda x, y: None if x is None else f(x, y), a, b, is_leaf=lambda x: x is None)`.\n\n## jax 0.4.28 (May 9, 2024)\n\n* Bug fixes\n  * Reverted a change to `make_jaxpr` that was breaking Equinox (#21116).\n\n* Deprecations & removals\n  * The ``kind`` argument to {func}`jax.numpy.sort` and {func}`jax.numpy.argsort`\n    is now removed. Use `stable=True` or `stable=False` instead.\n  * Removed ``get_compute_capability`` from the ``jax.experimental.pallas.gpu``\n    module. Use the ``compute_capability`` attribute of a GPU device, returned\n    by {func}`jax.devices` or {func}`jax.local_devices`, instead.\n  * The ``newshape`` argument to {func}`jax.numpy.reshape`is being deprecated\n    and will soon be removed. Use `shape` instead.\n\n* Changes\n  * The minimum jaxlib version of this release is 0.4.27.\n\n## jaxlib 0.4.28 (May 9, 2024)\n\n* Bug fixes\n  * Fixes a memory corruption bug in the type name of Array and JIT Python\n    objects in Python 3.10 or earlier.\n  * Fixed a warning `'+ptx84' is not a recognized feature for this target`\n    under CUDA 12.4.\n  * Fixed a slow compilation problem on CPU.\n\n* Changes\n  * The Windows build is now built with Clang instead of MSVC.\n\n\n## jax 0.4.27 (May 7, 2024)\n\n* New Functionality\n  * Added {func}`jax.numpy.unstack` and {func}`jax.numpy.cumulative_sum`,\n    following their addition in the array API 2023 standard, soon to be\n    adopted by NumPy.\n  * Added a new config option `jax_cpu_collectives_implementation` to select the\n    implementation of cross-process collective operations used by the CPU backend.\n    Choices available are `'none'`(default), `'gloo'` and `'mpi'` (requires jaxlib 0.4.26).\n    If set to `'none'`, cross-process collective operations are disabled.\n\n* Changes\n  * {func}`jax.pure_callback`, {func}`jax.experimental.io_callback`\n    and {func}`jax.debug.callback` now use {class}`jax.Array` instead\n    of {class}`np.ndarray`. You can recover the old behavior by transforming\n    the arguments via `jax.tree.map(np.asarray, args)` before passing them\n    to the callback.\n  * `complex_arr.astype(bool)` now follows the same semantics as NumPy, returning\n    False where `complex_arr` is equal to `0 + 0j`, and True otherwise.\n  * `core.Token` now is a non-trivial class which wraps a `jax.Array`. It could\n    be created and threaded in and out of computations to build up dependency.\n    The singleton object `core.token` has been removed, users now should create\n    and use fresh `core.Token` objects instead.\n  * On GPU, the Threefry PRNG implementation no longer lowers to a kernel call\n    by default. This choice can improve runtime memory usage at a compile-time\n    cost. Prior behavior, which produces a kernel call, can be recovered with\n    `jax.config.update('jax_threefry_gpu_kernel_lowering', True)`. If the new\n    default causes issues, please file a bug. Otherwise, we intend to remove\n    this flag in a future release.\n\n* Deprecations & Removals\n  * Pallas now exclusively uses XLA for compiling kernels on GPU. The old\n    lowering pass via Triton Python APIs has been removed and the\n    `JAX_TRITON_COMPILE_VIA_XLA` environment variable no longer has any effect.\n  * {func}`jax.numpy.clip` has a new argument signature: `a`, `a_min`, and\n    `a_max` are deprecated in favor of `x` (positional only), `min`, and\n    `max` ({jax-issue}`20550`).\n  * The `device()` method of JAX arrays has been removed, after being deprecated\n    since JAX v0.4.21. Use `arr.devices()` instead.\n  * The `initial` argument to {func}`jax.nn.softmax` and {func}`jax.nn.log_softmax`\n    is deprecated; empty inputs to softmax are now supported without setting this.\n  * In {func}`jax.jit`, passing invalid `static_argnums` or `static_argnames`\n    now leads to an error rather than a warning.\n  * The minimum jaxlib version is now 0.4.23.\n  * The {func}`jax.numpy.hypot` function now issues a deprecation warning when\n    passing complex-valued inputs to it. This will raise an error when the\n    deprecation is completed.\n  * Scalar arguments to {func}`jax.numpy.nonzero`, {func}`jax.numpy.where`, and\n    related functions now raise an error, following a similar change in NumPy.\n  * The config option `jax_cpu_enable_gloo_collectives` is deprecated.\n    Use `jax.config.update('jax_cpu_collectives_implementation', 'gloo')` instead.\n  * The `jax.Array.device_buffer` and `jax.Array.device_buffers` methods have\n    been removed after being deprecated in JAX v0.4.22. Instead use\n    {attr}`jax.Array.addressable_shards` and {meth}`jax.Array.addressable_data`.\n  * The `condition`, `x`, and `y` parameters of `jax.numpy.where` are now\n    positional-only, following deprecation of the keywords in JAX v0.4.21.\n  * Non-array arguments to functions in {mod}`jax.lax.linalg` now must be\n    specified by keyword. Previously, this raised a DeprecationWarning.\n  * Array-like arguments are now required in several {func}`jax.numpy` APIs,\n    including {func}`~jax.numpy.apply_along_axis`,\n    {func}`~jax.numpy.apply_over_axes`, {func}`~jax.numpy.inner`,\n    {func}`~jax.numpy.outer`, {func}`~jax.numpy.cross`,\n    {func}`~jax.numpy.kron`, and {func}`~jax.numpy.lexsort`.\n\n* Bug fixes\n  * {func}`jax.numpy.astype` will now always return a copy when `copy=True`.\n    Previously, no copy would be made when the output array would have the same\n    dtype as the input array. This may result in some increased memory usage.\n    The default value is set to `copy=False` to preserve backwards compatibility.\n\n## jaxlib 0.4.27 (May 7, 2024)\n\n## jax 0.4.26 (April 3, 2024)\n\n* New Functionality\n  * Added {func}`jax.numpy.trapezoid`, following the addition of this function in\n    NumPy 2.0.\n\n* Changes\n  * Complex-valued {func}`jax.numpy.geomspace` now chooses the logarithmic spiral\n    branch consistent with that of NumPy 2.0.\n  * The behavior of `lax.rng_bit_generator`, and in turn the `'rbg'`\n    and `'unsafe_rbg'` PRNG implementations, under `jax.vmap` [has\n    changed](https://github.com/jax-ml/jax/issues/19085) so that\n    mapping over keys results in random generation only from the first\n    key in the batch.\n  * Docs now use `jax.random.key` for construction of PRNG key arrays\n    rather than `jax.random.PRNGKey`.\n\n* Deprecations & Removals\n  * {func}`jax.tree_map` is deprecated; use `jax.tree.map` instead, or for backward\n    compatibility with older JAX versions, use {func}`jax.tree_util.tree_map`.\n  * {func}`jax.clear_backends` is deprecated as it does not necessarily do what\n    its name suggests and can lead to unexpected consequences, e.g., it will not\n    destroy existing backends and release corresponding owned resources. Use\n    {func}`jax.clear_caches` if you only want to clean up compilation caches.\n    For backward compatibility or you really need to switch/reinitialize the\n    default backend, use {func}`jax.extend.backend.clear_backends`.\n  * The `jax.experimental.maps` module and `jax.experimental.maps.xmap` are\n    deprecated. Use `jax.experimental.shard_map` or `jax.vmap` with the\n    `spmd_axis_name` argument for expressing SPMD device-parallel computations.\n  * The `jax.experimental.host_callback` module is deprecated.\n    Use instead the [new JAX external callbacks](https://docs.jax.dev/en/latest/notebooks/external_callbacks.html).\n    Added `JAX_HOST_CALLBACK_LEGACY` flag to assist in the transition to the\n    new callbacks. See {jax-issue}`#20385` for a discussion.\n  * Passing arguments to {func}`jax.numpy.array_equal` and {func}`jax.numpy.array_equiv`\n    that cannot be converted to a JAX array now results in an exception.\n  * The deprecated flag `jax_parallel_functions_output_gda` has been removed.\n    This flag was long deprecated and did nothing; its use was a no-op.\n  * The previously-deprecated imports `jax.interpreters.ad.config` and\n    `jax.interpreters.ad.source_info_util` have now been removed. Use `jax.config`\n    and `jax.extend.source_info_util` instead.\n  * JAX export does not support older serialization versions anymore. Version 9\n    has been supported since October 27th, 2023 and has become the default\n    since February 1, 2024.\n    See [a description of the versions](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#native-serialization-versions).\n    This change could break clients that set a specific\n    JAX serialization version lower than 9.\n\n## jaxlib 0.4.26 (April 3, 2024)\n\n* Changes\n  * JAX now supports CUDA 12.1 or newer only. Support for CUDA 11.8 has been\n    dropped.\n  * JAX now supports NumPy 2.0.\n\n## jax 0.4.25 (Feb 26, 2024)\n\n* New Features\n  * Added [CUDA Array\n    Interface](https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html)\n    import support (requires jaxlib 0.4.24).\n  * JAX arrays now support NumPy-style scalar boolean indexing, e.g. `x[True]` or `x[False]`.\n  * Added {mod}`jax.tree` module, with a more convenient interface for referencing functions\n    in {mod}`jax.tree_util`.\n  * {func}`jax.tree.transpose` (i.e. {func}`jax.tree_util.tree_transpose`) now accepts\n    `inner_treedef=None`, in which case the inner treedef will be automatically inferred.\n\n* Changes\n  * Pallas now uses XLA instead of the Triton Python APIs to compile Triton\n    kernels. You can revert to the old behavior by setting the\n    `JAX_TRITON_COMPILE_VIA_XLA` environment variable to `\"0\"`.\n  * Several deprecated APIs in {mod}`jax.interpreters.xla` that were removed in v0.4.24\n    have been re-added in v0.4.25, including `backend_specific_translations`,\n    `translations`, `register_translation`, `xla_destructure`, `TranslationRule`,\n    `TranslationContext`, and `XLAOp`. These are still considered deprecated, and\n    will be removed again in the future when better replacements are available.\n    Refer to {jax-issue}`#19816` for discussion.\n\n* Deprecations & Removals\n  * {func}`jax.numpy.linalg.solve` now shows a deprecation warning for batched 1D\n    solves with `b.ndim > 1`. In the future these will be treated as batched 2D\n    solves.\n  * Conversion of a non-scalar array to a Python scalar now raises an error, regardless\n    of the size of the array. Previously a deprecation warning was raised in the case of\n    non-scalar arrays of size 1. This follows a similar deprecation in NumPy.\n  * The previously deprecated configuration APIs have been removed\n    following a standard 3 months deprecation cycle (see {ref}`api-compatibility`).\n    These include\n    * the `jax.config.config` object and\n    * the `define_*_state` and `DEFINE_*` methods of {data}`jax.config`.\n  * Importing the `jax.config` submodule via `import jax.config` is deprecated.\n    To configure JAX use `import jax` and then reference the config object\n    via `jax.config`.\n  * The minimum jaxlib version is now 0.4.20.\n\n## jaxlib 0.4.25 (Feb 26, 2024)\n\n## jax 0.4.24 (Feb 6, 2024)\n\n* Changes\n\n  * JAX lowering to StableHLO does not depend on physical devices anymore.\n    If your primitive wraps custom_partitioning or JAX callbacks in the lowering\n    rule i.e. function passed to `rule` parameter of `mlir.register_lowering` then add your\n    primitive to `jax._src.dispatch.prim_requires_devices_during_lowering` set.\n    This is needed because custom_partitioning and JAX callbacks need physical\n    devices to create `Sharding`s during lowering.\n    This is a temporary state until we can create `Sharding`s without physical\n    devices.\n  * {func}`jax.numpy.argsort` and {func}`jax.numpy.sort` now support the `stable`\n    and `descending` arguments.\n  * Several changes to the handling of shape polymorphism (used in\n    {mod}`jax.experimental.jax2tf` and {mod}`jax.experimental.export`):\n    * cleaner pretty-printing of symbolic expressions ({jax-issue}`#19227`)\n    * added the ability to specify symbolic constraints on the dimension variables.\n      This makes shape polymorphism more expressive, and gives a way to workaround\n      limitations in the reasoning about inequalities.\n      See https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    * with the addition of symbolic constraints ({jax-issue}`#19235`) we now\n      consider dimension variables from different scopes to be different, even\n      if they have the same name. Symbolic expressions from different scopes\n      cannot interact, e.g., in arithmetic operations.\n      Scopes are introduced by {func}`jax.experimental.jax2tf.convert`,\n      {func}`jax.experimental.export.symbolic_shape`, {func}`jax.experimental.export.symbolic_args_specs`.\n      The scope of a symbolic expression `e` can be read with `e.scope` and passed\n      into the above functions to direct them to construct symbolic expressions in\n      a given scope.\n      See https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    * simplified and faster equality comparisons, where we consider two symbolic dimensions\n      to be equal if the normalized form of their difference reduces to 0\n      ({jax-issue}`#19231`; note that this may result in user-visible behavior\n        changes)\n    * improved the error messages for inconclusive inequality comparisons\n      ({jax-issue}`#19235`).\n    * the `core.non_negative_dim` API (introduced recently)\n      was deprecated and `core.max_dim` and `core.min_dim` were introduced\n      ({jax-issue}`#18953`) to express `max` and `min` for symbolic dimensions.\n      You can use `core.max_dim(d, 0)` instead of `core.non_negative_dim(d)`.\n    * the `shape_poly.is_poly_dim` is deprecated in favor of `export.is_symbolic_dim`\n      ({jax-issue}`#19282`).\n    * the `export.args_specs` is deprecated in favor of `export.symbolic_args_specs\n      ({jax-issue}`#19283`).\n    * the `shape_poly.PolyShape` and `jax2tf.PolyShape` are deprecated, use\n      strings for polymorphic shapes specifications ({jax-issue}`#19284`).\n    * JAX default native serialization version is now 9. This is relevant\n      for {mod}`jax.experimental.jax2tf` and {mod}`jax.experimental.export`.\n      See [description of version numbers](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#native-serialization-versions).\n  * Refactored the API for `jax.experimental.export`. Instead of\n    `from jax.experimental.export import export` you should use now\n    `from jax.experimental import export`. The old way of importing will\n    continue to work for a deprecation period of 3 months.\n  * Added {func}`jax.scipy.stats.sem`.\n  * {func}`jax.numpy.unique` with `return_inverse = True` returns inverse indices\n    reshaped to the dimension of the input, following a similar change to\n    {func}`numpy.unique` in NumPy 2.0.\n  * {func}`jax.numpy.sign` now returns `x / abs(x)` for nonzero complex inputs. This is\n    consistent with the behavior of {func}`numpy.sign` in NumPy version 2.0.\n  * {func}`jax.scipy.special.logsumexp` with `return_sign=True` now uses the NumPy 2.0\n    convention for the complex sign, `x / abs(x)`. This is consistent with the behavior\n    of {func}`scipy.special.logsumexp` in SciPy v1.13.\n  * JAX now supports the bool DLPack type for both import and export.\n    Previously bool values could not be imported and were exported as integers.\n\n* Deprecations & Removals\n  * A number of previously deprecated functions have been removed, following a\n    standard 3+ month deprecation cycle (see {ref}`api-compatibility`).\n    This includes:\n    * From {mod}`jax.core`: `TracerArrayConversionError`,\n      `TracerIntegerConversionError`, `UnexpectedTracerError`,\n      `as_hashable_function`, `collections`, `dtypes`, `lu`, `map`,\n      `namedtuple`, `partial`, `pp`, `ref`, `safe_zip`, `safe_map`,\n      `source_info_util`, `total_ordering`, `traceback_util`, `tuple_delete`,\n      `tuple_insert`, and `zip`.\n    * From {mod}`jax.lax`: `dtypes`, `itertools`, `naryop`, `naryop_dtype_rule`,\n      `standard_abstract_eval`, `standard_naryop`, `standard_primitive`,\n      `standard_unop`, `unop`, and `unop_dtype_rule`.\n    * The `jax.linear_util` submodule and all its contents.\n    * The `jax.prng` submodule and all its contents.\n    * From {mod}`jax.random`: `PRNGKeyArray`, `KeyArray`, `default_prng_impl`,\n      `threefry_2x32`, `threefry2x32_key`, `threefry2x32_p`, `rbg_key`, and\n      `unsafe_rbg_key`.\n    * From {mod}`jax.tree_util`: `register_keypaths`, `AttributeKeyPathEntry`, and\n      `GetItemKeyPathEntry`.\n    * from {mod}`jax.interpreters.xla`: `backend_specific_translations`, `translations`,\n      `register_translation`, `xla_destructure`, `TranslationRule`, `TranslationContext`,\n      `axis_groups`, `ShapedArray`, `ConcreteArray`, `AxisEnv`, `backend_compile`,\n      and `XLAOp`.\n    * from {mod}`jax.numpy`: `NINF`, `NZERO`, `PZERO`, `row_stack`, `issubsctype`,\n      `trapz`, and `in1d`.\n    * from {mod}`jax.scipy.linalg`: `tril` and `triu`.\n  * The previously-deprecated method `PRNGKeyArray.unsafe_raw_array` has been\n    removed. Use {func}`jax.random.key_data` instead.\n  * `bool(empty_array)` now raises an error rather than returning `False`. This\n    previously raised a deprecation warning, and follows a similar change in NumPy.\n  * Support for the mhlo MLIR dialect has been deprecated. JAX no longer uses\n    the mhlo dialect, in favor of stablehlo. APIs that refer to \"mhlo\" will be\n    removed in the future. Use the \"stablehlo\" dialect instead.\n  * {mod}`jax.random`: passing batched keys directly to random number generation functions,\n    such as {func}`~jax.random.bits`, {func}`~jax.random.gamma`, and others, is deprecated\n    and will emit a `FutureWarning`.  Use `jax.vmap` for explicit batching.\n  * {func}`jax.lax.tie_in` is deprecated: it has been a no-op since JAX v0.2.0.\n\n## jaxlib 0.4.24 (Feb 6, 2024)\n\n* Changes\n\n  * JAX now supports CUDA 12.3 and CUDA 11.8. Support for CUDA 12.2 has been\n    dropped.\n  * `cost_analysis` now works with cross-compiled `Compiled` objects (i.e. when\n    using `.lower().compile()` with a topology object, e.g., to compile for\n    Cloud TPU from a non-TPU computer).\n  * Added [CUDA Array\n    Interface](https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html)\n    import support (requires jax 0.4.25).\n\n## jax 0.4.23 (Dec 13, 2023)\n\n## jaxlib 0.4.23 (Dec 13, 2023)\n\n* Fixed a bug that caused verbose logging from the GPU compiler during\n  compilation.\n\n## jax 0.4.22 (Dec 13, 2023)\n\n* Deprecations\n  * The `device_buffer` and `device_buffers` properties of JAX arrays are deprecated.\n    Explicit buffers have been replaced by the more flexible array sharding interface,\n    but the previous outputs can be recovered this way:\n    * `arr.device_buffer` becomes `arr.addressable_data(0)`\n    * `arr.device_buffers` becomes `[x.data for x in arr.addressable_shards]`\n\n## jaxlib 0.4.22 (Dec 13, 2023)\n\n## jax 0.4.21 (Dec 4 2023)\n\n* New Features\n  * Added {obj}`jax.nn.squareplus`.\n\n* Changes\n  * The minimum jaxlib version is now 0.4.19.\n  * Released wheels are built now with clang instead of gcc.\n  * Enforce that the device backend has not been initialized prior to calling `jax.distributed.initialize()`.\n  * Automate arguments to `jax.distributed.initialize()` in cloud TPU environments.\n\n* Deprecations\n  * The previously-deprecated `sym_pos` argument has been removed from\n    {func}`jax.scipy.linalg.solve`. Use `assume_a='pos'` instead.\n  * Passing `None` to {func}`jax.array` or {func}`jax.asarray`, either directly or\n    within a list or tuple, is deprecated and now raises a {obj}`FutureWarning`.\n    It currently is converted to NaN, and in the future will raise a {obj}`TypeError`.\n  * Passing the `condition`, `x`, and `y` parameters to `jax.numpy.where` by\n    keyword arguments has been deprecated, to match `numpy.where`.\n  * Passing arguments to {func}`jax.numpy.array_equal` and {func}`jax.numpy.array_equiv`\n    that cannot be converted to a JAX array is deprecated and now raises a\n    {obj}`DeprecationWaning`. Currently the functions return False, in the future this\n    will raise an exception.\n  * The `device()` method of JAX arrays is deprecated. Depending on the context, it may\n    be replaced with one of the following:\n    - {meth}`jax.Array.devices` returns the set of all devices used by the array.\n    - {attr}`jax.Array.sharding` gives the sharding configuration used by the array.\n\n## jaxlib 0.4.21 (Dec 4 2023)\n\n* Changes\n  * In preparation for adding distributed CPU support, JAX now treats CPU\n    devices identically to GPU and TPU devices, that is:\n\n    * `jax.devices()` includes all devices present in a distributed job, even\n      those not local to the current process. `jax.local_devices()` still only\n      includes devices local to the current process, so if the change to\n      `jax.devices()` breaks you, you most likely want to use\n      `jax.local_devices()` instead.\n    * CPU devices now receive a globally unique ID number within a distributed\n      job; previously CPU devices would receive a process-local ID number.\n    * The `process_index` of each CPU device will now match any GPU or TPU\n      devices within the same process; previously the `process_index` of a CPU\n      device was always 0.\n\n  * On NVIDIA GPU, JAX now prefers a Jacobi SVD solver for matrices up to\n    1024x1024. The Jacobi solver appears faster than the non-Jacobi version.\n\n* Bug fixes\n  * Fixed error/hang when an array with non-finite values is passed to a\n    non-symmetric eigendecomposition (#18226). Arrays with non-finite values now\n    produce arrays full of NaNs as outputs.\n\n## jax 0.4.20 (Nov 2, 2023)\n\n## jaxlib 0.4.20 (Nov 2, 2023)\n\n* Bug fixes\n  * Fixed some type confusion between E4M3 and E5M2 float8 types.\n\n## jax 0.4.19 (Oct 19, 2023)\n\n* New Features\n  * Added {obj}`jax.typing.DTypeLike`, which can be used to annotate objects that\n    are convertible to JAX dtypes.\n  * Added `jax.numpy.fill_diagonal`.\n\n* Changes\n  * JAX now requires SciPy 1.9 or newer.\n\n* Bug fixes\n  * Only process 0 in a multicontroller distributed JAX program will write\n    persistent compilation cache entries. This fixes write contention if the\n    cache is placed on a network file system such as GCS.\n  * The version check for cusolver and cufft no longer considers the patch\n    versions when determining if the installed version of these libraries is at\n    least as new as the versions against which JAX was built.\n\n## jaxlib 0.4.19 (Oct 19, 2023)\n\n* Changes\n  * jaxlib will now always prefer pip-installed NVIDIA CUDA libraries\n    (nvidia-... packages) over any other CUDA installation if they are\n    installed, including installations named in `LD_LIBRARY_PATH`. If this\n    causes problems and the intent is to use a system-installed CUDA, the fix is\n    to remove the pip installed CUDA library packages.\n\n## jax 0.4.18 (Oct 6, 2023)\n\n## jaxlib 0.4.18 (Oct 6, 2023)\n\n* Changes\n  * CUDA jaxlibs now depend on the user to install a compatible NCCL version.\n    If using the recommended `cuda12_pip` installation, NCCL should be installed\n    automatically. Currently, NCCL 2.16 or newer is required.\n  * We now provide Linux aarch64 wheels, both with and without NVIDIA GPU\n    support.\n  * {meth}`jax.Array.item` now supports optional index arguments.\n\n* Deprecations\n  * A number of internal utilities and inadvertent exports in {mod}`jax.lax` have\n    been deprecated, and will be removed in a future release.\n    * `jax.lax.dtypes`: use `jax.dtypes` instead.\n    * `jax.lax.itertools`: use `itertools` instead.\n    * `naryop`, `naryop_dtype_rule`, `standard_abstract_eval`, `standard_naryop`,\n      `standard_primitive`, `standard_unop`, `unop`, and `unop_dtype_rule` are\n      internal utilities, now deprecated without replacement.\n\n* Bug fixes\n  * Fixed Cloud TPU regression where compilation would OOM due to smem.\n\n## jax 0.4.17 (Oct 3, 2023)\n\n* New features\n  * Added new {func}`jax.numpy.bitwise_count` function, matching the API of the similar\n    function recently added to NumPy.\n* Deprecations\n  * Removed the deprecated module `jax.abstract_arrays` and all its contents.\n  * Named key constructors in {mod}`jax.random` are deprecated. Pass the `impl` argument\n    to {func}`jax.random.PRNGKey` or {func}`jax.random.key` instead:\n    * `random.threefry2x32_key(seed)` becomes `random.PRNGKey(seed, impl='threefry2x32')`\n    * `random.rbg_key(seed)` becomes `random.PRNGKey(seed, impl='rbg')`\n    * `random.unsafe_rbg_key(seed)` becomes `random.PRNGKey(seed, impl='unsafe_rbg')`\n* Changes:\n  * CUDA: JAX now verifies that the CUDA libraries it finds are at least as new\n    as the CUDA libraries that JAX was built against. If older libraries are\n    found, JAX raises an exception since that is preferable to mysterious\n    failures and crashes.\n  * Removed the \"No GPU/TPU\" found warning. Instead warn if, on Linux, an\n    NVIDIA GPU or a Google TPU are found but not used and `--jax_platforms` was\n    not specified.\n  * {func}`jax.scipy.stats.mode` now returns a 0 count if the mode is taken\n    across a size-0 axis, matching the behavior of `scipy.stats.mode` in SciPy\n    1.11.\n  * Most `jax.numpy` functions and attributes now have fully-defined type stubs.\n    Previously many of these were treated as `Any` by static type checkers like\n    `mypy` and `pytype`.\n\n## jaxlib 0.4.17 (Oct 3, 2023)\n\n* Changes:\n  * Python 3.12 wheels were added in this release.\n  * The CUDA 12 wheels now require CUDA 12.2 or newer and cuDNN 8.9.4 or newer.\n\n* Bug fixes:\n  * Fixed log spam from ABSL when the JAX CPU backend was initialized.\n\n## jax 0.4.16 (Sept 18, 2023)\n\n* Changes\n  * Added {class}`jax.numpy.ufunc`, as well as {func}`jax.numpy.frompyfunc`, which can convert\n    any scalar-valued function into a {func}`numpy.ufunc`-like object, with methods such as\n    {meth}`~jax.numpy.ufunc.outer`, {meth}`~jax.numpy.ufunc.reduce`,\n    {meth}`~jax.numpy.ufunc.accumulate`, {meth}`~jax.numpy.ufunc.at`, and\n    {meth}`~jax.numpy.ufunc.reduceat` ({jax-issue}`#17054`).\n  * Added {func}`jax.scipy.integrate.trapezoid`.\n  * When not running under IPython: when an exception is raised, JAX now filters out the\n    entirety of its internal frames from tracebacks. (Without the \"unfiltered stack trace\"\n    that previously appeared.) This should produce much friendlier-looking tracebacks. See\n    [here](https://github.com/jax-ml/jax/pull/16949) for an example.\n    This behavior can be changed by setting `JAX_TRACEBACK_FILTERING=remove_frames` (for two\n    separate unfiltered/filtered tracebacks, which was the old behavior) or\n    `JAX_TRACEBACK_FILTERING=off` (for one unfiltered traceback).\n  * jax2tf default serialization version is now 7, which introduces new shape\n    [safety assertions](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#errors-in-presence-of-shape-polymorphism).\n  * Devices passed to `jax.sharding.Mesh` should be hashable. This specifically\n    applies to mock devices or user created devices. `jax.devices()` are\n    already hashable.\n\n* Breaking changes:\n  * jax2tf now uses native serialization by default. See\n    the [jax2tf documentation](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n    for details and for mechanisms to override the default.\n  * The option `--jax_coordination_service` has been removed. It is now always\n    `True`.\n  * `jax.jaxpr_util` has been removed from the public JAX namespace.\n  * `JAX_USE_PJRT_C_API_ON_TPU` no longer has an effect (i.e. it always defaults to true).\n  * The backwards compatibility flag `--jax_host_callback_ad_transforms`\n    introduced in December 2021, has been removed.\n\n* Deprecations:\n  * Several `jax.numpy` APIs have been deprecated following\n    [NumPy NEP-52](https://numpy.org/neps/nep-0052-python-api-cleanup.html):\n    * `jax.numpy.NINF` has been deprecated. Use `-jax.numpy.inf` instead.\n    * `jax.numpy.PZERO` has been deprecated. Use `0.0` instead.\n    * `jax.numpy.NZERO` has been deprecated. Use `-0.0` instead.\n    * `jax.numpy.issubsctype(x, t)` has been deprecated. Use `jax.numpy.issubdtype(x.dtype, t)`.\n    * `jax.numpy.row_stack` has been deprecated. Use `jax.numpy.vstack` instead.\n    * `jax.numpy.in1d` has been deprecated. Use `jax.numpy.isin` instead.\n    * `jax.numpy.trapz` has been deprecated. Use `jax.scipy.integrate.trapezoid` instead.\n  * `jax.scipy.linalg.tril` and `jax.scipy.linalg.triu` have been deprecated,\n    following SciPy. Use `jax.numpy.tril` and `jax.numpy.triu` instead.\n  * `jax.lax.prod` has been removed after being deprecated in JAX v0.4.11.\n    Use the built-in `math.prod` instead.\n  * A number of exports from `jax.interpreters.xla` related to defining\n    HLO lowering rules for custom JAX primitives have been deprecated. Custom\n    primitives should be defined using the StableHLO lowering utilities in\n    `jax.interpreters.mlir` instead.\n  * The following previously-deprecated functions have been removed after a\n    three-month deprecation period:\n    * `jax.abstract_arrays.ShapedArray`: use `jax.core.ShapedArray`.\n    * `jax.abstract_arrays.raise_to_shaped`: use `jax.core.raise_to_shaped`.\n    * `jax.numpy.alltrue`: use `jax.numpy.all`.\n    * `jax.numpy.sometrue`: use `jax.numpy.any`.\n    * `jax.numpy.product`: use `jax.numpy.prod`.\n    * `jax.numpy.cumproduct`: use `jax.numpy.cumprod`.\n\n* Deprecations/removals:\n  * The internal submodule `jax.prng` is now deprecated. Its contents are available at\n    {mod}`jax.extend.random`.\n  * The internal submodule path `jax.linear_util` has been deprecated. Use\n    {mod}`jax.extend.linear_util` instead (Part of {ref}`jax-extend-jep`)\n  * `jax.random.PRNGKeyArray` and `jax.random.KeyArray` are deprecated.  Use {class}`jax.Array`\n    for type annotations, and `jax.dtypes.issubdtype(arr.dtype, jax.dtypes.prng_key)` for\n    runtime detection of typed prng keys.\n  * The method `PRNGKeyArray.unsafe_raw_array` is deprecated. Use\n    {func}`jax.random.key_data` instead.\n  * `jax.experimental.pjit.with_sharding_constraint` is deprecated. Use\n    `jax.lax.with_sharding_constraint` instead.\n  * The internal utilities `jax.core.is_opaque_dtype` and `jax.core.has_opaque_dtype`\n    have been removed. Opaque dtypes have been renamed to Extended dtypes; use\n    `jnp.issubdtype(dtype, jax.dtypes.extended)` instead (available since jax v0.4.14).\n  * The utility `jax.interpreters.xla.register_collective_primitive` has been\n    removed. This utility did nothing useful in recent JAX releases and calls\n    to it can be safely removed.\n  * The internal submodule path `jax.linear_util` has been deprecated. Use\n    {mod}`jax.extend.linear_util` instead (Part of {ref}`jax-extend-jep`)\n\n## jaxlib 0.4.16 (Sept 18, 2023)\n\n* Changes:\n  * Sparse CSR matrix multiplications via the experimental jax sparse APIs\n    no longer uses a deterministic algorithm on NVIDIA GPUs. This change was\n    made to improve compatibility with CUDA 12.2.1.\n\n* Bug fixes:\n  * Fixed a crash on Windows due to a fatal LLVM error related to out-of-order\n    sections and IMAGE_REL_AMD64_ADDR32NB relocations\n    (https://github.com/openxla/xla/commit/cb732a921f0c4184995cbed82394931011d12bd4).\n\n## jax 0.4.14 (July 27, 2023)\n\n* Changes\n  * `jax.jit` takes `donate_argnames` as an argument. It's semantics are similar\n    to `static_argnames`.\n    If neither donate_argnums nor donate_argnames is provided, no\n    arguments are donated. If donate_argnums is not provided but\n    donate_argnames is, or vice versa, JAX uses\n    `inspect.signature(fun)` to find any positional arguments that\n    correspond to donate_argnames (or vice versa). If both donate_argnums and donate_argnames are provided, inspect.signature is not used, and only actual\n    parameters listed in either donate_argnums or donate_argnames will\n    be donated.\n  * {func}`jax.random.gamma` has been re-factored to a more efficient algorithm\n    with more robust endpoint behavior ({jax-issue}`#16779`). This means that the\n    sequence of values returned for a given `key` will change between JAX v0.4.13\n    and v0.4.14 for `gamma` and related samplers (including {func}`jax.random.ball`,\n    {func}`jax.random.beta`, {func}`jax.random.chisquare`, {func}`jax.random.dirichlet`,\n    {func}`jax.random.generalized_normal`, {func}`jax.random.loggamma`, {func}`jax.random.t`).\n\n* Deletions\n  * `in_axis_resources` and `out_axis_resources` have been deleted from pjit since\n    it has been more than 3 months since their deprecation. Please use\n    `in_shardings` and `out_shardings` as the replacement.\n    This is a safe and trivial name replacement. It does not change any of the\n    current pjit semantics and doesn't break any code.\n    You can still pass in `PartitionSpecs` to in_shardings and out_shardings.\n\n\n* Deprecations\n  * Python 3.8 support has been dropped as per\n    https://docs.jax.dev/en/latest/deprecation.html\n  * JAX now requires NumPy 1.22 or newer as per\n    https://docs.jax.dev/en/latest/deprecation.html\n  * Passing optional arguments to {attr}`jax.numpy.ndarray.at` by position is\n    no longer supported, after being deprecated in JAX version 0.4.7.\n    For example, instead of `x.at[i].get(True)`, use `x.at[i].get(indices_are_sorted=True)`\n  * The following `jax.Array` methods have been removed, after being deprecated\n    in JAX v0.4.5:\n    * `jax.Array.broadcast`: use {func}`jax.lax.broadcast` instead.\n    * `jax.Array.broadcast_in_dim`: use {func}`jax.lax.broadcast_in_dim` instead.\n    * `jax.Array.split`: use {func}`jax.numpy.split` instead.\n  * The following APIs have been removed after previous deprecation:\n    * `jax.ad`: use {mod}`jax.interpreters.ad`.\n    * `jax.curry`: use ``curry = lambda f: partial(partial, f)``.\n    * `jax.partial_eval`: use {mod}`jax.interpreters.partial_eval`.\n    * `jax.pxla`: use {mod}`jax.interpreters.pxla`.\n    * `jax.xla`: use {mod}`jax.interpreters.xla`.\n    * `jax.ShapedArray`: use {class}`jax.core.ShapedArray`.\n    * `jax.interpreters.pxla.device_put`: use {func}`jax.device_put`.\n    * `jax.interpreters.pxla.make_sharded_device_array`: use {func}`jax.make_array_from_single_device_arrays`.\n    * `jax.interpreters.pxla.ShardedDeviceArray`: use {class}`jax.Array`.\n    * `jax.numpy.DeviceArray`: use {class}`jax.Array`.\n    * `jax.stages.Compiled.compiler_ir`: use {func}`jax.stages.Compiled.as_text`.\n\n* Breaking changes\n  * JAX now requires ml_dtypes version 0.2.0 or newer.\n  * To fix a corner case, calls to {func}`jax.lax.cond` with five\n    arguments will always resolve to the \"common operands\" `cond`\n    behavior (as documented) if the second and third arguments are\n    callable, even if other operands are callable as well. See\n    [#16413](https://github.com/jax-ml/jax/issues/16413).\n  * The deprecated config options `jax_array` and `jax_jit_pjit_api_merge`,\n    which did nothing, have been removed. These options have been true by\n    default for many releases.\n\n* New features\n  * JAX now supports a configuration flag --jax_serialization_version\n    and a JAX_SERIALIZATION_VERSION environment variable to control the\n    serialization version ({jax-issue}`#16746`).\n  * jax2tf in presence of shape polymorphism now generates code that checks\n    certain shape constraints, if the serialization version is at least 7.\n    See https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#errors-in-presence-of-shape-polymorphism.\n\n## jaxlib 0.4.14 (July 27, 2023)\n\n* Deprecations\n  * Python 3.8 support has been dropped as per\n      https://docs.jax.dev/en/latest/deprecation.html\n\n## jax 0.4.13 (June 22, 2023)\n\n* Changes\n  * `jax.jit` now allows `None` to be passed to `in_shardings` and\n    `out_shardings`. The semantics are as follows:\n      * For in_shardings, JAX will mark is as replicated but this behavior\n        can change in the future.\n      * For out_shardings, we will rely on the XLA GSPMD partitioner to\n        determine the output shardings.\n  * `jax.experimental.pjit.pjit` also allows `None` to be passed to\n    `in_shardings` and `out_shardings`. The semantics are as follows:\n    * If the mesh context manager is *not* provided, JAX has the freedom to\n      choose whatever sharding it wants.\n      * For in_shardings, JAX will mark is as replicated but this behavior\n        can change in the future.\n      * For out_shardings, we will rely on the XLA GSPMD partitioner to\n        determine the output shardings.\n    * If the mesh context manager is provided, None will imply that the value\n      will be replicated on all devices of the mesh.\n  * Executable.cost_analysis() works on Cloud TPU\n  * Added a warning if a non-allowlisted `jaxlib` plugin is in use.\n  * Added `jax.tree_util.tree_leaves_with_path`.\n  * `None` is not a valid input to\n    `jax.experimental.multihost_utils.host_local_array_to_global_array` or\n    `jax.experimental.multihost_utils.global_array_to_host_local_array`.\n    Please use `jax.sharding.PartitionSpec()` if you wanted to replicate your\n    input.\n\n* Bug fixes\n  * Fixed incorrect wheel name in CUDA 12 releases (#16362); the correct wheel\n    is named `cudnn89` instead of `cudnn88`.\n\n* Deprecations\n  * The `native_serialization_strict_checks` parameter to\n    {func}`jax.experimental.jax2tf.convert` is deprecated in favor of the\n    new `native_serializaation_disabled_checks` ({jax-issue}`#16347`).\n\n## jaxlib 0.4.13 (June 22, 2023)\n\n* Changes\n  * Added Windows CPU-only wheels to the `jaxlib` Pypi release.\n\n* Bug fixes\n  * `__cuda_array_interface__` was broken in previous jaxlib versions and is now\n    fixed ({jax-issue}`16440`).\n  * Concurrent CUDA kernel tracing is now enabled by default on NVIDIA GPUs.\n\n## jax 0.4.12 (June 8, 2023)\n\n* Changes\n  * Added {class}`scipy.spatial.transform.Rotation` and {class}`scipy.spatial.transform.Slerp`\n\n* Deprecations\n  * `jax.abstract_arrays` and its contents are now deprecated. See related\n    functionality in {mod}`jax.core`.\n  * `jax.numpy.alltrue`: use `jax.numpy.all`. This follows the deprecation\n    of `numpy.alltrue` in NumPy version 1.25.0.\n  * `jax.numpy.sometrue`: use `jax.numpy.any`. This follows the deprecation\n    of `numpy.sometrue` in NumPy version 1.25.0.\n  * `jax.numpy.product`: use `jax.numpy.prod`. This follows the deprecation\n    of `numpy.product` in NumPy version 1.25.0.\n  * `jax.numpy.cumproduct`: use `jax.numpy.cumprod`. This follows the deprecation\n    of `numpy.cumproduct` in NumPy version 1.25.0.\n  * `jax.sharding.OpShardingSharding` has been removed since it has been 3\n    months since it was deprecated.\n\n## jaxlib 0.4.12 (June 8, 2023)\n\n* Changes\n  * Includes PTX/SASS for Hopper (SM version 9.0+) GPUs. Previous\n    versions of jaxlib should work on Hopper but would have a long\n    JIT-compilation delay the first time a JAX operation was executed.\n\n* Bug fixes\n  * Fixes incorrect source line information in JAX-generated Python tracebacks\n    under Python 3.11.\n  * Fixes crash when printing local variables of frames in JAX-generated Python\n    tracebacks (#16027).\n\n## jax 0.4.11 (May 31, 2023)\n\n* Deprecations\n  * The following APIs have been removed after a 3 month deprecation period, in\n    accordance with the {ref}`api-compatibility` policy:\n    * `jax.experimental.PartitionSpec`: use `jax.sharding.PartitionSpec`.\n    * `jax.experimental.maps.Mesh`: use `jax.sharding.Mesh`\n    * `jax.experimental.pjit.NamedSharding`: use `jax.sharding.NamedSharding`.\n    * `jax.experimental.pjit.PartitionSpec`: use `jax.sharding.PartitionSpec`.\n    * `jax.experimental.pjit.FROM_GDA`. Instead pass sharded `jax.Array` objects\n      as input and remove the optional `in_shardings` argument to `pjit`.\n    * `jax.interpreters.pxla.PartitionSpec`: use `jax.sharding.PartitionSpec`.\n    * `jax.interpreters.pxla.Mesh`: use `jax.sharding.Mesh`\n    * `jax.interpreters.xla.Buffer`: use `jax.Array`.\n    * `jax.interpreters.xla.Device`: use `jax.Device`.\n    * `jax.interpreters.xla.DeviceArray`: use `jax.Array`.\n    * `jax.interpreters.xla.device_put`: use `jax.device_put`.\n    * `jax.interpreters.xla.xla_call_p`: use `jax.experimental.pjit.pjit_p`.\n    * `axis_resources` argument of `with_sharding_constraint` is removed. Please\n      use `shardings` instead.\n\n\n## jaxlib 0.4.11 (May 31, 2023)\n\n* Changes\n  * Added `memory_stats()` method to `Device`s. If supported, this returns a\n    dict of string stat names with int values, e.g. `\"bytes_in_use\"`, or None if\n    the platform doesn't support memory statistics. The exact stats returned may\n    vary across platforms. Currently only implemented on Cloud TPU.\n  * Re-added support for the Python buffer protocol (`memoryview`) on CPU\n    devices.\n\n## jax 0.4.10 (May 11, 2023)\n\n## jaxlib 0.4.10 (May 11, 2023)\n\n* Changes\n  * Fixed `'apple-m1' is not a recognized processor for this target (ignoring\n    processor)` issue that prevented previous release from running on Mac M1.\n\n## jax 0.4.9 (May 9, 2023)\n\n* Changes\n  * The flags experimental_cpp_jit, experimental_cpp_pjit and\n    experimental_cpp_pmap have been removed.\n    They are now always on.\n  * Accuracy of singular value decomposition (SVD) on TPU has been improved\n    (requires jaxlib 0.4.9).\n\n* Deprecations\n  * `jax.experimental.gda_serialization` is deprecated and has been renamed to\n    `jax.experimental.array_serialization`.\n    Please change your imports to use `jax.experimental.array_serialization`.\n  * The `in_axis_resources` and `out_axis_resources` arguments of pjit have been\n    deprecated. Please use `in_shardings` and `out_shardings` respectively.\n  * The function `jax.numpy.msort` has been removed. It has been deprecated since\n    JAX v0.4.1. Use `jnp.sort(a, axis=0)` instead.\n  * `in_parts` and `out_parts` arguments have been removed from `jax.xla_computation`\n    since they were only used with sharded_jit and sharded_jit is long gone.\n  * `instantiate_const_outputs` argument has been removed from `jax.xla_computation`\n    since it has been unused for a very long time.\n\n## jaxlib 0.4.9 (May 9, 2023)\n\n## jax 0.4.8 (March 29, 2023)\n\n* Breaking changes\n  * A major component of the Cloud TPU runtime has been upgraded. This enables\n    the following new features on Cloud TPU:\n    * {func}`jax.debug.print`, {func}`jax.debug.callback`, and\n      {func}`jax.debug.breakpoint()` now work on Cloud TPU\n    * Automatic TPU memory defragmentation\n\n    {func}`jax.experimental.host_callback` is no longer supported on Cloud TPU\n    with the new runtime component. Please file an issue on the [JAX issue\n    tracker](https://github.com/jax-ml/jax/issues) if the new `jax.debug` APIs\n    are insufficient for your use case.\n\n    The old runtime component will be available for at least the next three\n    months by setting the environment variable\n    `JAX_USE_PJRT_C_API_ON_TPU=false`. If you find you need to disable the new\n    runtime for any reason, please let us know on the [JAX issue\n    tracker](https://github.com/jax-ml/jax/issues).\n\n* Changes\n  * The minimum jaxlib version has been bumped from 0.4.6 to 0.4.7.\n\n* Deprecations\n  * CUDA 11.4 support has been dropped. JAX GPU wheels only support\n    CUDA 11.8 and CUDA 12. Older CUDA versions may work if jaxlib is built\n    from source.\n  * `global_arg_shapes` argument of pmap only worked with sharded_jit and has\n    been removed from pmap. Please migrate to pjit and remove global_arg_shapes\n    from pmap.\n\n## jax 0.4.7 (March 27, 2023)\n\n* Changes\n  * As per https://docs.jax.dev/en/latest/jax_array_migration.html#jax-array-migration\n    `jax.config.jax_array` cannot be disabled anymore.\n  * `jax.config.jax_jit_pjit_api_merge` cannot be disabled anymore.\n  * {func}`jax.experimental.jax2tf.convert` now supports the `native_serialization`\n    parameter to use JAX's native lowering to StableHLO to obtain a\n    StableHLO module for the entire JAX function instead of lowering each JAX\n    primitive to a TensorFlow op. This simplifies the internals and increases\n    the confidence that what you serialize matches the JAX native semantics.\n    See [documentation](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md).\n    As part of this change the config flag `--jax2tf_default_experimental_native_lowering`\n    has been renamed to `--jax2tf_native_serialization`.\n  * JAX now depends on `ml_dtypes`, which contains definitions of NumPy types\n    like bfloat16. These definitions were previously internal to JAX, but have\n    been split into a separate package to facilitate sharing them with other\n    projects.\n  * JAX now requires NumPy 1.21 or newer and SciPy 1.7 or newer.\n\n* Deprecations\n  * The type `jax.numpy.DeviceArray` is deprecated. Use `jax.Array` instead,\n    for which it is an alias.\n  * The type `jax.interpreters.pxla.ShardedDeviceArray` is deprecated. Use\n    `jax.Array` instead.\n  * Passing additional arguments to {attr}`jax.numpy.ndarray.at` by position is deprecated.\n    For example, instead of `x.at[i].get(True)`, use `x.at[i].get(indices_are_sorted=True)`\n  * `jax.interpreters.xla.device_put` is deprecated. Please use `jax.device_put`.\n  * `jax.interpreters.pxla.device_put` is deprecated. Please use `jax.device_put`.\n  * `jax.experimental.pjit.FROM_GDA` is deprecated. Please pass in sharded\n    jax.Arrays as input and remove the `in_shardings` argument to pjit since\n    it is optional.\n\n## jaxlib 0.4.7 (March 27, 2023)\n\nChanges:\n  * jaxlib now depends on `ml_dtypes`, which contains definitions of NumPy types\n    like bfloat16. These definitions were previously internal to JAX, but have\n    been split into a separate package to facilitate sharing them with other\n    projects.\n\n## jax 0.4.6 (Mar 9, 2023)\n\n* Changes\n  * `jax.tree_util` now contain a set of APIs that allow user to define keys for their\n    custom pytree node. This includes:\n    * `tree_flatten_with_path` that flattens a tree and return not only each leaf but\n      also their key paths.\n    * `tree_map_with_path` that can map a function that takes the key path as an argument.\n    * `register_pytree_with_keys` to register how the key path and leaves should looks\n      like in a custom pytree node.\n    * `keystr` that pretty-prints a key path.\n\n  * {func}`jax2tf.call_tf` has a new parameter `output_shape_dtype` (default `None`)\n    that can be used to declare the output shape and type of the result. This enables\n    {func}`jax2tf.call_tf` to work in the presence of shape polymorphism. ({jax-issue}`#14734`).\n\n* Deprecations\n  * The old key-path APIs in `jax.tree_util` are deprecated and will be removed 3 months\n    from Mar 10 2023:\n    * `register_keypaths`: use {func}`jax.tree_util.register_pytree_with_keys` instead.\n    * `AttributeKeyPathEntry` : use `GetAttrKey` instead.\n    * `GetitemKeyPathEntry` : use `SequenceKey` or `DictKey` instead.\n\n## jaxlib 0.4.6 (Mar 9, 2023)\n\n## jax 0.4.5 (Mar 2, 2023)\n\n* Deprecations\n  * `jax.sharding.OpShardingSharding` has been renamed to `jax.sharding.GSPMDSharding`.\n    `jax.sharding.OpShardingSharding` will be removed in 3 months from Feb 17, 2023.\n  * The following `jax.Array` methods are deprecated and will be removed 3 months from\n    Feb 23 2023:\n    * `jax.Array.broadcast`: use {func}`jax.lax.broadcast` instead.\n    * `jax.Array.broadcast_in_dim`: use {func}`jax.lax.broadcast_in_dim` instead.\n    * `jax.Array.split`: use {func}`jax.numpy.split` instead.\n\n## jax 0.4.4 (Feb 16, 2023)\n\n* Changes\n  * The implementation of `jit` and `pjit` has been merged. Merging jit and pjit\n    changes the internals of JAX without affecting the public API of JAX.\n    Before, `jit` was a final style primitive. Final style means that the creation\n    of jaxpr was delayed as much as possible and transformations were stacked\n    on top of each other. With the `jit`-`pjit` implementation merge, `jit`\n    becomes an initial style primitive which means that we trace to jaxpr\n    as early as possible. For more information see\n    [this section in autodidax](https://docs.jax.dev/en/latest/autodidax.html#on-the-fly-final-style-and-staged-initial-style-processing).\n    Moving to initial style should simplify JAX's internals and make\n    development of features like dynamic shapes, etc easier.\n    You can disable it only via the environment variable i.e.\n    `os.environ['JAX_JIT_PJIT_API_MERGE'] = '0'`.\n    The merge must be disabled via an environment variable since it affects JAX\n    at import time so it needs to be disabled before jax is imported.\n  * `axis_resources` argument of `with_sharding_constraint` is deprecated.\n    Please use `shardings` instead. There is no change needed if you were using\n    `axis_resources` as an arg. If you were using it as a kwarg, then please\n    use `shardings` instead. `axis_resources` will be removed after 3 months\n    from Feb 13, 2023.\n  * added the {mod}`jax.typing` module, with tools for type annotations of JAX\n    functions.\n  * The following names have been deprecated:\n    * `jax.xla.Device` and `jax.interpreters.xla.Device`: use `jax.Device`.\n    * `jax.experimental.maps.Mesh`. Use `jax.sharding.Mesh`\n    instead.\n    * `jax.experimental.pjit.NamedSharding`: use `jax.sharding.NamedSharding`.\n    * `jax.experimental.pjit.PartitionSpec`: use `jax.sharding.PartitionSpec`.\n    * `jax.interpreters.pxla.Mesh`: use `jax.sharding.Mesh`.\n    * `jax.interpreters.pxla.PartitionSpec`: use `jax.sharding.PartitionSpec`.\n* Breaking Changes\n  * the `initial` argument to reduction functions like {func}`jax.numpy.sum`\n    is now required to be a scalar, consistent with the corresponding NumPy API.\n    The previous behavior of broadcasting the output against non-scalar `initial`\n    values was an unintentional implementation detail ({jax-issue}`#14446`).\n\n## jaxlib 0.4.4 (Feb 16, 2023)\n  * Breaking changes\n    * Support for NVIDIA Kepler series GPUs has been removed from the default\n      `jaxlib` builds. If Kepler support is needed, it is still possible to\n      build `jaxlib` from source with Kepler support (via the\n      `--cuda_compute_capabilities=sm_35` option to `build.py`), however note\n      that CUDA 12 has completely dropped support for Kepler GPUs.\n\n## jax 0.4.3 (Feb 8, 2023)\n  * Breaking changes\n    * Deleted {func}`jax.scipy.linalg.polar_unitary`, which was a deprecated JAX\n      extension to the scipy API. Use {func}`jax.scipy.linalg.polar` instead.\n\n  * Changes\n    * Added {func}`jax.scipy.stats.rankdata`.\n\n## jaxlib 0.4.3 (Feb 8, 2023)\n  * `jax.Array` now has the non-blocking `is_ready()` method, which returns `True`\n    if the array is ready (see also {func}`jax.block_until_ready`).\n\n## jax 0.4.2 (Jan 24, 2023)\n\n* Breaking changes\n  * Deleted `jax.experimental.callback`\n  * Operations with dimensions in presence of jax2tf shape polymorphism have\n    been generalized to work in more scenarios, by converting the symbolic\n    dimension to JAX arrays. Operations involving symbolic dimensions and\n    `np.ndarray` now can raise errors when the result is used as a shape value\n    ({jax-issue}`#14106`).\n  * jaxpr objects now raise an error on attribute setting in order to avoid\n    problematic mutations ({jax-issue}`14102`)\n\n* Changes\n  * {func}`jax2tf.call_tf` has a new parameter `has_side_effects` (default `True`)\n    that can be used to declare whether an instance can be removed or replicated\n    by JAX optimizations such as dead-code elimination ({jax-issue}`#13980`).\n  * Added more support for floordiv and mod for jax2tf shape polymorphism. Previously,\n    certain division operations resulted in errors in presence of symbolic dimensions\n    ({jax-issue}`#14108`).\n\n## jaxlib 0.4.2 (Jan 24, 2023)\n\n* Changes\n  * Set JAX_USE_PJRT_C_API_ON_TPU=1 to enable new Cloud TPU runtime, featuring\n    automatic device memory defragmentation.\n\n## jax 0.4.1 (Dec 13, 2022)\n\n* Changes\n  * Support for Python 3.7 has been dropped, in accordance with JAX's\n    {ref}`version-support-policy`.\n  * We introduce `jax.Array` which is a unified array type that subsumes\n    `DeviceArray`, `ShardedDeviceArray`, and `GlobalDeviceArray` types in JAX.\n    The `jax.Array` type helps make parallelism a core feature of JAX,\n    simplifies and unifies JAX internals, and allows us to unify `jit` and\n    `pjit`.  `jax.Array` has been enabled by default in JAX 0.4 and makes some\n    breaking change to the `pjit` API.  The [jax.Array migration\n    guide](https://docs.jax.dev/en/latest/jax_array_migration.html) can\n    help you migrate your codebase to `jax.Array`. You can also look at the\n    [Distributed arrays and automatic parallelization](https://docs.jax.dev/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html)\n    tutorial to understand the new concepts.\n  * `PartitionSpec` and `Mesh` are now out of experimental. The new API endpoints\n    are `jax.sharding.PartitionSpec` and `jax.sharding.Mesh`.\n    `jax.experimental.maps.Mesh` and `jax.experimental.PartitionSpec` are\n    deprecated and will be removed in 3 months.\n  * `with_sharding_constraint`s new public endpoint is\n    `jax.lax.with_sharding_constraint`.\n  * If using ABSL flags together with `jax.config`, the ABSL flag values are no\n    longer read or written after the JAX configuration options are initially\n    populated from the ABSL flags. This change improves performance of reading\n    `jax.config` options, which are used pervasively in JAX.\n  * The jax2tf.call_tf function now uses for TF lowering the first TF\n    device of the same platform as used by the embedding JAX computation.\n    Before, it was using the 0th device for the JAX-default backend.\n  * A number of `jax.numpy` functions now have their arguments marked as\n    positional-only, matching NumPy.\n  * `jnp.msort` is now deprecated, following the deprecation of `np.msort` in numpy 1.24.\n    It will be removed in a future release, in accordance with the {ref}`api-compatibility`\n    policy. It can be replaced with `jnp.sort(a, axis=0)`.\n\n## jaxlib 0.4.1 (Dec 13, 2022)\n\n* Changes\n  * Support for Python 3.7 has been dropped, in accordance with JAX's\n    {ref}`version-support-policy`.\n  * The behavior of `XLA_PYTHON_CLIENT_MEM_FRACTION=.XX` has been changed to allocate XX% of\n    the total GPU memory instead of the previous behavior of using currently available GPU memory\n    to calculate preallocation. Please refer to\n    [GPU memory allocation](https://docs.jax.dev/en/latest/gpu_memory_allocation.html) for\n    more details.\n  * The deprecated method `.block_host_until_ready()` has been removed. Use\n    `.block_until_ready()` instead.\n\n## jax 0.4.0 (Dec 12, 2022)\n\n* The release was yanked.\n\n## jaxlib 0.4.0 (Dec 12, 2022)\n\n* The release was yanked.\n\n## jax 0.3.25 (Nov 15, 2022)\n* Changes\n  * {func}`jax.numpy.linalg.pinv` now supports the `hermitian` option.\n  * {func}`jax.scipy.linalg.hessenberg` is now supported on CPU only. Requires\n    jaxlib > 0.3.24.\n  * New functions {func}`jax.lax.linalg.hessenberg`,\n    {func}`jax.lax.linalg.tridiagonal`, and\n    {func}`jax.lax.linalg.householder_product` were added. Householder reduction\n    is currently CPU-only and tridiagonal reductions are supported on CPU and\n    GPU only.\n  * The gradients of `svd` and `jax.numpy.linalg.pinv` are now computed more\n    economically for non-square matrices.\n* Breaking Changes\n  * Deleted the `jax_experimental_name_stack` config option.\n  * Convert a string `axis_names` arguments to the\n    {class}`jax.experimental.maps.Mesh` constructor into a singleton tuple\n    instead of unpacking the string into a sequence of character axis names.\n\n## jaxlib 0.3.25 (Nov 15, 2022)\n* Changes\n  * Added support for tridiagonal reductions on CPU and GPU.\n  * Added support for upper Hessenberg reductions on CPU.\n* Bugs\n  * Fixed a bug that meant that frames in tracebacks captured by JAX were\n    incorrectly mapped to source lines under Python 3.10+\n\n## jax 0.3.24 (Nov 4, 2022)\n* Changes\n  * JAX should be faster to import. We now import scipy lazily, which accounted\n    for a significant fraction of JAX's import time.\n  * Setting the env var `JAX_PERSISTENT_CACHE_MIN_COMPILE_TIME_SECS=$N` can be\n    used to limit the number of cache entries written to the persistent cache.\n    By default, computations that take 1 second or more to compile will be\n    cached.\n    * Added {func}`jax.scipy.stats.mode`.\n  * The default device order used by `pmap` on TPU if no order is specified now\n    matches `jax.devices()` for single-process jobs. Previously the\n    two orderings differed, which could lead to unnecessary copies or\n    out-of-memory errors. Requiring the orderings to agree simplifies matters.\n* Breaking Changes\n    * {func}`jax.numpy.gradient` now behaves like most other functions in {mod}`jax.numpy`,\n      and forbids passing lists or tuples in place of arrays ({jax-issue}`#12958`)\n    * Functions in {mod}`jax.numpy.linalg` and {mod}`jax.numpy.fft` now uniformly\n      require inputs to be array-like: i.e. lists and tuples cannot be used in place\n      of arrays. Part of {jax-issue}`#7737`.\n* Deprecations\n  * `jax.sharding.MeshPspecSharding` has been renamed to `jax.sharding.NamedSharding`.\n    `jax.sharding.MeshPspecSharding` name will be removed in 3 months.\n\n## jaxlib 0.3.24 (Nov 4, 2022)\n* Changes\n  * Buffer donation now works on CPU. This may break code that marked buffers\n    for donation on CPU but relied on donation not being implemented.\n\n## jax 0.3.23 (Oct 12, 2022)\n* Changes\n  * Update Colab TPU driver version for new jaxlib release.\n\n## jax 0.3.22 (Oct 11, 2022)\n* Changes\n  * Add `JAX_PLATFORMS=tpu,cpu` as default setting in TPU initialization,\n  so JAX will raise an error if TPU cannot be initialized instead of falling\n  back to CPU. Set `JAX_PLATFORMS=''` to override this behavior and automatically\n  choose an available backend (the original default), or set `JAX_PLATFORMS=cpu`\n  to always use CPU regardless of if the TPU is available.\n* Deprecations\n  * Several test utilities deprecated in JAX v0.3.8 are now removed from\n    {mod}`jax.test_util`.\n\n## jaxlib 0.3.22 (Oct 11, 2022)\n\n## jax 0.3.21 (Sep 30, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.20...jax-v0.3.21).\n* Changes\n  * The persistent compilation cache will now warn instead of raising an\n    exception on error ({jax-issue}`#12582`), so program execution can continue\n    if something goes wrong with the cache. Set\n    `JAX_RAISE_PERSISTENT_CACHE_ERRORS=true` to revert this behavior.\n\n## jax 0.3.20 (Sep 28, 2022)\n* Bug fixes:\n  * Adds missing `.pyi` files that were missing from the previous release ({jax-issue}`#12536`).\n  * Fixes an incompatibility between `jax` 0.3.19 and the libtpu version it pinned ({jax-issue}`#12550`). Requires jaxlib 0.3.20.\n  * Fix incorrect `pip` url in `setup.py` comment ({jax-issue}`#12528`).\n\n## jaxlib 0.3.20 (Sep 28, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jaxlib-v0.3.15...jaxlib-v0.3.20).\n* Bug fixes\n  * Fixes support for limiting the visible CUDA devices via\n   `jax_cuda_visible_devices` in distributed jobs. This functionality is needed for\n   the JAX/SLURM integration on GPU ({jax-issue}`#12533`).\n\n## jax 0.3.19 (Sep 27, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.18...jax-v0.3.19).\n* Fixes required jaxlib version.\n\n## jax 0.3.18 (Sep 26, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.17...jax-v0.3.18).\n* Changes\n  * Ahead-of-time lowering and compilation functionality (tracked in\n    {jax-issue}`#7733`) is stable and public. See [the\n    overview](https://docs.jax.dev/en/latest/aot.html) and the API docs\n    for {mod}`jax.stages`.\n  * Introduced {class}`jax.Array`, intended to be used for both `isinstance` checks\n    and type annotations for array types in JAX. Notice that this included some subtle\n    changes to how `isinstance` works for {class}`jax.numpy.ndarray` for jax-internal\n    objects, as {class}`jax.numpy.ndarray` is now a simple alias of {class}`jax.Array`.\n* Breaking changes\n  * `jax._src` is no longer imported into the public `jax` namespace.\n    This may break users that were using JAX internals.\n  * `jax.soft_pmap` has been deleted. Please use `pjit` or `xmap` instead.\n    `jax.soft_pmap` is undocumented. If it were documented, a deprecation period\n    would have been provided.\n\n## jax 0.3.17 (Aug 31, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.16...jax-v0.3.17).\n* Bugs\n  * Fix corner case issue in gradient of `lax.pow` with an exponent of zero\n    ({jax-issue}`12041`)\n* Breaking changes\n  * {func}`jax.checkpoint`, also known as {func}`jax.remat`, no longer supports\n    the `concrete` option, following the previous version's deprecation; see\n    [JEP 11830](https://docs.jax.dev/en/latest/jep/11830-new-remat-checkpoint.html).\n* Changes\n  * Added {func}`jax.pure_callback` that enables calling back to pure Python functions from compiled functions (e.g. functions decorated with `jax.jit` or `jax.pmap`).\n* Deprecations:\n  * The deprecated `DeviceArray.tile()` method has been removed. Use {func}`jax.numpy.tile`\n    ({jax-issue}`#11944`).\n  * `DeviceArray.to_py()` has been deprecated. Use `np.asarray(x)` instead.\n\n## jax 0.3.16\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.15...main).\n* Breaking changes\n  * Support for NumPy 1.19 has been dropped, per the\n    [deprecation policy](https://docs.jax.dev/en/latest/deprecation.html).\n    Please upgrade to NumPy 1.20 or newer.\n* Changes\n  * Added {mod}`jax.debug` that includes utilities for runtime value debugging such at {func}`jax.debug.print` and {func}`jax.debug.breakpoint`.\n  * Added new documentation for [runtime value debugging](https://github.com/jax-ml/jax/blob/7ac8181cce087d8bcd564d07e19f5067cb5d9d3b/docs/debugging/index.md)\n* Deprecations\n  * {func}`jax.mask` {func}`jax.shapecheck` APIs have been removed.\n    See {jax-issue}`#11557`.\n  * {mod}`jax.experimental.loops` has been removed. See {jax-issue}`#10278`\n    for an alternative API.\n  * {func}`jax.tree_util.tree_multimap` has been removed. It has been deprecated since\n    JAX release 0.3.5, and {func}`jax.tree_util.tree_map` is a direct replacement.\n  * Removed `jax.experimental.stax`; it has long been a deprecated alias of\n    {mod}`jax.example_libraries.stax`.\n  * Removed `jax.experimental.optimizers`; it has long been a deprecated alias of\n    {mod}`jax.example_libraries.optimizers`.\n  * {func}`jax.checkpoint`, also known as {func}`jax.remat`, has a new\n    implementation switched on by default, meaning the old implementation is\n    deprecated; see [JEP 11830](https://docs.jax.dev/en/latest/jep/11830-new-remat-checkpoint.html).\n\n## jax 0.3.15 (July 22, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.14...jax-v0.3.15).\n* Changes\n  * `JaxTestCase` and `JaxTestLoader` have been removed from `jax.test_util`. These\n    classes have been deprecated since v0.3.1 ({jax-issue}`#11248`).\n  * Added {class}`jax.scipy.gaussian_kde` ({jax-issue}`#11237`).\n  * Binary operations between JAX arrays and built-in collections (`dict`, `list`, `set`, `tuple`)\n    now raise a `TypeError` in all cases. Previously some cases (particularly equality and inequality)\n    would return boolean scalars inconsistent with similar operations in NumPy ({jax-issue}`#11234`).\n  * Several {mod}`jax.tree_util` routines accessed as top-level JAX package imports are now\n    deprecated, and will be removed in a future JAX release in accordance with the\n    {ref}`api-compatibility` policy:\n    * {func}`jax.treedef_is_leaf` is deprecated in favor of {func}`jax.tree_util.treedef_is_leaf`\n    * {func}`jax.tree_flatten` is deprecated in favor of {func}`jax.tree_util.tree_flatten`\n    * {func}`jax.tree_leaves` is deprecated in favor of {func}`jax.tree_util.tree_leaves`\n    * {func}`jax.tree_structure` is deprecated in favor of {func}`jax.tree_util.tree_structure`\n    * {func}`jax.tree_transpose` is deprecated in favor of {func}`jax.tree_util.tree_transpose`\n    * {func}`jax.tree_unflatten` is deprecated in favor of {func}`jax.tree_util.tree_unflatten`\n  * The `sym_pos` argument of {func}`jax.scipy.linalg.solve` is deprecated in favor of `assume_a='pos'`,\n    following a similar deprecation in {func}`scipy.linalg.solve`.\n\n## jaxlib 0.3.15 (July 22, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jaxlib-v0.3.14...jaxlib-v0.3.15).\n\n## jax 0.3.14 (June 27, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.13...jax-v0.3.14).\n* Breaking changes\n  * {func}`jax.experimental.compilation_cache.initialize_cache` does not support\n    `max_cache_size_  bytes` anymore and will not get that as an input.\n  * `JAX_PLATFORMS` now raises an exception when platform initialization fails.\n* Changes\n  * Fixed compatibility problems with NumPy 1.23.\n  * {func}`jax.numpy.linalg.slogdet` now accepts an optional `method` argument\n    that allows selection between an LU-decomposition based implementation and\n    an implementation based on QR decomposition.\n  * {func}`jax.numpy.linalg.qr` now supports `mode=\"raw\"`.\n  * `pickle`, `copy.copy`, and `copy.deepcopy` now have more complete support when\n    used on jax arrays ({jax-issue}`#10659`). In particular:\n    - `pickle` and `deepcopy` previously returned `np.ndarray` objects when used\n      on a `DeviceArray`; now `DeviceArray` objects are returned. For `deepcopy`,\n      the copied array is on the same device as the original. For `pickle` the\n      deserialized array will be on the default device.\n    - Within function transformations (i.e. traced code), `deepcopy` and `copy`\n      previously were no-ops. Now they use the same mechanism as `DeviceArray.copy()`.\n    - Calling `pickle` on a traced array now results in an explicit\n      `ConcretizationTypeError`.\n  * The implementation of singular value decomposition (SVD) and\n    symmetric/Hermitian eigendecomposition should be significantly faster on\n    TPU, especially for matrices above 1000x1000 or so. Both now use a spectral\n    divide-and-conquer algorithm for eigendecomposition (QDWH-eig).\n  * {func}`jax.numpy.ldexp` no longer silently promotes all inputs to float64,\n    instead it promotes to float32 for integer inputs of size int32 or smaller\n    ({jax-issue}`#10921`).\n  * Add a `create_perfetto_link` option to {func}`jax.profiler.start_trace` and\n    {func}`jax.profiler.start_trace`. When used, the profiler will generate a\n    link to the Perfetto UI to view the trace.\n  * Changed the semantics of {func}`jax.profiler.start_server(...)` to store the\n    keepalive globally, rather than requiring the user to keep a reference to\n    it.\n  * Added {func}`jax.random.generalized_normal`.\n  * Added {func}`jax.random.ball`.\n  * Added {func}`jax.default_device`.\n  * Added a `python -m jax.collect_profile` script to manually capture program\n    traces as an alternative to the TensorBoard UI.\n  * Added a `jax.named_scope` context manager that adds profiler metadata to\n    Python programs (similar to `jax.named_call`).\n  * In scatter-update operations (i.e. {attr}`jax.numpy.ndarray.at`), unsafe implicit\n    dtype casts are deprecated, and now result in a `FutureWarning`.\n    In a future release, this will become an error. An example of an unsafe implicit\n    cast is `jnp.zeros(4, dtype=int).at[0].set(1.5)`, in which `1.5` previously was\n    silently truncated to `1`.\n  * {func}`jax.experimental.compilation_cache.initialize_cache` now supports gcs\n    bucket path as input.\n  * Added {func}`jax.scipy.stats.gennorm`.\n  * {func}`jax.numpy.roots` is now better behaved when `strip_zeros=False` when\n    coefficients have leading zeros ({jax-issue}`#11215`).\n\n## jaxlib 0.3.14 (June 27, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jaxlib-v0.3.10...jaxlib-v0.3.14).\n  * x86-64 Mac wheels now require Mac OS 10.14 (Mojave) or newer. Mac OS 10.14\n    was released in 2018, so this should not be a very onerous requirement.\n  * The bundled version of NCCL was updated to 2.12.12, fixing some deadlocks.\n  * The Python flatbuffers package is no longer a dependency of jaxlib.\n\n## jax 0.3.13 (May 16, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.12...jax-v0.3.13).\n\n## jax 0.3.12 (May 15, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.11...jax-v0.3.12).\n* Changes\n  * Fixes [#10717](https://github.com/jax-ml/jax/issues/10717).\n\n## jax 0.3.11 (May 15, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.10...jax-v0.3.11).\n* Changes\n  * {func}`jax.lax.eigh` now accepts an optional `sort_eigenvalues` argument\n    that allows users to opt out of eigenvalue sorting on TPU.\n* Deprecations\n  * Non-array arguments to functions in {mod}`jax.lax.linalg` are now marked\n    keyword-only. As a backward-compatibility step passing keyword-only\n    arguments positionally yields a warning, but in a future JAX release passing\n    keyword-only arguments positionally will fail.\n    However, most users should prefer to use {mod}`jax.numpy.linalg` instead.\n  * {func}`jax.scipy.linalg.polar_unitary`, which was a JAX extension to the\n    scipy API, is deprecated. Use {func}`jax.scipy.linalg.polar` instead.\n\n## jax 0.3.10 (May 3, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.9...jax-v0.3.10).\n\n## jaxlib 0.3.10 (May 3, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jaxlib-v0.3.7...jaxlib-v0.3.10).\n* Changes\n  * [TF commit](https://github.com/tensorflow/tensorflow/commit/207d50d253e11c3a3430a700af478a1d524a779a)\n    fixes an issue in the MHLO canonicalizer that caused constant folding to\n    take a long time or crash for certain programs.\n\n## jax 0.3.9 (May 2, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.8...jax-v0.3.9).\n* Changes\n  * Added support for fully asynchronous checkpointing for GlobalDeviceArray.\n\n## jax 0.3.8 (April 29 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.7...jax-v0.3.8).\n* Changes\n  * {func}`jax.numpy.linalg.svd` on TPUs uses a qdwh-svd solver.\n  * {func}`jax.numpy.linalg.cond` on TPUs now accepts complex input.\n  * {func}`jax.numpy.linalg.pinv` on TPUs now accepts complex input.\n  * {func}`jax.numpy.linalg.matrix_rank` on TPUs now accepts complex input.\n  * {func}`jax.scipy.cluster.vq.vq` has been added.\n  * `jax.experimental.maps.mesh` has been deleted.\n    Please use `jax.experimental.maps.Mesh`. Please see https://docs.jax.dev/en/latest/_autosummary/jax.experimental.maps.Mesh.html#jax.experimental.maps.Mesh\n    for more information.\n  * {func}`jax.scipy.linalg.qr` now returns a length-1 tuple rather than the raw array when\n    `mode='r'`, in order to match the behavior of `scipy.linalg.qr` ({jax-issue}`#10452`)\n  * {func}`jax.numpy.take_along_axis` now takes an optional `mode` parameter\n    that specifies the behavior of out-of-bounds indexing. By default,\n    invalid values (e.g., NaN) will be returned for out-of-bounds indices. In\n    previous versions of JAX, invalid indices were clamped into range. The\n    previous behavior can be restored by passing `mode=\"clip\"`.\n  * {func}`jax.numpy.take` now defaults to `mode=\"fill\"`, which returns\n    invalid values (e.g., NaN) for out-of-bounds indices.\n  * Scatter operations, such as `x.at[...].set(...)`, now have `\"drop\"` semantics.\n    This has no effect on the scatter operation itself, but it means that when\n    differentiated the gradient of a scatter will yield zero cotangents for\n    out-of-bounds indices. Previously out-of-bounds indices were clamped into\n    range for the gradient, which was not mathematically correct.\n  * {func}`jax.numpy.take_along_axis` now raises a `TypeError` if its indices\n    are not of an integer type, matching the behavior of\n    {func}`numpy.take_along_axis`. Previously non-integer indices were silently\n    cast to integers.\n  * {func}`jax.numpy.ravel_multi_index` now raises a `TypeError` if its `dims` argument\n    is not of an integer type, matching the behavior of\n    {func}`numpy.ravel_multi_index`. Previously non-integer `dims` was silently\n    cast to integers.\n  * {func}`jax.numpy.split` now raises a `TypeError` if its `axis` argument\n    is not of an integer type, matching the behavior of\n    {func}`numpy.split`. Previously non-integer `axis` was silently\n    cast to integers.\n  * {func}`jax.numpy.indices` now raises a `TypeError` if its dimensions\n    are not of an integer type, matching the behavior of\n    {func}`numpy.indices`. Previously non-integer dimensions were silently\n    cast to integers.\n  * {func}`jax.numpy.diag` now raises a `TypeError` if its `k` argument\n    is not of an integer type, matching the behavior of\n    {func}`numpy.diag`. Previously non-integer `k` was silently\n    cast to integers.\n  * Added {func}`jax.random.orthogonal`.\n* Deprecations\n  * Many functions and objects available in {mod}`jax.test_util` are now deprecated and will raise a\n    warning on import. This includes `cases_from_list`, `check_close`, `check_eq`, `device_under_test`,\n    `format_shape_dtype_string`, `rand_uniform`, `skip_on_devices`, `with_config`, `xla_bridge`, and\n    `_default_tolerance` ({jax-issue}`#10389`). These, along with previously-deprecated `JaxTestCase`,\n    `JaxTestLoader`, and `BufferDonationTestCase`, will be removed in a future JAX release.\n    Most of these utilities can be replaced by calls to standard python & numpy testing utilities found\n    in e.g.  {mod}`unittest`, {mod}`absl.testing`, {mod}`numpy.testing`, etc. JAX-specific functionality\n    such as device checking can be replaced through the use of public APIs such as {func}`jax.devices`.\n    Many of the deprecated utilities will still exist in {mod}`jax._src.test_util`, but these are not\n    public APIs and as such may be changed or removed without notice in future releases.\n\n## jax 0.3.7 (April 15, 2022)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.3.6...jax-v0.3.7).\n* Changes:\n  * Fixed a performance problem if the indices passed to\n    {func}`jax.numpy.take_along_axis` were broadcasted ({jax-issue}`#10281`).\n  * {func}`jax.scipy.special.expit` and {func}`jax.scipy.special.logit` now\n    require their arguments to be scalars or JAX arrays. They also now promote\n    integer arguments to floating point.\n  * The `DeviceArray.tile()` method is deprecated, because numpy arrays do not have a\n    `tile()` method. As a replacement for this, use {func}`jax.numpy.tile`\n    ({jax-issue}`#10266`).\n\n## jaxlib 0.3.7 (April 15, 2022)\n* Changes:\n  * Linux wheels are now built conforming to the `manylinux2014` standard, instead\n    of `manylinux2010`.\n\n## jax 0.3.6 (April 12, 2022)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.3.5...jax-v0.3.6).\n* Changes:\n  * Upgraded libtpu wheel to a version that fixes a hang when initializing a TPU\n    pod. Fixes [#10218](https://github.com/jax-ml/jax/issues/10218).\n* Deprecations:\n  * {mod}`jax.experimental.loops` is being deprecated. See {jax-issue}`#10278`\n    for an alternative API.\n\n## jax 0.3.5 (April 7, 2022)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.3.4...jax-v0.3.5).\n* Changes:\n  * added {func}`jax.random.loggamma` & improved behavior of {func}`jax.random.beta`\n    and {func}`jax.random.dirichlet` for small parameter values ({jax-issue}`#9906`).\n  * the private `lax_numpy` submodule is no longer exposed in the `jax.numpy` namespace ({jax-issue}`#10029`).\n  * added array creation routines {func}`jax.numpy.frombuffer`, {func}`jax.numpy.fromfunction`,\n    and {func}`jax.numpy.fromstring` ({jax-issue}`#10049`).\n  * `DeviceArray.copy()` now returns a `DeviceArray` rather than a `np.ndarray` ({jax-issue}`#10069`)\n  * added {func}`jax.scipy.linalg.rsf2csf`\n  * `jax.experimental.sharded_jit` has been deprecated and will be removed soon.\n* Deprecations:\n  * {func}`jax.nn.normalize` is being deprecated. Use {func}`jax.nn.standardize` instead ({jax-issue}`#9899`).\n  * {func}`jax.tree_util.tree_multimap` is deprecated. Use {func}`jax.tree_util.tree_map` instead ({jax-issue}`#5746`).\n  * `jax.experimental.sharded_jit` is deprecated. Use `pjit` instead.\n\n## jaxlib 0.3.5 (April 7, 2022)\n* Bug fixes\n  * Fixed a bug where double-precision complex-to-real IRFFTs would mutate their\n    input buffers on GPU ({jax-issue}`#9946`).\n  * Fixed incorrect constant-folding of complex scatters ({jax-issue}`#10159`)\n\n## jax 0.3.4 (March 18, 2022)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.3.3...jax-v0.3.4).\n\n\n## jax 0.3.3 (March 17, 2022)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.3.2...jax-v0.3.3).\n\n\n## jax 0.3.2 (March 16, 2022)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.3.1...jax-v0.3.2).\n* Changes:\n  * The functions `jax.ops.index_update`, `jax.ops.index_add`, which were\n    deprecated in 0.2.22, have been removed. Please use\n    [the `.at` property on JAX arrays](https://docs.jax.dev/en/latest/_autosummary/jax.numpy.ndarray.at.html)\n    instead, e.g., `x.at[idx].set(y)`.\n  * Moved `jax.experimental.ann.approx_*_k` into `jax.lax`. These functions are\n    optimized alternatives to `jax.lax.top_k`.\n  * {func}`jax.numpy.broadcast_arrays` and {func}`jax.numpy.broadcast_to` now require scalar\n    or array-like inputs, and will fail if they are passed lists (part of {jax-issue}`#7737`).\n  * The standard jax[tpu] install can now be used with Cloud TPU v4 VMs.\n  * `pjit` now works on CPU (in addition to previous TPU and GPU support).\n\n\n## jaxlib 0.3.2 (March 16, 2022)\n* Changes\n  * ``XlaComputation.as_hlo_text()`` now supports printing large constants by\n    passing boolean flag ``print_large_constants=True``.\n* Deprecations:\n  * The ``.block_host_until_ready()`` method on JAX arrays has been deprecated.\n    Use ``.block_until_ready()`` instead.\n\n## jax 0.3.1 (Feb 18, 2022)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.3.0...jax-v0.3.1).\n\n* Changes:\n  * `jax.test_util.JaxTestCase` and `jax.test_util.JaxTestLoader` are now deprecated.\n    The suggested replacement is to use `parametrized.TestCase` directly. For tests that\n    rely on custom asserts such as `JaxTestCase.assertAllClose()`, the suggested replacement\n    is to use standard numpy testing utilities such as {func}`numpy.testing.assert_allclose()`,\n    which work directly with JAX arrays ({jax-issue}`#9620`).\n  * `jax.test_util.JaxTestCase` now sets `jax_numpy_rank_promotion='raise'` by default\n    ({jax-issue}`#9562`). To recover the previous behavior, use the new\n    `jax.test_util.with_config` decorator:\n    ```python\n    @jtu.with_config(jax_numpy_rank_promotion='allow')\n    class MyTestCase(jtu.JaxTestCase):\n      ...\n    ```\n  * Added {func}`jax.scipy.linalg.schur`, {func}`jax.scipy.linalg.sqrtm`,\n    {func}`jax.scipy.signal.csd`, {func}`jax.scipy.signal.stft`,\n    {func}`jax.scipy.signal.welch`.\n\n\n## jax 0.3.0 (Feb 10, 2022)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.2.28...jax-v0.3.0).\n\n* Changes\n  * jax version has been bumped to 0.3.0. Please see the [design doc](https://docs.jax.dev/en/latest/design_notes/jax_versioning.html)\n    for the explanation.\n\n## jaxlib 0.3.0 (Feb 10, 2022)\n* Changes\n  * Bazel 5.0.0 is now required to build jaxlib.\n  * jaxlib version has been bumped to 0.3.0. Please see the [design doc](https://docs.jax.dev/en/latest/design_notes/jax_versioning.html)\n    for the explanation.\n\n## jax 0.2.28 (Feb 1, 2022)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.2.27...jax-v0.2.28).\n  * `jax.jit(f).lower(...).compiler_ir()` now defaults to the MHLO dialect if no\n    `dialect=` is passed.\n  * The `jax.jit(f).lower(...).compiler_ir(dialect='mhlo')` now returns an MLIR\n    `ir.Module` object instead of its string representation.\n\n## jaxlib 0.1.76 (Jan 27, 2022)\n\n* New features\n  * Includes precompiled SASS for NVidia compute capability 8.0 GPUS\n    (e.g. A100). Removes precompiled SASS for compute capability 6.1 so as not\n    to increase the number of compute capabilities: GPUs with compute capability\n    6.1 can use the 6.0 SASS.\n  * With jaxlib 0.1.76, JAX uses the MHLO MLIR dialect as its primary target compiler IR\n    by default.\n* Breaking changes\n  * Support for NumPy 1.18 has been dropped, per the\n    [deprecation policy](https://docs.jax.dev/en/latest/deprecation.html).\n    Please upgrade to a supported NumPy version.\n* Bug fixes\n  * Fixed a bug where apparently identical pytreedef objects constructed by different routes\n    do not compare as equal (#9066).\n  * The JAX jit cache requires two static arguments to have identical types for a cache hit (#9311).\n\n## jax 0.2.27 (Jan 18 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.26...jax-v0.2.27).\n\n* Breaking changes:\n  * Support for NumPy 1.18 has been dropped, per the\n    [deprecation policy](https://docs.jax.dev/en/latest/deprecation.html).\n    Please upgrade to a supported NumPy version.\n  * The host_callback primitives have been simplified to drop the\n    special autodiff handling for hcb.id_tap and id_print.\n    From now on, only the primals are tapped. The old behavior can be\n    obtained (for a limited time) by setting the ``JAX_HOST_CALLBACK_AD_TRANSFORMS``\n    environment variable, or the ```--jax_host_callback_ad_transforms``` flag.\n    Additionally, added documentation for how to implement the old behavior\n    using JAX custom AD APIs ({jax-issue}`#8678`).\n  * Sorting now matches the behavior of NumPy for ``0.0`` and ``NaN`` regardless of the\n    bit representation. In particular, ``0.0`` and ``-0.0`` are now treated as equivalent,\n    where previously ``-0.0`` was treated as less than ``0.0``. Additionally all ``NaN``\n    representations are now treated as equivalent and sorted to the end of the array.\n    Previously negative ``NaN`` values were sorted to the front of the array, and ``NaN``\n    values with different internal bit representations were not treated as equivalent, and\n    were sorted according to those bit patterns ({jax-issue}`#9178`).\n  * {func}`jax.numpy.unique` now treats ``NaN`` values in the same way as `np.unique` in\n    NumPy versions 1.21 and newer: at most one ``NaN`` value will appear in the uniquified\n    output ({jax-issue}`9184`).\n\n* Bug fixes:\n  * host_callback now supports ad_checkpoint.checkpoint ({jax-issue}`#8907`).\n\n* New features:\n  * add `jax.block_until_ready` ({jax-issue}`#8941)\n  * Added a new debugging flag/environment variable `JAX_DUMP_IR_TO=/path`.\n    If set, JAX dumps the MHLO/HLO IR it generates for each computation to a\n    file under the given path.\n  * Added `jax.ensure_compile_time_eval` to the public api ({jax-issue}`#7987`).\n  * jax2tf now supports a flag jax2tf_associative_scan_reductions to change\n    the lowering for associative reductions, e.g., jnp.cumsum, to behave\n    like JAX on CPU and GPU (to use an associative scan). See the jax2tf README\n    for more details ({jax-issue}`#9189`).\n\n\n## jaxlib 0.1.75 (Dec 8, 2021)\n* New features:\n  * Support for python 3.10.\n\n## jax 0.2.26 (Dec 8, 2021)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.2.25...jax-v0.2.26).\n\n* Bug fixes:\n  * Out-of-bounds indices to `jax.ops.segment_sum` will now be handled with\n    `FILL_OR_DROP` semantics, as documented. This primarily affects the\n    reverse-mode derivative, where gradients corresponding to out-of-bounds\n    indices will now be returned as 0. (#8634).\n  * jax2tf will force the converted code to use XLA for the code fragments\n    under jax.jit, e.g., most jax.numpy functions ({jax-issue}`#7839`).\n\n## jaxlib 0.1.74 (Nov 17, 2021)\n* Enabled peer-to-peer copies between GPUs. Previously, GPU copies were bounced via\n  the host, which is usually slower.\n* Added experimental MLIR Python bindings for use by JAX.\n\n## jax 0.2.25 (Nov 10, 2021)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.2.24...jax-v0.2.25).\n\n* New features:\n  * (Experimental) `jax.distributed.initialize` exposes multi-host GPU backend.\n  * `jax.random.permutation` supports new `independent` keyword argument\n    ({jax-issue}`#8430`)\n* Breaking changes\n  * Moved `jax.experimental.stax` to `jax.example_libraries.stax`\n  * Moved `jax.experimental.optimizers` to `jax.example_libraries.optimizers`\n* New features:\n  * Added `jax.lax.linalg.qdwh`.\n\n## jax 0.2.24 (Oct 19, 2021)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.2.22...jax-v0.2.24).\n\n* New features:\n  * `jax.random.choice` and `jax.random.permutation` now support\n    multidimensional arrays and an optional `axis` argument ({jax-issue}`#8158`)\n* Breaking changes:\n  * `jax.numpy.take` and `jax.numpy.take_along_axis` now require array-like inputs\n    (see {jax-issue}`#7737`)\n\n## jaxlib 0.1.73 (Oct 18, 2021)\n\n* Multiple cuDNN versions are now supported for jaxlib GPU `cuda11` wheels.\n  * cuDNN 8.2 or newer. We recommend using the cuDNN 8.2 wheel if your cuDNN\n    installation is new enough, since it supports additional functionality.\n  * cuDNN 8.0.5 or newer.\n\n* Breaking changes:\n  * The install commands for GPU jaxlib are as follows:\n\n    ```bash\n    pip install --upgrade pip\n\n    # Installs the wheel compatible with CUDA 11 and cuDNN 8.2 or newer.\n    pip install --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_releases.html\n\n    # Installs the wheel compatible with Cuda 11 and cudnn 8.2 or newer.\n    pip install jax[cuda11_cudnn82] -f https://storage.googleapis.com/jax-releases/jax_releases.html\n\n    # Installs the wheel compatible with Cuda 11 and cudnn 8.0.5 or newer.\n    pip install jax[cuda11_cudnn805] -f https://storage.googleapis.com/jax-releases/jax_releases.html\n    ```\n\n## jax 0.2.22 (Oct 12, 2021)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.2.21...jax-v0.2.22).\n* Breaking Changes\n  * Static arguments to `jax.pmap` must now be hashable.\n\n    Unhashable static arguments have long been disallowed on `jax.jit`, but they\n    were still permitted on `jax.pmap`; `jax.pmap` compared unhashable static\n    arguments using object identity.\n\n    This behavior is a footgun, since comparing arguments using\n    object identity leads to recompilation each time the object identity\n    changes. Instead, we now ban unhashable arguments: if a user of `jax.pmap`\n    wants to compare static arguments by object identity, they can define\n    `__hash__` and `__eq__` methods on their objects that do that, or wrap their\n    objects in an object that has those operations with object identity\n    semantics. Another option is to use `functools.partial` to encapsulate the\n    unhashable static arguments into the function object.\n  * `jax.util.partial` was an accidental export that has now been removed. Use\n    `functools.partial` from the Python standard library instead.\n* Deprecations\n  * The functions `jax.ops.index_update`, `jax.ops.index_add` etc. are\n    deprecated and will be removed in a future JAX release. Please use\n    [the `.at` property on JAX arrays](https://docs.jax.dev/en/latest/_autosummary/jax.numpy.ndarray.at.html)\n    instead, e.g., `x.at[idx].set(y)`. For now, these functions produce a\n    `DeprecationWarning`.\n* New features:\n  * An optimized C++ code-path improving the dispatch time for `pmap` is now the\n    default when using jaxlib 0.1.72 or newer. The feature can be disabled using\n    the `--experimental_cpp_pmap` flag (or `JAX_CPP_PMAP` environment variable).\n  * `jax.numpy.unique` now supports an optional `fill_value` argument ({jax-issue}`#8121`)\n\n## jaxlib 0.1.72 (Oct 12, 2021)\n  * Breaking changes:\n    * Support for CUDA 10.2 and CUDA 10.1 has been dropped. Jaxlib now supports\n      CUDA 11.1+.\n  * Bug fixes:\n    * Fixes https://github.com/jax-ml/jax/issues/7461, which caused wrong\n      outputs on all platforms due to incorrect buffer aliasing inside the XLA\n      compiler.\n\n## jax 0.2.21 (Sept 23, 2021)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.2.20...jax-v0.2.21).\n* Breaking Changes\n  * `jax.api` has been removed. Functions that were available as `jax.api.*`\n    were aliases for functions in `jax.*`; please use the functions in\n    `jax.*` instead.\n  * `jax.partial`, and `jax.lax.partial` were accidental exports that have now\n    been removed. Use `functools.partial` from the Python standard library\n    instead.\n  * Boolean scalar indices now raise a `TypeError`; previously this silently\n    returned wrong results ({jax-issue}`#7925`).\n  * Many more `jax.numpy` functions now require array-like inputs, and will error\n    if passed a list ({jax-issue}`#7747` {jax-issue}`#7802` {jax-issue}`#7907`).\n    See {jax-issue}`#7737` for a discussion of the rationale behind this change.\n  * When inside a transformation such as `jax.jit`, `jax.numpy.array` always\n    stages the array it produces into the traced computation. Previously\n    `jax.numpy.array` would sometimes produce a on-device array, even under\n    a `jax.jit` decorator. This change may break code that used JAX arrays to\n    perform shape or index computations that must be known statically; the\n    workaround is to perform such computations using classic NumPy arrays\n    instead.\n  * `jnp.ndarray` is now a true base-class for JAX arrays. In particular, this\n    means that for a standard numpy array `x`, `isinstance(x, jnp.ndarray)` will\n    now return `False` ({jax-issue}`7927`).\n* New features:\n  * Added {func}`jax.numpy.insert` implementation ({jax-issue}`#7936`).\n\n## jax 0.2.20 (Sept 2, 2021)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.2.19...jax-v0.2.20).\n* Breaking Changes\n  * `jnp.poly*` functions now require array-like inputs ({jax-issue}`#7732`)\n  * `jnp.unique` and other set-like operations now require array-like inputs\n    ({jax-issue}`#7662`)\n\n## jaxlib 0.1.71 (Sep 1, 2021)\n* Breaking changes:\n  * Support for CUDA 11.0 and CUDA 10.1 has been dropped. Jaxlib now supports\n    CUDA 10.2 and CUDA 11.1+.\n\n## jax 0.2.19 (Aug 12, 2021)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.2.18...jax-v0.2.19).\n* Breaking changes:\n  * Support for NumPy 1.17 has been dropped, per the\n    [deprecation policy](https://docs.jax.dev/en/latest/deprecation.html).\n    Please upgrade to a supported NumPy version.\n  * The `jit` decorator has been added around the implementation of a number of\n    operators on JAX arrays. This speeds up dispatch times for common\n    operators such as `+`.\n\n    This change should largely be transparent to most users. However, there is\n    one known behavioral change, which is that large integer constants may now\n    produce an error when passed directly to a JAX operator\n    (e.g., `x + 2**40`). The workaround is to cast the constant to an\n    explicit type (e.g., `np.float64(2**40)`).\n* New features:\n  * Improved the support for shape polymorphism in jax2tf for operations that\n    need to use a dimension size in array computation, e.g., `jnp.mean`.\n    ({jax-issue}`#7317`)\n* Bug fixes:\n  * Some leaked trace errors from the previous release ({jax-issue}`#7613`)\n\n## jaxlib 0.1.70 (Aug 9, 2021)\n* Breaking changes:\n  * Support for Python 3.6 has been dropped, per the\n    [deprecation policy](https://docs.jax.dev/en/latest/deprecation.html).\n    Please upgrade to a supported Python version.\n  * Support for NumPy 1.17 has been dropped, per the\n    [deprecation policy](https://docs.jax.dev/en/latest/deprecation.html).\n    Please upgrade to a supported NumPy version.\n\n  * The host_callback mechanism now uses one thread per local device for\n    making the calls to the Python callbacks. Previously there was a single\n    thread for all devices. This means that the callbacks may now be called\n    interleaved. The callbacks corresponding to one device will still be\n    called in sequence.\n\n## jax 0.2.18 (July 21 2021)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.17...jax-v0.2.18).\n\n* Breaking changes:\n  * Support for Python 3.6 has been dropped, per the\n    [deprecation policy](https://docs.jax.dev/en/latest/deprecation.html).\n    Please upgrade to a supported Python version.\n  * The minimum jaxlib version is now 0.1.69.\n  * The `backend` argument to {py:func}`jax.dlpack.from_dlpack` has been\n    removed.\n\n* New features:\n  * Added a polar decomposition ({py:func}`jax.scipy.linalg.polar`).\n\n* Bug fixes:\n  * Tightened the checks for lax.argmin and lax.argmax to ensure they are\n    not used with an invalid `axis` value, or with an empty reduction dimension.\n    ({jax-issue}`#7196`)\n\n\n## jaxlib 0.1.69 (July 9 2021)\n* Fix bugs in TFRT CPU backend that results in incorrect results.\n\n## jax 0.2.17 (July 9 2021)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.16...jax-v0.2.17).\n* Bug fixes:\n  * Default to the older \"stream_executor\" CPU runtime for jaxlib <= 0.1.68\n    to work around #7229, which caused wrong outputs on CPU due to a concurrency\n    problem.\n* New features:\n  * New SciPy function {py:func}`jax.scipy.special.sph_harm`.\n  * Reverse-mode autodiff functions ({func}`jax.grad`,\n    {func}`jax.value_and_grad`, {func}`jax.vjp`, and\n    {func}`jax.linear_transpose`) support a parameter that indicates which named\n    axes should be summed over in the backward pass if they were broadcasted\n    over in the forward pass. This enables use of these APIs in a\n    non-per-example way inside maps (initially only\n    {func}`jax.experimental.maps.xmap`) ({jax-issue}`#6950`).\n\n\n## jax 0.2.16 (June 23 2021)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.15...jax-v0.2.16).\n\n## jax 0.2.15 (June 23 2021)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.14...jax-v0.2.15).\n* New features:\n  * [#7042](https://github.com/jax-ml/jax/pull/7042) Turned on TFRT CPU backend\n    with significant dispatch performance improvements on CPU.\n  * The {func}`jax2tf.convert` supports inequalities and min/max for booleans\n    ({jax-issue}`#6956`).\n  * New SciPy function {py:func}`jax.scipy.special.lpmn_values`.\n\n* Breaking changes:\n  * Support for NumPy 1.16 has been dropped, per the\n    [deprecation policy](https://docs.jax.dev/en/latest/deprecation.html).\n\n* Bug fixes:\n  * Fixed bug that prevented round-tripping from JAX to TF and back:\n    `jax2tf.call_tf(jax2tf.convert)` ({jax-issue}`#6947`).\n\n## jaxlib 0.1.68 (June 23 2021)\n* Bug fixes:\n  * Fixed bug in TFRT CPU backend that gets nans when transfer TPU buffer to\n    CPU.\n\n## jax 0.2.14 (June 10 2021)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.13...jax-v0.2.14).\n* New features:\n  * The {func}`jax2tf.convert` now has support for `pjit` and `sharded_jit`.\n  * A new configuration option JAX_TRACEBACK_FILTERING controls how JAX filters\n    tracebacks.\n  * A new traceback filtering mode using `__tracebackhide__` is now enabled by\n    default in sufficiently recent versions of IPython.\n  * The {func}`jax2tf.convert` supports shape polymorphism even when the\n    unknown dimensions are used in arithmetic operations, e.g., `jnp.reshape(-1)`\n    ({jax-issue}`#6827`).\n  * The {func}`jax2tf.convert` generates custom attributes with location information\n   in TF ops. The code that XLA generates after jax2tf\n   has the same location information as JAX/XLA.\n  * New SciPy function {py:func}`jax.scipy.special.lpmn`.\n\n* Bug fixes:\n  * The {func}`jax2tf.convert` now ensures that it uses the same typing rules\n    for Python scalars and for choosing 32-bit vs. 64-bit computations\n    as JAX ({jax-issue}`#6883`).\n  * The {func}`jax2tf.convert` now scopes the `enable_xla` conversion parameter\n    properly to apply only during the just-in-time conversion\n    ({jax-issue}`#6720`).\n  * The {func}`jax2tf.convert` now converts `lax.dot_general` using the\n    `XlaDot` TensorFlow op, for better fidelity w.r.t. JAX numerical precision\n    ({jax-issue}`#6717`).\n  * The {func}`jax2tf.convert` now has support for inequality comparisons and\n    min/max for complex numbers ({jax-issue}`#6892`).\n\n## jaxlib 0.1.67 (May 17 2021)\n\n## jaxlib 0.1.66 (May 11 2021)\n\n* New features:\n  * CUDA 11.1 wheels are now supported on all CUDA 11 versions 11.1 or higher.\n\n    NVidia now promises compatibility between CUDA minor releases starting with\n    CUDA 11.1. This means that JAX can release a single CUDA 11.1 wheel that\n    is compatible with CUDA 11.2 and 11.3.\n\n    There is no longer a separate jaxlib release for CUDA 11.2 (or higher); use\n    the CUDA 11.1 wheel for those versions (cuda111).\n  * Jaxlib now bundles `libdevice.10.bc` in CUDA wheels. There should be no need\n    to point JAX to a CUDA installation to find this file.\n  * Added automatic support for static keyword arguments to the {func}`jit`\n    implementation.\n  * Added support for pretransformation exception traces.\n  * Initial support for pruning unused arguments from {func}`jit` -transformed\n    computations.\n    Pruning is still a work in progress.\n  * Improved the string representation of {class}`PyTreeDef` objects.\n  * Added support for XLA's variadic ReduceWindow.\n* Bug fixes:\n  * Fixed a bug in the remote cloud TPU support when large numbers of arguments\n    are passed to a computation.\n  * Fix a bug that meant that JAX garbage collection was not triggered by\n    {func}`jit` transformed functions.\n\n## jax 0.2.13 (May 3 2021)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.12...jax-v0.2.13).\n* New features:\n  * When combined with jaxlib 0.1.66, {func}`jax.jit` now supports static\n    keyword arguments. A new `static_argnames` option has been added to specify\n    keyword arguments as static.\n  * {func}`jax.nonzero` has a new optional `size` argument that allows it to\n    be used within `jit` ({jax-issue}`#6501`)\n  * {func}`jax.numpy.unique` now supports the `axis` argument ({jax-issue}`#6532`).\n  * {func}`jax.experimental.host_callback.call` now supports `pjit.pjit` ({jax-issue}`#6569`).\n  * Added {func}`jax.scipy.linalg.eigh_tridiagonal` that computes the\n    eigenvalues of a tridiagonal matrix. Only eigenvalues are supported at\n    present.\n  * The order of the filtered and unfiltered stack traces in exceptions has been\n    changed. The traceback attached to an exception thrown from JAX-transformed\n    code is now filtered, with an `UnfilteredStackTrace` exception\n    containing the original trace as the `__cause__` of the filtered exception.\n    Filtered stack traces now also work with Python 3.6.\n  * If an exception is thrown by code that has been transformed by reverse-mode\n    automatic differentiation, JAX now attempts to attach as a `__cause__` of\n    the exception a `JaxStackTraceBeforeTransformation` object that contains the\n    stack trace that created the original operation in the forward pass.\n    Requires jaxlib 0.1.66.\n\n* Breaking changes:\n  * The following function names have changed. There are still aliases, so this\n    should not break existing code, but the aliases will eventually be removed\n    so please change your code.\n    * `host_id` --> {func}`~jax.process_index`\n    * `host_count` --> {func}`~jax.process_count`\n    * `host_ids` --> `range(jax.process_count())`\n  * Similarly, the argument to {func}`~jax.local_devices` has been renamed from\n    `host_id` to `process_index`.\n  * Arguments to {func}`jax.jit` other than the function are now marked as\n    keyword-only. This change is to prevent accidental breakage when arguments\n    are added to `jit`.\n* Bug fixes:\n  * The {func}`jax2tf.convert` now works in presence of gradients for functions\n    with integer inputs ({jax-issue}`#6360`).\n  * Fixed assertion failure in {func}`jax2tf.call_tf` when used with captured\n    `tf.Variable` ({jax-issue}`#6572`).\n\n## jaxlib 0.1.65 (April 7 2021)\n\n## jax 0.2.12 (April 1 2021)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.11...v0.2.12).\n* New features\n  * New profiling APIs: {func}`jax.profiler.start_trace`,\n    {func}`jax.profiler.stop_trace`, and {func}`jax.profiler.trace`\n  * {func}`jax.lax.reduce` is now differentiable.\n* Breaking changes:\n  * The minimum jaxlib version is now 0.1.64.\n  * Some profiler APIs names have been changed. There are still aliases, so this\n    should not break existing code, but the aliases will eventually be removed\n    so please change your code.\n    * `TraceContext` --> {func}`~jax.profiler.TraceAnnotation`\n    * `StepTraceContext` --> {func}`~jax.profiler.StepTraceAnnotation`\n    * `trace_function` --> {func}`~jax.profiler.annotate_function`\n  * Omnistaging can no longer be disabled. See [omnistaging](https://github.com/jax-ml/jax/blob/main/docs/design_notes/omnistaging.md)\n    for more information.\n  * Python integers larger than the maximum `int64` value will now lead to an overflow\n    in all cases, rather than being silently converted to `uint64` in some cases ({jax-issue}`#6047`).\n  * Outside X64 mode, Python integers outside the range representable by `int32` will now lead to an\n    `OverflowError` rather than having their value silently truncated.\n* Bug fixes:\n  * `host_callback` now supports empty arrays in arguments and results ({jax-issue}`#6262`).\n  * {func}`jax.random.randint` clips rather than wraps of out-of-bounds limits, and can now generate\n    integers in the full range of the specified dtype ({jax-issue}`#5868`)\n\n## jax 0.2.11 (March 23 2021)\n\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.2.10...jax-v0.2.11).\n* New features:\n  * [#6112](https://github.com/jax-ml/jax/pull/6112) added context managers:\n    `jax.enable_checks`, `jax.check_tracer_leaks`, `jax.debug_nans`,\n    `jax.debug_infs`, `jax.log_compiles`.\n  * [#6085](https://github.com/jax-ml/jax/pull/6085) added `jnp.delete`\n\n* Bug fixes:\n  * [#6136](https://github.com/jax-ml/jax/pull/6136) generalized\n    `jax.flatten_util.ravel_pytree` to handle integer dtypes.\n  * [#6129](https://github.com/jax-ml/jax/issues/6129) fixed a bug with handling\n    some constants like `enum.IntEnums`\n  * [#6145](https://github.com/jax-ml/jax/pull/6145) fixed batching issues with\n    incomplete beta functions\n  * [#6014](https://github.com/jax-ml/jax/pull/6014) fixed H2D transfers during\n    tracing\n  * [#6165](https://github.com/jax-ml/jax/pull/6165) avoids OverflowErrors when\n    converting some large Python integers to floats\n* Breaking changes:\n  * The minimum jaxlib version is now 0.1.62.\n\n\n## jaxlib 0.1.64 (March 18 2021)\n\n## jaxlib 0.1.63 (March 17 2021)\n\n## jax 0.2.10 (March 5 2021)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.9...jax-v0.2.10).\n* New features:\n  * {func}`jax.scipy.stats.chi2` is now available as a distribution with logpdf and pdf methods.\n  * {func}`jax.scipy.stats.betabinom` is now available as a distribution with logpmf and pmf methods.\n  * Added {func}`jax.experimental.jax2tf.call_tf` to call TensorFlow functions\n    from JAX ({jax-issue}`#5627`)\n    and [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax)).\n  * Extended the batching rule for `lax.pad` to support batching of the padding values.\n* Bug fixes:\n  * {func}`jax.numpy.take` properly handles negative indices ({jax-issue}`#5768`)\n* Breaking changes:\n  * JAX's promotion rules were adjusted to make promotion more consistent and\n    invariant to JIT. In particular, binary operations can now result in weakly-typed\n    values when appropriate. The main user-visible effect of the change is that\n    some operations result in outputs of different precision than before; for\n    example the expression `jnp.bfloat16(1) + 0.1 * jnp.arange(10)`\n    previously returned a `float64` array, and now returns a `bfloat16` array.\n    JAX's type promotion behavior is described at {ref}`type-promotion`.\n  * {func}`jax.numpy.linspace` now computes the floor of integer values, i.e.,\n    rounding towards -inf rather than 0. This change was made to match NumPy\n    1.20.0.\n  * {func}`jax.numpy.i0` no longer accepts complex numbers. Previously the\n    function computed the absolute value of complex arguments. This change was\n    made to match the semantics of NumPy 1.20.0.\n  * Several {mod}`jax.numpy` functions no longer accept tuples or lists in place\n    of array arguments: {func}`jax.numpy.pad`, :func`jax.numpy.ravel`,\n    {func}`jax.numpy.repeat`, {func}`jax.numpy.reshape`.\n    In general, {mod}`jax.numpy` functions should be used with scalars or array arguments.\n\n## jaxlib 0.1.62 (March 9 2021)\n\n* New features:\n  * jaxlib wheels are now built to require AVX instructions on x86-64 machines\n    by default. If you want to use JAX on a machine that doesn't support AVX,\n    you can build a jaxlib from source using the `--target_cpu_features` flag\n    to `build.py`. `--target_cpu_features` also replaces\n    `--enable_march_native`.\n\n## jaxlib 0.1.61 (February 12 2021)\n\n## jaxlib 0.1.60 (February 3 2021)\n\n* Bug fixes:\n  * Fixed a memory leak when converting CPU DeviceArrays to NumPy arrays. The\n    memory leak was present in jaxlib releases 0.1.58 and 0.1.59.\n  * `bool`, `int8`, and `uint8` are now considered safe to cast to\n    `bfloat16` NumPy extension type.\n\n## jax 0.2.9 (January 26 2021)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.8...jax-v0.2.9).\n* New features:\n  * Extend the {mod}`jax.experimental.loops` module with support for pytrees. Improved\n    error checking and error messages.\n  * Add {func}`jax.experimental.enable_x64` and {func}`jax.experimental.disable_x64`.\n    These are context managers which allow X64 mode to be temporarily enabled/disabled\n    within a session.\n* Breaking changes:\n  * {func}`jax.ops.segment_sum` now drops segment IDs that are out of range rather\n    than wrapping them into the segment ID space. This was done for performance\n    reasons.\n\n## jaxlib 0.1.59 (January 15 2021)\n\n## jax 0.2.8 (January 12 2021)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.7...jax-v0.2.8).\n* New features:\n  * Add {func}`jax.closure_convert` for use with higher-order custom\n    derivative functions. ({jax-issue}`#5244`)\n  * Add {func}`jax.experimental.host_callback.call` to call a custom Python\n    function on the host and return a result to the device computation.\n    ({jax-issue}`#5243`)\n* Bug fixes:\n  * `jax.numpy.arccosh` now returns the same branch as `numpy.arccosh` for\n    complex inputs ({jax-issue}`#5156`)\n  * `host_callback.id_tap` now works for `jax.pmap` also. There is an\n    optional parameter for `id_tap` and `id_print` to request that the\n    device from which the value is tapped be passed as a keyword argument\n    to the tap function ({jax-issue}`#5182`).\n* Breaking changes:\n  * `jax.numpy.pad` now takes keyword arguments. Positional argument `constant_values`\n    has been removed. In addition, passing unsupported keyword arguments raises an error.\n  * Changes for {func}`jax.experimental.host_callback.id_tap` ({jax-issue}`#5243`):\n    * Removed support for `kwargs` for {func}`jax.experimental.host_callback.id_tap`.\n      (This support has been deprecated for a few months.)\n    * Changed the printing of tuples for {func}`jax.experimental.host_callback.id_print`\n      to use '(' instead of '['.\n    * Changed the {func}`jax.experimental.host_callback.id_print` in presence of JVP\n      to print a pair of primal and tangent. Previously, there were two separate\n      print operations for the primals and the tangent.\n    * `host_callback.outfeed_receiver` has been removed (it is not necessary,\n      and was deprecated a few months ago).\n* New features:\n  * New flag for debugging `inf`, analogous to that for `NaN` ({jax-issue}`#5224`).\n\n## jax 0.2.7 (Dec 4 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.6...jax-v0.2.7).\n* New features:\n  * Add `jax.device_put_replicated`\n  * Add multi-host support to `jax.experimental.sharded_jit`\n  * Add support for differentiating eigenvalues computed by `jax.numpy.linalg.eig`\n  * Add support for building on Windows platforms\n  * Add support for general in_axes and out_axes in `jax.pmap`\n  * Add complex support for `jax.numpy.linalg.slogdet`\n* Bug fixes:\n  * Fix higher-than-second order derivatives of `jax.numpy.sinc` at zero\n  * Fix some hard-to-hit bugs around symbolic zeros in transpose rules\n* Breaking changes:\n  * `jax.experimental.optix` has been deleted, in favor of the standalone\n    `optax` Python package.\n  * indexing of JAX arrays with non-tuple sequences now raises a `TypeError`. This type of indexing\n    has been deprecated in Numpy since v1.16, and in JAX since v0.2.4.\n    See {jax-issue}`#4564`.\n\n## jax 0.2.6 (Nov 18 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.5...jax-v0.2.6).\n* New Features:\n  * Add support for shape-polymorphic tracing for the jax.experimental.jax2tf converter.\n    See [README.md](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md).\n* Breaking change cleanup\n\n  * Raise an error on non-hashable static arguments for jax.jit and\n    xla_computation.  See [cb48f42](https://github.com/jax-ml/jax/commit/cb48f42).\n  * Improve consistency of type promotion behavior ({jax-issue}`#4744`):\n    * Adding a complex Python scalar to a JAX floating point number respects the precision of\n      the JAX float. For example, `jnp.float32(1) + 1j` now returns `complex64`, where previously\n      it returned `complex128`.\n    * Results of type promotion with 3 or more terms involving uint64, a signed int, and a third type\n      are now independent of the order of arguments. For example:\n      `jnp.result_type(jnp.uint64, jnp.int64, jnp.float16)` and\n      `jnp.result_type(jnp.float16, jnp.uint64, jnp.int64)` both return `float16`, where previously\n      the first returned `float64` and the second returned `float16`.\n  * The contents of the (undocumented) `jax.lax_linalg` linear algebra module\n    are now exposed publicly as `jax.lax.linalg`.\n  * `jax.random.PRNGKey` now produces the same results in and out of JIT compilation\n    ({jax-issue}`#4877`).\n    This required changing the result for a given seed in a few particular cases:\n    * With `jax_enable_x64=False`, negative seeds passed as Python integers now return a different result\n      outside JIT mode. For example, `jax.random.PRNGKey(-1)` previously returned\n      `[4294967295, 4294967295]`, and now returns `[0, 4294967295]`. This matches the behavior in JIT.\n    * Seeds outside the range representable by `int64` outside JIT now result in an `OverflowError`\n      rather than a `TypeError`. This matches the behavior in JIT.\n\n    To recover the keys returned previously for negative integers with `jax_enable_x64=False`\n    outside JIT, you can use:\n\n    ```\n    key = random.PRNGKey(-1).at[0].set(0xFFFFFFFF)\n    ```\n  * DeviceArray now raises `RuntimeError` instead of `ValueError` when trying\n    to access its value while it has been deleted.\n\n## jaxlib 0.1.58 (January 12ish 2021)\n\n* Fixed a bug that meant JAX sometimes return platform-specific types (e.g.,\n  `np.cint`) instead of standard types (e.g., `np.int32`). (#4903)\n* Fixed a crash when constant-folding certain int16 operations. (#4971)\n* Added an `is_leaf` predicate to {func}`pytree.flatten`.\n\n## jaxlib 0.1.57 (November 12 2020)\n\n* Fixed manylinux2010 compliance issues in GPU wheels.\n* Switched the CPU FFT implementation from Eigen to PocketFFT.\n* Fixed a bug where the hash of bfloat16 values was not correctly initialized\n  and could change (#4651).\n* Add support for retaining ownership when passing arrays to DLPack (#4636).\n* Fixed a bug for batched triangular solves with sizes greater than 128 but not\n  a multiple of 128.\n* Fixed a bug when performing concurrent FFTs on multiple GPUs (#3518).\n* Fixed a bug in profiler where tools are missing (#4427).\n* Dropped support for CUDA 10.0.\n\n## jax 0.2.5 (October 27 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.4...jax-v0.2.5).\n* Improvements:\n  * Ensure that `check_jaxpr` does not perform FLOPS.  See {jax-issue}`#4650`.\n  * Expanded the set of JAX primitives converted by jax2tf.\n    See [primitives_with_limited_support.md](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/primitives_with_limited_support.md).\n\n## jax 0.2.4 (October 19 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.3...jax-v0.2.4).\n* Improvements:\n  * Add support for `remat` to jax.experimental.host_callback.  See {jax-issue}`#4608`.\n* Deprecations\n\n  * Indexing with non-tuple sequences is now deprecated, following a similar deprecation in Numpy.\n    In a future release, this will result in a TypeError. See {jax-issue}`#4564`.\n\n## jaxlib 0.1.56 (October 14, 2020)\n\n## jax 0.2.3 (October 14 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.2...jax-v0.2.3).\n* The reason for another release so soon is we need to temporarily roll back a\n  new jit fastpath while we look into a performance degradation\n\n## jax 0.2.2 (October 13 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.1...jax-v0.2.2).\n\n## jax 0.2.1 (October 6 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.0...jax-v0.2.1).\n* Improvements:\n  * As a benefit of omnistaging, the host_callback functions are executed (in program\n    order) even if the result of the {py:func}`jax.experimental.host_callback.id_print`/\n    {py:func}`jax.experimental.host_callback.id_tap` is not used in the computation.\n\n## jax (0.2.0) (September 23 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.77...jax-v0.2.0).\n* Improvements:\n  * Omnistaging on by default. See {jax-issue}`#3370` and\n    [omnistaging](https://github.com/jax-ml/jax/blob/main/docs/design_notes/omnistaging.md)\n\n## jax (0.1.77) (September 15 2020)\n\n* Breaking changes:\n  * New simplified interface for {py:func}`jax.experimental.host_callback.id_tap` (#4101)\n\n## jaxlib 0.1.55 (September 8, 2020)\n\n* Update XLA:\n  * Fix bug in DLPackManagedTensorToBuffer (#4196)\n\n## jax 0.1.76 (September 8, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.75...jax-v0.1.76).\n\n## jax 0.1.75 (July 30, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.74...jax-v0.1.75).\n* Bug Fixes:\n  * make jnp.abs() work for unsigned inputs (#3914)\n* Improvements:\n  * \"Omnistaging\" behavior added behind a flag, disabled by default (#3370)\n\n## jax 0.1.74 (July 29, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.73...jax-v0.1.74).\n* New Features:\n  * BFGS (#3101)\n  * TPU support for half-precision arithmetic (#3878)\n* Bug Fixes:\n  * Prevent some accidental dtype warnings (#3874)\n  * Fix a multi-threading bug in custom derivatives (#3845, #3869)\n* Improvements:\n  * Faster searchsorted implementation (#3873)\n  * Better test coverage for jax.numpy sorting algorithms (#3836)\n\n## jaxlib 0.1.52 (July 22, 2020)\n\n* Update XLA.\n\n## jax 0.1.73 (July 22, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.72...jax-v0.1.73).\n* The minimum jaxlib version is now 0.1.51.\n* New Features:\n  * jax.image.resize. (#3703)\n  * hfft and ihfft (#3664)\n  * jax.numpy.intersect1d (#3726)\n  * jax.numpy.lexsort (#3812)\n  * `lax.scan` and the `scan` primitive support an `unroll`\n    parameter for loop unrolling when lowering to XLA\n    ({jax-issue}`#3738`).\n* Bug Fixes:\n  * Fix reduction repeated axis error (#3618)\n  * Fix shape rule for lax.pad for input dimensions of size 0. (#3608)\n  * make psum transpose handle zero cotangents (#3653)\n  * Fix shape error when taking JVP of reduce-prod over size 0 axis. (#3729)\n  * Support differentiation through jax.lax.all_to_all (#3733)\n  * address nan issue in jax.scipy.special.zeta (#3777)\n* Improvements:\n  * Many improvements to jax2tf\n  * Reimplement argmin/argmax using a single pass variadic reduction. (#3611)\n  * Enable XLA SPMD partitioning by default. (#3151)\n  * Add support for 0d transpose convolution (#3643)\n  * Make LU gradient work for low-rank matrices (#3610)\n  * support multiple_results and custom JVPs in jet (#3657)\n  * Generalize reduce-window padding to support (lo, hi) pairs. (#3728)\n  * Implement complex convolutions on CPU and GPU. (#3735)\n  * Make jnp.take work for empty slices of empty arrays. (#3751)\n  * Relax dimension ordering rules for dot_general. (#3778)\n  * Enable buffer donation for GPU. (#3800)\n  * Add support for base dilation and window dilation to reduce window op\u2026 (#3803)\n\n## jaxlib 0.1.51 (July 2, 2020)\n\n* Update XLA.\n* Add new runtime support for host_callback.\n\n## jax 0.1.72 (June 28, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.71...jax-v0.1.72).\n* Bug fixes:\n  * Fix an odeint bug introduced in the previous release, see\n    {jax-issue}`#3587`.\n\n## jax 0.1.71 (June 25, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.70...jax-v0.1.71).\n* The minimum jaxlib version is now 0.1.48.\n* Bug fixes:\n  * Allow `jax.experimental.ode.odeint` dynamics functions to close over\n    values with respect to which we're differentiating\n    {jax-issue}`#3562`.\n\n## jaxlib 0.1.50 (June 25, 2020)\n\n* Add support for CUDA 11.0.\n* Drop support for CUDA 9.2 (we only maintain support for the last four CUDA\n  versions.)\n* Update XLA.\n\n## jaxlib 0.1.49 (June 19, 2020)\n\n* Bug fixes:\n  * Fix build issue that could result in slow compiles\n    (<https://github.com/tensorflow/tensorflow/commit/f805153a25b00d12072bd728e91bb1621bfcf1b1>)\n\n## jaxlib 0.1.48 (June 12, 2020)\n\n* New features:\n  * Adds support for fast traceback collection.\n  * Adds preliminary support for on-device heap profiling.\n  * Implements `np.nextafter` for `bfloat16` types.\n  * Complex128 support for FFTs on CPU and GPU.\n* Bug fixes:\n  * Improved float64 `tanh` accuracy on GPU.\n  * float64 scatters on GPU are much faster.\n  * Complex matrix multiplication on CPU should be much faster.\n  * Stable sorts on CPU should actually be stable now.\n  * Concurrency bug fix in CPU backend.\n\n## jax 0.1.70 (June 8, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.69...jax-v0.1.70).\n* New features:\n  * `lax.switch` introduces indexed conditionals with multiple\n    branches, together with a generalization of the `cond`\n    primitive\n    {jax-issue}`#3318`.\n\n## jax 0.1.69 (June 3, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.68...jax-v0.1.69).\n\n## jax 0.1.68 (May 21, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.67...jax-v0.1.68).\n* New features:\n  * {func}`lax.cond` supports a single-operand form, taken as the argument\n    to both branches\n    {jax-issue}`#2993`.\n* Notable changes:\n  * The format of the `transforms` keyword for the {func}`jax.experimental.host_callback.id_tap`\n    primitive has changed {jax-issue}`#3132`.\n\n## jax 0.1.67 (May 12, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.66...jax-v0.1.67).\n* New features:\n  * Support for reduction over subsets of a pmapped axis using `axis_index_groups`\n    {jax-issue}`#2382`.\n  * Experimental support for printing and calling host-side Python function from\n    compiled code. See [id_print and id_tap](https://docs.jax.dev/en/latest/jax.experimental.host_callback.html)\n    ({jax-issue}`#3006`).\n* Notable changes:\n  * The visibility of names exported from {mod}`jax.numpy` has been\n    tightened. This may break code that was making use of names that were\n    previously exported accidentally.\n\n## jaxlib 0.1.47 (May 8, 2020)\n\n* Fixes crash for outfeed.\n\n## jax 0.1.66 (May 5, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.65...jax-v0.1.66).\n* New features:\n  * Support for `in_axes=None` on {func}`pmap`\n    {jax-issue}`#2896`.\n\n## jaxlib 0.1.46 (May 5, 2020)\n\n* Fixes crash for linear algebra functions on Mac OS X (#432).\n* Fixes an illegal instruction crash caused by using AVX512 instructions when\n  an operating system or hypervisor disabled them (#2906).\n\n## jax 0.1.65 (April 30, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.64...jax-v0.1.65).\n* New features:\n  * Differentiation of determinants of singular matrices\n    {jax-issue}`#2809`.\n* Bug fixes:\n  * Fix {func}`odeint` differentiation with respect to time of ODEs with\n    time-dependent dynamics {jax-issue}`#2817`,\n    also add ODE CI testing.\n  * Fix {func}`lax_linalg.qr` differentiation\n    {jax-issue}`#2867`.\n\n## jaxlib 0.1.45 (April 21, 2020)\n\n* Fixes segfault: {jax-issue}`#2755`\n* Plumb is_stable option on Sort HLO through to Python.\n\n## jax 0.1.64 (April 21, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.63...jax-v0.1.64).\n* New features:\n  * Add syntactic sugar for functional indexed updates\n    {jax-issue}`#2684`.\n  * Add {func}`jax.numpy.linalg.multi_dot` {jax-issue}`#2726`.\n  * Add {func}`jax.numpy.unique` {jax-issue}`#2760`.\n  * Add {func}`jax.numpy.rint` {jax-issue}`#2724`.\n  * Add {func}`jax.numpy.rint` {jax-issue}`#2724`.\n  * Add more primitive rules for {func}`jax.experimental.jet`.\n* Bug fixes:\n  * Fix {func}`logaddexp` and {func}`logaddexp2` differentiation at zero {jax-issue}`#2107`.\n  * Improve memory usage in reverse-mode autodiff without {func}`jit`\n    {jax-issue}`#2719`.\n* Better errors:\n  * Improves error message for reverse-mode differentiation of {func}`lax.while_loop`\n    {jax-issue}`#2129`.\n\n## jaxlib 0.1.44 (April 16, 2020)\n\n* Fixes a bug where if multiple GPUs of different models were present, JAX\n  would only compile programs suitable for the first GPU.\n* Bugfix for `batch_group_count` convolutions.\n* Added precompiled SASS for more GPU versions to avoid startup PTX compilation\n  hang.\n\n## jax 0.1.63 (April 12, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.62...jax-v0.1.63).\n* Added `jax.custom_jvp` and `jax.custom_vjp` from {jax-issue}`#2026`, see the [tutorial notebook](https://docs.jax.dev/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html). Deprecated `jax.custom_transforms` and removed it from the docs (though it still works).\n* Add `scipy.sparse.linalg.cg` {jax-issue}`#2566`.\n* Changed how Tracers are printed to show more useful information for debugging {jax-issue}`#2591`.\n* Made `jax.numpy.isclose` handle `nan` and `inf` correctly {jax-issue}`#2501`.\n* Added several new rules for `jax.experimental.jet` {jax-issue}`#2537`.\n* Fixed `jax.experimental.stax.BatchNorm` when `scale`/`center` isn't provided.\n* Fix some missing cases of broadcasting in `jax.numpy.einsum` {jax-issue}`#2512`.\n* Implement `jax.numpy.cumsum` and `jax.numpy.cumprod` in terms of a parallel prefix scan {jax-issue}`#2596` and make `reduce_prod` differentiable to arbitrary order {jax-issue}`#2597`.\n* Add `batch_group_count` to `conv_general_dilated` {jax-issue}`#2635`.\n* Add docstring for `test_util.check_grads` {jax-issue}`#2656`.\n* Add `callback_transform` {jax-issue}`#2665`.\n* Implement `rollaxis`, `convolve`/`correlate` 1d & 2d, `copysign`,\n  `trunc`, `roots`, and `quantile`/`percentile` interpolation options.\n\n## jaxlib 0.1.43 (March 31, 2020)\n\n* Fixed a performance regression for Resnet-50 on GPU.\n\n## jax 0.1.62 (March 21, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.61...jax-v0.1.62).\n* JAX has dropped support for Python 3.5. Please upgrade to Python 3.6 or newer.\n* Removed the internal function `lax._safe_mul`, which implemented the\n  convention `0. * nan == 0.`. This change means some programs when\n  differentiated will produce nans when they previously produced correct\n  values, though it ensures nans rather than silently incorrect results are\n  produced for other programs. See #2447 and #1052 for details.\n* Added an `all_gather` parallel convenience function.\n* More type annotations in core code.\n\n## jaxlib 0.1.42 (March 19, 2020)\n\n* jaxlib 0.1.41 broke cloud TPU support due to an API incompatibility. This\n  release fixes it again.\n* JAX has dropped support for Python 3.5. Please upgrade to Python 3.6 or newer.\n\n## jax 0.1.61 (March 17, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.60...jax-v0.1.61).\n* Fixes Python 3.5 support. This will be the last JAX or jaxlib release that\n  supports Python 3.5.\n\n## jax 0.1.60 (March 17, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.59...jax-v0.1.60).\n* New features:\n  * {py:func}`jax.pmap` has `static_broadcast_argnums` argument which allows\n    the user to specify arguments that should be treated as compile-time\n    constants and should be broadcasted to all devices. It works analogously to\n    `static_argnums` in {py:func}`jax.jit`.\n  * Improved error messages for when tracers are mistakenly saved in global state.\n  * Added {py:func}`jax.nn.one_hot` utility function.\n  * Added {mod}`jax.experimental.jet` for exponentially faster\n    higher-order automatic differentiation.\n  * Added more correctness checking to arguments of {py:func}`jax.lax.broadcast_in_dim`.\n* The minimum jaxlib version is now 0.1.41.\n\n## jaxlib 0.1.40 (March 4, 2020)\n\n* Adds experimental support in Jaxlib for TensorFlow profiler, which allows\n  tracing of CPU and GPU computations from TensorBoard.\n* Includes prototype support for multihost GPU computations that communicate via\n  NCCL.\n* Improves performance of NCCL collectives on GPU.\n* Adds TopK, CustomCallWithoutLayout, CustomCallWithLayout, IGammaGradA and\n  RandomGamma implementations.\n* Supports device assignments known at XLA compilation time.\n\n## jax 0.1.59 (February 11, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.58...jax-v0.1.59).\n* Breaking changes\n\n  * The minimum jaxlib version is now 0.1.38.\n  * Simplified {py:class}`Jaxpr` by removing the `Jaxpr.freevars` and\n    `Jaxpr.bound_subjaxprs`. The call primitives (`xla_call`, `xla_pmap`,\n    `sharded_call`, and `remat_call`) get a new parameter `call_jaxpr` with a\n    fully-closed (no `constvars`) jaxpr. Also, added a new field `call_primitive`\n    to primitives.\n* New features:\n  * Reverse-mode automatic differentiation (e.g. `grad`) of `lax.cond`, making it\n    now differentiable in both modes ({jax-issue}`#2091`)\n  * JAX now supports DLPack, which allows sharing CPU and GPU arrays in a\n    zero-copy way with other libraries, such as PyTorch.\n  * JAX GPU DeviceArrays now support `__cuda_array_interface__`, which is another\n    zero-copy protocol for sharing GPU arrays with other libraries such as CuPy\n    and Numba.\n  * JAX CPU device buffers now implement the Python buffer protocol, which allows\n    zero-copy buffer sharing between JAX and NumPy.\n  * Added JAX_SKIP_SLOW_TESTS environment variable to skip tests known as slow.\n\n## jaxlib 0.1.39 (February 11, 2020)\n\n* Updates XLA.\n\n## jaxlib 0.1.38 (January 29, 2020)\n\n* CUDA 9.0 is no longer supported.\n* CUDA 10.2 wheels are now built by default.\n\n## jax 0.1.58 (January 28, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/46014da21...jax-v0.1.58).\n* Breaking changes\n\n  * JAX has dropped Python 2 support, because Python 2 reached its end of life on\n    January 1, 2020. Please update to Python 3.5 or newer.\n* New features\n\n  >   > * Forward-mode automatic differentiation (`jvp`) of while loop\n  >   ({jax-issue}`#1980`)\n  > * New NumPy and SciPy functions:\n  >\n  >   * {py:func}`jax.numpy.fft.fft2`\n  >   * {py:func}`jax.numpy.fft.ifft2`\n  >   * {py:func}`jax.numpy.fft.rfft`\n  >   * {py:func}`jax.numpy.fft.irfft`\n  >   * {py:func}`jax.numpy.fft.rfft2`\n  >   * {py:func}`jax.numpy.fft.irfft2`\n  >   * {py:func}`jax.numpy.fft.rfftn`\n  >   * {py:func}`jax.numpy.fft.irfftn`\n  >   * {py:func}`jax.numpy.fft.fftfreq`\n  >   * {py:func}`jax.numpy.fft.rfftfreq`\n  >   * {py:func}`jax.numpy.linalg.matrix_rank`\n  >   * {py:func}`jax.numpy.linalg.matrix_power`\n  >   * {py:func}`jax.scipy.special.betainc`\n  > * Batched Cholesky decomposition on GPU now uses a more efficient batched\n  >   kernel.\n\n### Notable bug fixes\n\n* With the Python 3 upgrade, JAX no longer depends on `fastcache`, which should\n  help with installation.\n", "CONTRIBUTING.md": "# Contributing to JAX\n\nFor information on how to contribute to JAX, see\n[Contributing to JAX](https://docs.jax.dev/en/latest/contributing.html)\n", "LICENSE": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n", "README.md": "<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/jax-ml/jax/main/images/jax_logo_250px.png\" alt=\"logo\"></img>\n</div>\n\n# Transformable numerical computing at scale\n\n[![Continuous integration](https://github.com/jax-ml/jax/actions/workflows/ci-build.yaml/badge.svg)](https://github.com/jax-ml/jax/actions/workflows/ci-build.yaml)\n[![PyPI version](https://img.shields.io/pypi/v/jax)](https://pypi.org/project/jax/)\n\n[**Transformations**](#transformations)\n| [**Scaling**](#scaling)\n| [**Install guide**](#installation)\n| [**Change logs**](https://docs.jax.dev/en/latest/changelog.html)\n| [**Reference docs**](https://docs.jax.dev/en/latest/)\n\n\n## What is JAX?\n\nJAX is a Python library for accelerator-oriented array computation and program transformation,\ndesigned for high-performance numerical computing and large-scale machine learning.\n\nJAX can automatically differentiate native\nPython and NumPy functions. It can differentiate through loops, branches,\nrecursion, and closures, and it can take derivatives of derivatives of\nderivatives. It supports reverse-mode differentiation (a.k.a. backpropagation)\nvia [`jax.grad`](#automatic-differentiation-with-grad) as well as forward-mode differentiation,\nand the two can be composed arbitrarily to any order.\n\nJAX uses [XLA](https://www.openxla.org/xla)\nto compile and scale your NumPy programs on TPUs, GPUs, and other hardware accelerators.\nYou can compile your own pure functions with [`jax.jit`](#compilation-with-jit).\nCompilation and automatic differentiation can be composed arbitrarily.\n\nDig a little deeper, and you'll see that JAX is really an extensible system for\n[composable function transformations](#transformations) at [scale](#scaling).\n\nThis is a research project, not an official Google product. Expect\n[sharp edges](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html).\nPlease help by trying it out, [reporting bugs](https://github.com/jax-ml/jax/issues),\nand letting us know what you think!\n\n```python\nimport jax\nimport jax.numpy as jnp\n\ndef predict(params, inputs):\n  for W, b in params:\n    outputs = jnp.dot(inputs, W) + b\n    inputs = jnp.tanh(outputs)  # inputs to the next layer\n  return outputs                # no activation on last layer\n\ndef loss(params, inputs, targets):\n  preds = predict(params, inputs)\n  return jnp.sum((preds - targets)**2)\n\ngrad_loss = jax.jit(jax.grad(loss))  # compiled gradient evaluation function\nperex_grads = jax.jit(jax.vmap(grad_loss, in_axes=(None, 0, 0)))  # fast per-example grads\n```\n\n### Contents\n* [Transformations](#transformations)\n* [Scaling](#scaling)\n* [Current gotchas](#gotchas-and-sharp-bits)\n* [Installation](#installation)\n* [Citing JAX](#citing-jax)\n* [Reference documentation](#reference-documentation)\n\n## Transformations\n\nAt its core, JAX is an extensible system for transforming numerical functions.\nHere are three: `jax.grad`, `jax.jit`, and `jax.vmap`.\n\n### Automatic differentiation with `grad`\n\nUse [`jax.grad`](https://docs.jax.dev/en/latest/jax.html#jax.grad)\nto efficiently compute reverse-mode gradients:\n\n```python\nimport jax\nimport jax.numpy as jnp\n\ndef tanh(x):\n  y = jnp.exp(-2.0 * x)\n  return (1.0 - y) / (1.0 + y)\n\ngrad_tanh = jax.grad(tanh)\nprint(grad_tanh(1.0))\n# prints 0.4199743\n```\n\nYou can differentiate to any order with `grad`:\n\n```python\nprint(jax.grad(jax.grad(jax.grad(tanh)))(1.0))\n# prints 0.62162673\n```\n\nYou're free to use differentiation with Python control flow:\n\n```python\ndef abs_val(x):\n  if x > 0:\n    return x\n  else:\n    return -x\n\nabs_val_grad = jax.grad(abs_val)\nprint(abs_val_grad(1.0))   # prints 1.0\nprint(abs_val_grad(-1.0))  # prints -1.0 (abs_val is re-evaluated)\n```\n\nSee the [JAX Autodiff\nCookbook](https://docs.jax.dev/en/latest/notebooks/autodiff_cookbook.html)\nand the [reference docs on automatic\ndifferentiation](https://docs.jax.dev/en/latest/jax.html#automatic-differentiation)\nfor more.\n\n### Compilation with `jit`\n\nUse XLA to compile your functions end-to-end with\n[`jit`](https://docs.jax.dev/en/latest/jax.html#just-in-time-compilation-jit),\nused either as an `@jit` decorator or as a higher-order function.\n\n```python\nimport jax\nimport jax.numpy as jnp\n\ndef slow_f(x):\n  # Element-wise ops see a large benefit from fusion\n  return x * x + x * 2.0\n\nx = jnp.ones((5000, 5000))\nfast_f = jax.jit(slow_f)\n%timeit -n10 -r3 fast_f(x)\n%timeit -n10 -r3 slow_f(x)\n```\n\nUsing `jax.jit` constrains the kind of Python control flow\nthe function can use; see\nthe tutorial on [Control Flow and Logical Operators with JIT](https://docs.jax.dev/en/latest/control-flow.html)\nfor more.\n\n### Auto-vectorization with `vmap`\n\n[`vmap`](https://docs.jax.dev/en/latest/jax.html#vectorization-vmap) maps\na function along array axes.\nBut instead of just looping over function applications, it pushes the loop down\nonto the function\u2019s primitive operations, e.g. turning matrix-vector multiplies into\nmatrix-matrix multiplies for better performance.\n\nUsing `vmap` can save you from having to carry around batch dimensions in your\ncode:\n\n```python\nimport jax\nimport jax.numpy as jnp\n\ndef l1_distance(x, y):\n  assert x.ndim == y.ndim == 1  # only works on 1D inputs\n  return jnp.sum(jnp.abs(x - y))\n\ndef pairwise_distances(dist1D, xs):\n  return jax.vmap(jax.vmap(dist1D, (0, None)), (None, 0))(xs, xs)\n\nxs = jax.random.normal(jax.random.key(0), (100, 3))\ndists = pairwise_distances(l1_distance, xs)\ndists.shape  # (100, 100)\n```\n\nBy composing `jax.vmap` with `jax.grad` and `jax.jit`, we can get efficient\nJacobian matrices, or per-example gradients:\n\n```python\nper_example_grads = jax.jit(jax.vmap(jax.grad(loss), in_axes=(None, 0, 0)))\n```\n\n## Scaling\n\nTo scale your computations across thousands of devices, you can use any\ncomposition of these:\n* [**Compiler-based automatic parallelization**](https://docs.jax.dev/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html)\nwhere you program as if using a single global machine, and the compiler chooses\nhow to shard data and partition computation (with some user-provided constraints);\n* [**Explicit sharding and automatic partitioning**](https://docs.jax.dev/en/latest/notebooks/explicit-sharding.html)\nwhere you still have a global view but data shardings are\nexplicit in JAX types, inspectable using `jax.typeof`;\n* [**Manual per-device programming**](https://docs.jax.dev/en/latest/notebooks/shard_map.html)\nwhere you have a per-device view of data\nand computation, and can communicate with explicit collectives.\n\n| Mode | View? | Explicit sharding? | Explicit Collectives? |\n|---|---|---|---|\n| Auto | Global | \u274c | \u274c |\n| Explicit | Global | \u2705 | \u274c |\n| Manual | Per-device | \u2705 | \u2705 |\n\n```python\nfrom jax.sharding import set_mesh, AxisType, PartitionSpec as P\nmesh = jax.make_mesh((8,), ('data',), axis_types=(AxisType.Explicit,))\nset_mesh(mesh)\n\n# parameters are sharded for FSDP:\nfor W, b in params:\n  print(f'{jax.typeof(W)}')  # f32[512@data,512]\n  print(f'{jax.typeof(b)}')  # f32[512]\n\n# shard data for batch parallelism:\ninputs, targets = jax.device_put((inputs, targets), P('data'))\n\n# evaluate gradients, automatically parallelized!\ngradfun = jax.jit(jax.grad(loss))\nparam_grads = gradfun(params, (inputs, targets))\n```\n\nSee the [tutorial](https://docs.jax.dev/en/latest/sharded-computation.html) and\n[advanced guides](https://docs.jax.dev/en/latest/advanced_guide.html) for more.\n\n## Gotchas and sharp bits\n\nSee the [Gotchas\nNotebook](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html).\n\n## Installation\n\n### Supported platforms\n\n|            | Linux x86_64 | Linux aarch64 | Mac aarch64  | Windows x86_64 | Windows WSL2 x86_64 |\n|------------|--------------|---------------|--------------|----------------|---------------------|\n| CPU        | yes          | yes           | yes          | yes            | yes                 |\n| NVIDIA GPU | yes          | yes           | n/a          | no             | experimental        |\n| Google TPU | yes          | n/a           | n/a          | n/a            | n/a                 |\n| AMD GPU    | yes          | no            | n/a          | no             | experimental        |\n| Apple GPU  | n/a          | no            | experimental | n/a            | n/a                 |\n| Intel GPU  | experimental | n/a           | n/a          | no             | no                  |\n\n\n### Instructions\n\n| Platform        | Instructions                                                                                                    |\n|-----------------|-----------------------------------------------------------------------------------------------------------------|\n| CPU             | `pip install -U jax`                                                                                            |\n| NVIDIA GPU      | `pip install -U \"jax[cuda13]\"`                                                                                  |\n| Google TPU      | `pip install -U \"jax[tpu]\"`                                                                                     |\n| AMD GPU (Linux) | Follow [AMD's instructions](https://github.com/jax-ml/jax/blob/main/build/rocm/README.md).                      |\n| Mac GPU         | Follow [Apple's instructions](https://developer.apple.com/metal/jax/).                                          |\n| Intel GPU       | Follow [Intel's instructions](https://github.com/intel/intel-extension-for-openxla/blob/main/docs/acc_jax.md).  |\n\nSee [the documentation](https://docs.jax.dev/en/latest/installation.html)\nfor information on alternative installation strategies. These include compiling\nfrom source, installing with Docker, using other versions of CUDA, a\ncommunity-supported conda build, and answers to some frequently-asked questions.\n\n## Citing JAX\n\nTo cite this repository:\n\n```\n@software{jax2018github,\n  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},\n  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},\n  url = {http://github.com/jax-ml/jax},\n  version = {0.3.13},\n  year = {2018},\n}\n```\n\nIn the above bibtex entry, names are in alphabetical order, the version number\nis intended to be that from [jax/version.py](../main/jax/version.py), and\nthe year corresponds to the project's open-source release.\n\nA nascent version of JAX, supporting only automatic differentiation and\ncompilation to XLA, was described in a [paper that appeared at SysML\n2018](https://mlsys.org/Conferences/2019/doc/2018/146.pdf). We're currently working on\ncovering JAX's ideas and capabilities in a more comprehensive and up-to-date\npaper.\n\n## Reference documentation\n\nFor details about the JAX API, see the\n[reference documentation](https://docs.jax.dev/).\n\nFor getting started as a JAX developer, see the\n[developer documentation](https://docs.jax.dev/en/latest/developer.html).\n", "build/rocm/README.md": "# JAX on ROCm\nThis directory provides setup instructions and necessary files to build, test, and run JAX with ROCm support in a Docker environment, suitable for both runtime and CI workflows. Explore the following methods to use or build JAX on ROCm!\n\n## 1. Using Prebuilt Docker Images\n\nThe ROCm JAX team provides prebuilt Docker images, which the simplest way to use JAX on ROCm. These images are available on Docker Hub and come with JAX configured for ROCm.\n\nTo pull the latest ROCm JAX Docker image, run:\n\n```Bash\n> docker pull rocm/jax-community:latest\n```\n\nOnce the image is downloaded, launch a container using the following command:\n\n```Bash\n> docker run -it -d --network=host --device=/dev/kfd --device=/dev/dri --ipc=host --shm-size 64G --group-add video --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -v $(pwd):/jax_dir --name rocm_jax rocm/jax-community:latest /bin/bash\n\n> docker attach rocm_jax\n```\n\n### Notes:\n1. The `--shm-size` parameter allocates shared memory for the container. Adjust it based on your system's resources if needed.\n2. Replace `$(pwd)` with the absolute path to the directory you want to mount inside the container.\n\n***For older versions please review the periodically pushed docker images at:\n[ROCm JAX Community DockerHub](https://hub.docker.com/r/rocm/jax-community/tags).***\n\n### Testing your ROCm environment with JAX:\n\nAfter launching the container, test whether JAX detects ROCm devices as expected:\n\n```Bash\n> python -c \"import jax; print(jax.devices())\"\n[RocmDevice(id=0), RocmDevice(id=1), RocmDevice(id=2), RocmDevice(id=3)]\n```\n\nIf the setup is successful, the output should list all available ROCm devices.\n\n## 2. Using a ROCm Docker Image and Installing JAX\n\nIf you prefer to use the ROCm Ubuntu image or already have a ROCm Ubuntu container, follow these steps to install JAX in the container.\n\n### Step 1: Pull the ROCm Ubuntu Docker Image\n\nFor example, use the following command to pull the ROCm Ubuntu image:\n\n```Bash\n> docker pull rocm/dev-ubuntu-22.04:6.3-complete\n```\n\n### Step 2: Launch the Docker Container\n\nAfter pulling the image, launch a container using this command:\n\n```Bash\n> docker run -it -d --network=host --device=/dev/kfd --device=/dev/dri --ipc=host --shm-size 64G --group-add video --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -v $(pwd):/jax_dir --name rocm_jax rocm/dev-ubuntu-22.04:6.3-complete /bin/bash\n> docker attach rocm_jax\n```\n\n### Step 3: Install the Latest Version of JAX\n\nInside the running container, install the required version of JAX with ROCm support using pip:\n\n```Bash\n> pip3 install jax[rocm]\n```\n\n### Step 4: Verify the Installed JAX Version\n\nCheck whether the correct version of JAX and its ROCm plugins are installed:\n\n```Bash\n> pip3 freeze | grep jax\njax==0.4.35\njax-rocm60-pjrt==0.4.35\njax-rocm60-plugin==0.4.35\njaxlib==0.4.35\n```\n\n### Step 5: Set the `LLVM_PATH` Environment Variable\n\nExplicitly set the `LLVM_PATH` environment variable (This helps XLA find `ld.lld` in the PATH during runtime):\n\n```Bash\n> export LLVM_PATH=/opt/rocm/llvm\n```\n\n### Step 6: Verify the Installation of ROCm JAX\n\nRun the following command to verify that ROCm JAX is installed correctly:\n\n```Bash\n> python3 -c \"import jax; print(jax.devices())\"\n[RocmDevice(id=0), RocmDevice(id=1), RocmDevice(id=2), RocmDevice(id=3)]\n\n> python3 -c \"import jax.numpy as jnp; x = jnp.arange(5); print(x)\"\n[0 1 2 3 4]\n```\n\n## 3. Install JAX On Bare-metal or A Custom Container\n\nFollow these steps if you prefer to install ROCm manually on your host system or in a custom container.\n\n### Installing ROCm Libraries Manually\n\n### Step 1: Install ROCm\n\nPlease follow [ROCm installation guide](https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html) to install ROCm on your system.\n\nOnce installed, verify ROCm installation using:\n\n```Bash\n> rocm-smi\n\n========================================== ROCm System Management Interface ==========================================\n==================================================== Concise Info ====================================================\nDevice  [Model : Revision]    Temp        Power     Partitions      SCLK     MCLK     Fan  Perf  PwrCap  VRAM%  GPU%\n        Name (20 chars)       (Junction)  (Socket)  (Mem, Compute)\n======================================================================================================================\n0       [0x74a1 : 0x00]       50.0\u00b0C      170.0W    NPS1, SPX       131Mhz   900Mhz   0%   auto  750.0W    0%   0%\n        AMD Instinct MI300X\n1       [0x74a1 : 0x00]       51.0\u00b0C      176.0W    NPS1, SPX       132Mhz   900Mhz   0%   auto  750.0W    0%   0%\n        AMD Instinct MI300X\n2       [0x74a1 : 0x00]       50.0\u00b0C      177.0W    NPS1, SPX       132Mhz   900Mhz   0%   auto  750.0W    0%   0%\n        AMD Instinct MI300X\n3       [0x74a1 : 0x00]       53.0\u00b0C      176.0W    NPS1, SPX       132Mhz   900Mhz   0%   auto  750.0W    0%   0%\n        AMD Instinct MI300X\n======================================================================================================================\n================================================ End of ROCm SMI Log =================================================\n```\n\n### Step 2: Install the Latest Version of JAX\n\nInstall the required version of JAX with ROCm support using pip:\n\n```Bash\n> pip3 install jax[rocm]\n```\n\n### Step 3: Verify the Installed JAX Version\n\nCheck whether the correct version of JAX and its ROCm plugins are installed:\n\n```Bash\n> pip3 freeze | grep jax\njax==0.4.35\njax-rocm60-pjrt==0.4.35\njax-rocm60-plugin==0.4.35\njaxlib==0.4.35\n```\n\n### Step 4: Set the `LLVM_PATH` Environment Variable\n\nExplicitly set the `LLVM_PATH` environment variable (This helps XLA find `ld.lld` in the PATH during runtime):\n\n```Bash\n> export LLVM_PATH=/opt/rocm/llvm\n```\n\n### Step 5: Verify the Installation of ROCm JAX\n\nRun the following command to verify that ROCm JAX is installed correctly:\n\n```Bash\n> python3 -c \"import jax; print(jax.devices())\"\n[RocmDevice(id=0), RocmDevice(id=1), RocmDevice(id=2), RocmDevice(id=3)]\n\n> python3 -c \"import jax.numpy as jnp; x = jnp.arange(5); print(x)\"\n[0 1 2 3 4]\n```\n\n## 4. Build ROCm JAX from Source\n\nFollow these steps to build JAX with ROCm support from source:\n\n### Step 1: Clone the Repository\n\nClone the ROCm-specific fork of JAX for the desired branch:\n\n```Bash\n> git clone https://github.com/ROCm/jax -b <branch_name>\n> cd jax\n```\n\n### Step 2: Build the Wheels\n\nRun the following command to build the necessary wheels:\n\n```Bash\n> python3 ./build/build.py build --wheels=jaxlib,jax-rocm-plugin,jax-rocm-pjrt \\\n    --rocm_version=60 --rocm_path=/opt/rocm-[version]\n```\n\nThis will generate three wheels in the `dist/` directory:\n\n* jaxlib (generic, device agnostic library)\n* jax-rocm-plugin (ROCm-specific plugin)\n* jax-rocm-pjrt (ROCm-specific runtime)\n\n### Step 3: Then install custom JAX using:\n\n```Bash\n> python3 setup.py develop --user && pip3 -m pip install dist/*.whl\n```\n\n### Simplified Build Script\n\nFor a streamlined process, consider using the `jax/build/rocm/dev_build_rocm.py` script.\n", "ci/README.md": "# JAX Continuous Integration\n\nThis folder contains the configuration files and scripts used to build and test\nJAX. It is typically used by continuous integration (CI) jobs to automate builds\nand run comprehensive tests across various platforms and configurations. This\npage provides an overview of the JAX CI system, its components, and the\ndifferent workflows it supports.\n\n********************************************************************************\n\n## JAX's CI System\n\n![Overview of JAX's CI System](jax_ci_system.png)\n\nJAX's CI system is composed of several interacting components and orchestrates\nbuilds and tests using a hybrid approach, leveraging both an internal CI system\nand GitHub Actions as well as an internal build orchestrator for managing\nnightly and release flows. It encompasses several distinct workflows, including\ncomprehensive presubmit checks triggered on pull requests and branch pushes,\nbi-hourly continuous builds, extensive nightly builds with broad platform\ncoverage, and a controlled release process that culminates in PyPI publication.\n\nThese flows build four packages: `jax`, `jaxlib`, `jax-cuda-plugin`,\n`jax-cuda-pjrt` and support a range of environments, including:\n\n*   **Linux x86:** CPU, TPU, CUDA\n*   **Linux aarch64:** CPU, CUDA\n*   **Windows x86:** CPU\n*   **Mac Arm64:** CPU\n\n### Architecture Overview\n\n1.  **Internal CI System:** An internal CI system is used for specific build and\n    test tasks, such as nightly builds, release candidate (RC) builds, and\n    Mac-specific testing.\n\n2.  **GitHub Actions:** Used for presubmit checks, continuous integration builds\n    and tests, and nightly/release artifact testing.\n\n3.  **Build Orchestrator:** An internal tool used to manage complex workflows\n    such as nightly / release flows, promoting RC builds to release, etc.\n\n4.  **Artifact Storage:**\n\n*   Google Cloud Storage (GCS) Buckets: Used for temporary storage of artifacts\n    between jobs in GitHub Actions workflows and for storing packages built\n    during nightly and release flows before testing.\n*   Artifact Registry: Used to store nightly packages, RC packages and final\n    releases.\n*   PyPI: Where final releases are published.\n\n### CI Workflows and Where They Run\n\nJAX's CI system consists of the following workflows:\n\n1.  **Presubmits:** Presubmits are run in GitHub actions and are triggered on\n    pull requests that target the `main` branch and on pushes to the `main` and\n    `release` branch. JAX's presubmit run time SLO is about 10 minutes so these\n    are typically run using Bazel with remote build execution\n    ([RBE](https://bazel.build/remote/rbe)). RBE allows us to execute build and\n    test actions on a distributed system, separate from the local machine,\n    instead of solely on the local machine. This enables faster build and test\n    times by utilizing parallel computing resources and caching across a cluster\n    of machines. However, we also use Pytest in workflows where we are not able\n    to use RBE such as the TPU presubmit. In such presubmits, we usually run a\n    subset of tests to be able to satisfy the presubmit run time SLO. To see the\n    list of the presubmit workflows,\n    [click here](https://github.com/search?q=repo%3Ajax-ml%2Fjax+path%3A.github%2Fworkflows%2F+%28path%3A**%2F*.yml+OR+path%3A**%2F*.yaml%29+%22pull_request%22&type=code).\n\n2.  **Continuous:** These jobs are run in GitHub actions and are scheduled to\n    run once every 2 hours on the `main` branch. It builds JAX packages and runs\n    a wide range of tests targeting different environments such as CPU, CUDA\n    (L4, H100, B200, etc), and TPU (v4-8, v5e-8, etc.). For more information,\n    see\n    [wheel_tests_continuous.yml](https://github.com/jax-ml/jax/blob/main/.github/workflows/wheel_tests_continuous.yml)\n    ([An example run](https://github.com/jax-ml/jax/actions/workflows/wheel_tests_continuous.yml).)\n\n3.  **Nightly Builds and Tests:** These jobs use an hybrid approach of both the\n    internal CI system and GitHub actions. The jobs are triggered once every\n    night by the internal build orchestrator tool. It first triggers the jobs in\n    the internal CI system to build the JAX packages for different\n    configurations (Python versions, CUDA versions, etc) and uploads them to a\n    staging bucket in GCS as well as to the nightly artifact registry. Next,\n    testing jobs are triggered that download the artifacts from the staging\n    bucket and run tests. Mac testing jobs are run in the internal CI system.\n    For non-Mac testing, a trigger job is run that invokes the\n    [wheel_tests_nightly_release.yml](https://github.com/jax-ml/jax/blob/main/.github/workflows/wheel_tests_nightly_release.yml)\n    workflow in GitHub Actions. JAX's nightly artifacts can be found here:\n    [jax](https://us-python.pkg.dev/ml-oss-artifacts-published/jax-public-nightly-artifacts-registry/simple/jax),\n    [jaxlib](https://us-python.pkg.dev/ml-oss-artifacts-published/jax-public-nightly-artifacts-registry/simple/jaxlib),\n    [jax-cuda-plugin](https://us-python.pkg.dev/ml-oss-artifacts-published/jax-public-nightly-artifacts-registry/simple/jax-cuda12-plugin),\n    [jax-cuda-pjrt](https://us-python.pkg.dev/ml-oss-artifacts-published/jax-public-nightly-artifacts-registry/simple/jax-cuda12-pjrt).\n\n4.  **Release Builds and Tests:** Release flow is similar to the nightly flow\n    except for few differences. First, release process has to be triggered\n    manually in the internal build orchestrator and should be done only after a\n    release branch (E.g `release/0.5.3`) has been created. The build jobs build\n    two sets of artifacts for each package: 1. RC wheels 2. Final version\n    wheels. These two sets are pretty much the same package except for their\n    metadata and wheel tags. The RC wheels are then uploaded to the staging\n    bucket and release artifact registry. After the uploads are done, the test\n    jobs are triggered. As with the nightly flow, Mac test jobs are run in the\n    internal CI system while non-Mac test jobs are run in GitHub actions. To see\n    the GitHub actions run for a particular release, filter the workflow runs by\n    its branch name.\n    <!-- [To be added after the release process has been switched over to the new system] For e.g, here are the [runs](https://github.com/jax-ml/jax/actions/workflows/wheel_tests_nightly_release.yml?query=branch%3Arelease%2F0.5.3) for `release/0.5.3`. -->\n\n5.  **Promote RC to Final and Publish to PyPI:** If the RC wheels pass all\n    testing, then we are ready to promote it as the final version and publish it\n    to PyPI. This entire flow is internal and is run in our internal CI system.\n    Final version of the packages are published to PyPI and JAX's release\n    artifact registry. JAX's release artifacts (RC and final versions) can be\n    found here:\n    [jax](https://us-python.pkg.dev/ml-oss-artifacts-published/jax-public-release-artifacts-registry/simple/jax),\n    [jaxlib](https://us-python.pkg.dev/ml-oss-artifacts-published/jax-public-release-artifacts-registry/simple/jaxlib),\n    [jax-cuda-plugin](https://us-python.pkg.dev/ml-oss-artifacts-published/jax-public-release-artifacts-registry/simple/jax-cuda12-plugin),\n    [jax-cuda-pjrt](https://us-python.pkg.dev/ml-oss-artifacts-published/jax-public-release-artifacts-registry/simple/jax-cuda12-pjrt).\n\n### JAX's Official CI and Build/Test Scripts\n\nJAX's CI jobs (both internal and those on GitHub actions) run the scripts in\nthis folder. An overview of the different folders and their purpose is given\nbelow:\n\n-   **ci/**: Contains all build scripts, environment files, and utility scripts.\n-   **ci/utilities/**: Contains helper scripts used throughout the build/test\n    process. See\n    [README.md](https://github.com/jax-ml/jax/blob/main/ci/utilities/README.md)\n    for a brief overview of these utility scripts and their behavior.\n-   **ci/envs/**: Holds environment files that set `JAXCI` environment variables\n    that control build and test configurations. see\n    [README.md](https://github.com/jax-ml/jax/blob/main/ci/envs/README.md) to\n    see the complete list of these variables and their behavior.\n\nEvery build script in this folder first source the `JAXCI` envs in\n[default.env](https://github.com/jax-ml/jax/blob/main/ci/envs/default.env) and\nthen run the\n[setup_build_environment.sh](https://github.com/jax-ml/jax/blob/main/ci/utilities/setup_build_environment.sh)\nscript to set up the build environment.\n\nA brief overview of each build script in this folder is given below:\n\n> [!NOTE]\n> Both internal and GitHub action jobs run under the\n> [ml-build](https://github.com/tensorflow/tensorflow/tree/master/ci/official/containers)\n> Docker image which contains build tools such as Python, Bazelisk, LLVM/Clang,\n> manylinux compliant libraries (in Linux images), etc.\n\n-   **build_artifacts.sh:** These build the various JAX artifacts. We build\n    three different type of artifacts based on the type of job: Nightly,\n    RC/Release, or at HEAD.\n-   **run_bazel_test_cpu_rbe.sh/run_bazel_test_cuda_rbe.sh**: These run Bazel\n    tests with RBE on every GitHub PR. We test compatibility with both CPU and\n    CUDA. On platforms where RBE is not natively supported (e.g Linux Arm64), we\n    cross-compile the test targets for Linux Aarch64 on Linux x86. As the tests\n    still need to be run on the host machines and because running the tests on a\n    single machine can take a long time, we skip running them on these\n    platforms.\n    Note for `run_bazel_test_cpu_rbe.sh`:\n    - If `$JAXCI_BUILD_JAXLIB=false` and `$JAXCI_BUILD_JAX=false`, these jobs\n      depend on local JAX wheels and therefore require that the following wheels\n      to be present in the `../dist` folder: `jax`, and `jaxlib` wheels. In CI\n      builds, we first build these wheels from source and then run the\n      `bazel test` command.\n    - If `$JAXCI_BUILD_JAXLIB=false` and `$JAXCI_BUILD_JAX=true`, CPU jobs\n      depend on local jaxlib wheels and therefore require that `jaxlib` wheel to\n      be present in the `../dist` folder. GPU obs\n      depend on local jaxlib and CUDA wheels, and therefore require that the\n      following wheels to be present in the `../dist` folder: `jaxlib`,\n      `jax-cuda-plugin`, and `jax-cuda-pjrt` wheels. In CI builds, we first\n      build these wheels from source and then run the `bazel test` command.\n    - If `$JAXCI_BUILD_JAXLIB=wheel` and `$JAXCI_BUILD_JAX=wheel`, the Bazel\n      tests use\n      [py_import](https://github.com/openxla/xla/blob/8190847008eddd4c7f3e57449e16d28631770823/third_party/py/py_import.bzl#L47).\n    - If `$JAXCI_BUILD_JAXLIB=true` and `$JAXCI_BUILD_JAX=true`, Bazel will use\n      individual targets in the test dependencies.\n-   **run_bazel_test_cuda_non_rbe.sh**: These run the following Bazel CUDA\n    tests: Single accelerator tests with one GPU apiece and Multi-accelerator\n    tests with all GPUs.\n    - If `$JAXCI_BUILD_JAXLIB=false` and `$JAXCI_BUILD_JAX=false`, these jobs\n      depend on local JAX wheels and therefore require that the following wheels\n      to be present in the `../dist` folder: `jax`, `jaxlib`, `jax-cuda-plugin`,\n      and `jax-cuda-pjrt` wheels. In CI builds, we first build these wheels from\n      source and then run the `bazel test` command.\n    - If `$JAXCI_BUILD_JAXLIB=wheel` and `$JAXCI_BUILD_JAX=wheel`, the Bazel\n      tests use [py_import](https://github.com/openxla/xla/blob/8190847008eddd4c7f3e57449e16d28631770823/third_party/py/py_import.bzl#L47).\n-   **run_pytest_*.sh**: These run tests with Pytests and use the JAX wheel\n    packages installed on the system. In CI builds, we build the wheels first\n    from source and then run the `pytest` commands. We test compatibility with\n    CPU, CUDA, and TPU. These are primarily run as part of the continuous and\n    nightly/release test jobs except for TPU which is also run as a presubmit\n    testing a subset of the tests.\n\n## Different Test Configurations\n\nJAX's CI Test jobs run under different test configurations. These configurations\nare described briefly in the sections below.\n\n### XLA Versions\n\nJAX's CI builds rely on XLA, but use different versions depending on the type of\nbuild. To ensure stability and reproducibility, nightly and release builds use a\npinned XLA version specified in the JAX workspace defined in [revision.bzl](https://github.com/jax-ml/jax/blob/b8b8c308a88060a3db63fa69c5cb7d8d7f1c5078/third_party/xla/revision.bzl#L23-L24).\n\nHowever, to keep JAX compatible with the latest XLA developments, presubmit and\npostsubmit builds utilize the most recent XLA version. This is done by\noverriding the default XLA dependency with a local copy of the XLA repository.\nWe do this by passing `--override_repository=xla=/path/to/local/xla` which\ninstructs Bazel to depend on the XLA in the local system instead of the version\nin the workspace.\n\nThe CI system uses the `JAXCI` environment variables to manage this process.\nWhen running jobs that need to use XLA at head, we set `JAXCI_CLONE_MAIN_XLA=1`.\nThis clones the XLA repository at head and sets `JAXCI_XLA_GIT_DIR` to its path.\n[JAX build CLI](https://github.com/jax-ml/jax/blob/main/build/build.py)\nautomatically adds the necessary Bazel flag (`--override_repository`) to point\nto this local XLA version during the build process if `JAXCI_XLA_GIT_DIR` is\nset. In jobs where the build CLI is not used such as the RBE presubmits, we\nexplicitly include `--override_repository=xla=\"${JAXCI_XLA_GIT_DIR}\"` as part\nof the test command.\n\n### Enabling/Disabling 64-bit Data Types\n\nBy default, JAX enforces single-precision numbers to mitigate the Numpy API\u2019s\ntendency to aggressively promote operands to `double`. In order to use\ndouble-precision numbers, we need to set the `JAX_ENABLE_X64` environment\nvariable. In CI, we test both configurations in presubmits and postsubmits by\nusing the `JAXCI_ENABLE_X64` environment variable.\n\n<!-- ## Monitoring And Logs [TODO] -->\n\n## [Googlers Only] Connecting to CI Runners for Debugging\n\nIf you are a Googler, you can connect to one of the self-hosted runners we have\non GitHub to debug your workflow. For more information, see\ngo/ml-github-actions:connect.\n\n## Running These Scripts Locally on Your Machine\n\n> [!IMPORTANT]\n> If you are a Linux / Windows user, you need to have Docker installed as a\n> prerequisite. Additionally, if running on Windows, please run these commands\n> in a bash environment as all the scripts are written in Shell.\n\nFollow the steps below to run a CI script locally on your machine.\n\n1.  [Optional] Set `JAXCI` variables in your shell environment. See\n    [ci/envs/README.md](https://github.com/jax-ml/jax/blob/main/ci/envs/README.md)\n    for the list of `JAXCI` variables and their behavior.\n\n2.  [Linux/Windows]\n\n    Start the Docker container by running:\n\n    ```bash\n        ./ci/utilities/run_docker_container.sh\n    ```\n\n    This will start a Docker container named \"jax\". Note that if you set any\n    `JAXCI` variables in step 1, they will also be be set in the container.\n\n    Run the script under the Docker container.\n\n    ```bash\n        # docker exec jax <build-script>\n        docker exec jax ./ci/build_artifacts.sh jaxlib\n    ```\n\n3.  [Mac] Execute the build script directly.\n\n    ```bash\n        # ./<build-script>\n        ./ci/build_artifacts.sh jaxlib\n    ```\n", "ci/envs/README.md": "# JAXCI Environment Variables\n\nThis docpage describes the various `JAXCI` environment variables that are used\nin the CI scripts and their behaviors. These variables are used to control the\nbehavior of the CI scripts such as the Python version used, path to JAX/XLA\nrepo, if to clone XLA repo, etc.\n\nName                                        | Default Value                            | Behavior                                                                                                                                                                                                                                                                                                                                                     | Usage\n------------------------------------------- | ---------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -----\n`JAXCI_JAX_GIT_DIR`                         | Present working directory: `$(pwd)`      | Path to the JAX's Git directory.                                                                                                                                                                                                                                                                                                                             | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_JAX_GIT_DIR&type=code)\n`JAXCI_HERMETIC_PYTHON_VERSION`             | System default                           | Controls the version of hermetic Python to use. This affects the Bazel commands only such as when building artifacts or when running the Bazel test scripts.                                                                                                                                                                                                                                                                            | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_HERMETIC_PYTHON_VERSION&type=code)\n`JAXCI_CUDA_VERSION`                         | 12                                    | Controls the CUDA version to use when building the JAX artifacts or running the tests.                                                                | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_CUDA_VERSION&type=code)\n`JAXCI_XLA_GIT_DIR`                         | Unset                                    | When using a local copy of XLA, this points to the root of the XLA git repository.                                                                                                                                                                                                                                                                           | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_XLA_GIT_DIR&type=code)\n`JAXCI_CLONE_MAIN_XLA`                      | 0                                        | If set to 1, the XLA repository is cloned at HEAD and its path is set in `JAXCI_XLA_GIT_DIR`                                                                                                                                                                                                                                                                 | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_CLONE_MAIN_XLA&type=code)\n`JAXCI_XLA_COMMIT`                          | Unset                                    | Allows overriding the XLA commit that is used when using a local copy of XLA.                                                                                                                                                                                                                                                                                | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_XLA_COMMIT&type=code)\n`JAXCI_OUTPUT_DIR`                          | `$(pwd)/dist`                            | Controls the location where the artifacts are written to. The directory will be automatically created if it does not exist.                                                                                                                                                                                                                                  | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_OUTPUT_DIR&type=code)\n`JAXCI_BUILD_ARTIFACT_WITH_RBE`             | 0                                        | When set to 1, Bazel will use RBE to build the artifacts. Requires gcloud authentication and only certain platforms support RBE so this typically only set in CI builds                                                                                                                                                                                      | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_BUILD_ARTIFACT_WITH_RBE&type=code)\n`JAXCI_WRITE_TO_BAZEL_REMOTE_CACHE`         | 0                                        | When set to 1, Bazel will also try to push new cache entries to the cache bucket. Since writes to the bucket require authentication, this flag is enabled only for CI builds. Note that the builds using RBE use the RBE cache and not Bazel's remote cache, therefore this variable is a no-op if `JAXCI_BUILD_ARTIFACT_WITH_RBE` is set to 1. When `JAXCI_BUILD_ARTIFACT_WITH_RBE` and `JAXCI_WRITE_TO_BAZEL_REMOTE_CACHE` are both not set, Bazel will still read from the public cache bucket to try to speed up the build. | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_WRITE_TO_BAZEL_REMOTE_CACHE&type=code)\n`JAXCI_ARTIFACT_TYPE`                       | \"default\"                                | Controls the type of artifacts to build. Valid values are \"default\", \"release\", \"nightly\". This affects the wheel tag and metadata, see [ci/build_artifacts.sh](https://github.com/jax-ml/jax/blob/main/ci/build_artifacts.sh) to understand how.                                                                                                            | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_ARTIFACT_TYPE&type=code)\n`JAXCI_WHEEL_RC_VERSION`                    | Unset                                    | During the release process, we build a Release Candidate (RC) wheel in addition to the release wheel. This environment variable sets the version of the RC wheel to build. Values are set internally.                                                                                                                                                        | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_WHEEL_RC_VERSION&type=code)\n`JAXCI_PYTHON`                              | `python${JAXCI_HERMETIC_PYTHON_VERSION}` | Points to the system Python binary to use. It used by scripts that make use of the system Python such as the Pytest scripts.                                                                                                                                                                                                                                 | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_PYTHON&type=code)\n`JAXCI_ENABLE_X64`                          | 0                                        | By default, JAX enforces single-precision numbers to mitigate the Numpy API\u2019s tendency to aggressively promote operands to `double`. When set to 1, the tests will use double-precision numbers.                                                                                                                                                             | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_ENABLE_X64&type=code)\n`JAXCI_TPU_CORES`                           | Unset                                    | Sets the number of TPU cores for the TPU machine type. Values are set in the workflow files.                                                                                                                                                                                                                                                                 | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_TPU_CORES&type=code)\n`JAXCI_RUN_FULL_TPU_TEST_SUITE`             | 0                                        | When set to 1, the full TPU test suite is run. Otherwise, a subset of tests is run.                                                                                                                                                                                                                                                                          | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_RUN_FULL_TPU_TEST_SUITE&type=code)\n`JAXCI_JAX_PYPI_EXTRAS` | Unset                                    | Used to control the installation of JAX extras from PyPI. See JAX's [setup.py](https://github.com/jax-ml/jax/blob/c9934912885bb7c4b72c5a9271598235a6789a81/setup.py#L71) for the list of valid values.                                                                                                                 | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_JAX_PYPI_EXTRAS&type=code)\n`JAXCI_BUILD_JAXLIB` | true                                    | Used to control the value of [build_jaxlib](https://github.com/jax-ml/jax/blob/338b4ebc8a5478e3d22efc9530be71d69c3bb993/jax/BUILD#L55-L63) flag.                                                                                                                 | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_BUILD_JAXLIB&type=code)\n`JAXCI_BUILD_JAX` | true                                    | Used to control the value of [build_jax](https://github.com/jax-ml/jax/blob/338b4ebc8a5478e3d22efc9530be71d69c3bb993/jax/BUILD#L92-L100) flag.                                                                                                                 | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_BUILD_JAX&type=code)\n`JAXCI_BAZEL_OUTPUT_BASE` | Unset | Used to control the output base for Bazel builds. | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_BAZEL_OUTPUT_BASE&type=code)\n\n## Docker Specific Environment Variables\n\n> [!NOTE]\n> The following environment variables only affect the build if the\n> [run_docker_container.sh](https://github.com/jax-ml/jax/blob/main/ci/utilities/run_docker_container.sh)\n> script was invoked to start a Docker container and the build is running inside\n> that container. Typically, this would be the internal CI builds and local\n> builds. Note that while GitHub actions use the same Docker images, they do not\n> invoke \"run_docker_container.sh\" as they leverage built-in containerization\n> features to run jobs within a container.\n\nName                    | Default Value                                                                                                | Behavior                                                                                             | Usage\n----------------------- | ------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------- | -----\n`JAXCI_DOCKER_WORK_DIR` | \"/jax\"                                                                                                       | The path on the container where the JAX Git repository is mounted to.                                | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_DOCKER_WORK_DIR&type=code)\n`JAXCI_DOCKER_ARGS`     | Empty String                                                                                                 | Space separated string of additional arguments that will be passed when starting the Docker container | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_DOCKER_ARGS&type=code)\n`JAXCI_DOCKER_IMAGE`    | Depends on the system (see [ci/envs/docker.env](https://github.com/jax-ml/jax/blob/main/ci/envs/docker.env)) | Docker image to pull                                                                                 | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_DOCKER_IMAGE&type=code)\n", "ci/utilities/README.md": "# JAX CI Utility Scripts\n\nThis docpage gives a brief overview of the different utility scripts and what\nthey are used for.\n\n-   **setup_build_environment.sh**: Sets up the build environment such as\n    cloning the latest XLA, adjusting file paths (for Windows), etc.\n-   **convert_msys_paths_to_win_paths.py**: Converts MSYS Linux-like paths\n    stored in env variables to Windows paths.\n-   **install_wheels_locally.sh**: Used by Pytest scripts to install JAX wheels\n    and any additional extras on the system.\n-   **run_auditwheel.sh**: Verifies that the Linux artifacts are \"manylinux\"\n    compliant.\n-   **run_docker_container.sh**: Runs a Docker container called \"jax\". Images\n    are read from the `JAXCI_DOCKER_IMAGE` environment variable in\n    [ci/envs/docker.env](https://github.com/jax-ml/jax/blob/main/ci/envs/docker.env).\n", "cloud_tpu_colabs/README.md": "# JAX on Cloud TPU examples\n\nThe same JAX code that runs on CPU and GPU can also be run on TPU. Cloud TPUs\nhave the advantage of quickly giving you access to multiple TPU accelerators,\nincluding in [Colab](https://research.google.com/colaboratory/). All of the\nexample notebooks here use\n[`jax.pmap`](https://docs.jax.dev/en/latest/jax.html#jax.pmap) to run JAX\ncomputation across multiple TPU cores from Colab. You can also run the same code\ndirectly on a [Cloud TPU\nVM](https://cloud.google.com/tpu/docs/jax-quickstart-tpu-vm).\n\n## Example Cloud TPU notebooks\n\nThe following notebooks showcase how to use and what you can do with Cloud TPUs on Colab:\n\n### [Pmap Cookbook](https://colab.research.google.com/github/jax-ml/jax/blob/main/cloud_tpu_colabs/Pmap_Cookbook.ipynb)\nA guide to getting started with `pmap`, a transform for easily distributing SPMD\ncomputations across devices.\n\n### [Lorentz ODE Solver](https://colab.research.google.com/github/jax-ml/jax/blob/main/cloud_tpu_colabs/Lorentz_ODE_Solver.ipynb)\nContributed by Alex Alemi (alexalemi@)\n\nSolve and plot parallel ODE solutions with `pmap`.\n\n<img src=\"https://raw.githubusercontent.com/jax-ml/jax/main/cloud_tpu_colabs/images/lorentz.png\" width=65%></image>\n\n### [Wave Equation](https://colab.research.google.com/github/jax-ml/jax/blob/main/cloud_tpu_colabs/Wave_Equation.ipynb)\nContributed by Stephan Hoyer (shoyer@)\n\nSolve the wave equation with `pmap`, and make cool movies! The spatial domain is partitioned across the 8 cores of a Cloud TPU.\n\n![](https://raw.githubusercontent.com/jax-ml/jax/main/cloud_tpu_colabs/images/wave_movie.gif)\n\n### [JAX Demo](https://colab.research.google.com/github/jax-ml/jax/blob/main/cloud_tpu_colabs/JAX_demo.ipynb)\nAn overview of JAX presented at the [Program Transformations for ML workshop at NeurIPS 2019](https://program-transformations.github.io/) and the [Compilers for ML workshop at CGO 2020](https://www.c4ml.org/). Covers basic numpy usage, `grad`, `jit`, `vmap`, and `pmap`.\n\n## Performance notes\n\nThe [guidance on running TensorFlow on TPUs](https://cloud.google.com/tpu/docs/performance-guide) applies to JAX as well, with the exception of TensorFlow-specific details. Here we highlight a few important details that are particularly relevant to using TPUs in JAX.\n\n### Padding\n\nOne of the most common culprits for surprisingly slow code on TPUs is inadvertent padding:\n- Arrays in the Cloud TPU are tiled. This entails padding one of the dimensions to a multiple of 8, and a different dimension to a multiple of 128.\n- The matrix multiplication unit performs best with pairs of large matrices that minimize the need for padding.\n\n### bfloat16 dtype\n\nBy default\\*, matrix multiplication in JAX on TPUs [uses bfloat16](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus) with float32 accumulation. This can be controlled with the `precision` keyword argument on relevant `jax.numpy` functions (`matmul`, `dot`, `einsum`, etc). In particular:\n- `precision=jax.lax.Precision.DEFAULT`: uses mixed bfloat16 precision (fastest)\n- `precision=jax.lax.Precision.HIGH`: uses multiple MXU passes to achieve higher precision\n- `precision=jax.lax.Precision.HIGHEST`: uses even more MXU passes to achieve full float32 precision\n\nJAX also adds the `bfloat16` dtype, which you can use to explicitly cast arrays to bfloat16, e.g., `jax.numpy.array(x, dtype=jax.numpy.bfloat16)`.\n\n\\* We might change the default precision in the future, since it is arguably surprising. Please comment/vote on [this issue](https://github.com/jax-ml/jax/issues/2161) if it affects you!\n\n## Running JAX on a Cloud TPU VM\n\nRefer to the [Cloud TPU VM\ndocumentation](https://cloud.google.com/tpu/docs/jax-quickstart-tpu-vm).\n\n## Reporting issues and getting help\n\nIf you run into Cloud TPU-specific issues (e.g. trouble creating a Cloud TPU\nVM), please email <cloud-tpu-support@google.com>, or <trc-support@google.com> if\nyou are a [TRC](https://sites.research.google/trc/) member. You can also [file a\nJAX issue](https://github.com/jax-ml/jax/issues) or [ask a discussion\nquestion](https://github.com/jax-ml/jax/discussions) for any issues with these\nnotebooks or using JAX in general.\n\nIf you have any other questions or comments regarding JAX on Cloud TPUs, please\nemail <jax-cloud-tpu-team@google.com>. We\u2019d like to hear from you!\n", "docs/README.md": "To rebuild the documentation, \nsee [Update Documentation](https://docs.jax.dev/en/latest/developer.html#update-documentation).\n", "docs/_tutorials/advanced-compilation.md": "# Advanced compilation\n\n<!--* freshness: { reviewed: '2024-05-03' } *-->\n\n```{note}\nThis is a placeholder for a section in the new {ref}`jax-tutorials-draft`.\n\nFor the time being, you may find some related content in the old documentation:\n- {doc}`../aot`\n- {doc}`../pallas/index`\n```\n", "docs/_tutorials/advanced-debugging.md": "---\njupytext:\n  formats: md:myst\n  text_representation:\n    extension: .md\n    format_name: myst\n    format_version: 0.13\n    jupytext_version: 1.16.4\nkernelspec:\n  display_name: Python 3\n  language: python\n  name: python3\n---\n\n(advanced-debugging)=\n# Advanced debugging\n\n<!--* freshness: { reviewed: '2024-05-03' } *-->\n\n```{note}\nThis is a placeholder for a section in the new {ref}`jax-tutorials-draft`.\n\nFor the time being, you may find some related content in the old documentation:\n- {doc}`../debugging`\n```\n", "docs/_tutorials/index.rst": ":orphan:\n\n.. _jax-tutorials-draft:\n\nJAX tutorials draft\n===================\n\n.. note::\n\n   The tutorials below are a work in progress; for the time being, please refer\n   to the older tutorial content, including :ref:`beginner-guide`,\n   :ref:`jax-101`, and the now-deleted *JAX 101* tutorials.\n\nJAX 101\n-------\nMostly finalized at :ref:`jax-101`!\n\n.. toctree::\n   :maxdepth: 1\n\n   ../key-concepts\n   ../jit-compilation\n   ../automatic-vectorization\n   ../automatic-differentiation\n   ../debugging\n   ../random-numbers\n   ../working-with-pytrees\n   ../sharded-computation\n   ../stateful-computations\n   simple-neural-network\n\n\nJAX 201\n-------\n\n.. toctree::\n   :maxdepth: 1\n\n   parallelism\n   advanced-debugging\n   profiling-and-performance\n\nJAX 301\n-------\n\n.. toctree::\n   :maxdepth: 1\n\n   advanced-compilation\n", "docs/_tutorials/parallelism.md": "# Parallel computation\n\n<!--* freshness: { reviewed: '2024-05-03' } *-->\n\n```{note}\nThis is a placeholder for a section in the new {ref}`jax-tutorials-draft`.\n\nFor the time being, you may find some related content in the old documentation:\n- {doc}`../multi_process`\n- {doc}`../notebooks/Distributed_arrays_and_automatic_parallelization`\n```\n", "docs/_tutorials/profiling-and-performance.md": "# Profiling and performance\n\n<!--* freshness: { reviewed: '2024-05-03' } *-->\n\n```{note}\nThis is a placeholder for a section in the new {ref}`jax-tutorials-draft`.\n\nFor the time being, you may find some related content in the old documentation:\n- {doc}`../profiling`\n- {doc}`../device_memory_profiling`\n- {doc}`../transfer_guard`\n```\n", "docs/_tutorials/simple-neural-network.md": "# Example: Writing a simple neural network\n\n<!--* freshness: { reviewed: '2024-05-03' } *-->\n\n```{note}\nThis is a placeholder for a section in the new {ref}`jax-tutorials-draft`.\n```\n", "docs/about.md": "(about-the-project)=\n\n# About the project\n\nThe JAX project is led by the JAX core team. We develop in the open,\nand welcome open-source contributions from across the community. We\nfrequently see contributions from [Google\nDeepMind](https://deepmind.google/), Alphabet more broadly,\n[NVIDIA](https://docs.nvidia.com/deeplearning/frameworks/jax-release-notes/overview.html),\nand elsewhere.\n\nAt the heart of the project is the [JAX\ncore](http://github.com/jax-ml/jax) library, which focuses on the\nfundamentals of machine learning and numerical computing, at scale.\n\nWhen [developing](#development) the core, we want to maintain agility\nand a focused scope, so we lean heavily on a surrounding [modular\ntechnology stack](#components). First, we design the `jax` module\nto be\n[composable](https://github.com/jax-ml/jax?tab=readme-ov-file#transformations)\nand\n[extensible](https://docs.jax.dev/en/latest/jax.extend.html), so\nthat a wide variety of domain-specific libraries can thrive outside of\nit in a decentralized manner. Second, we lean heavily on a modular\nbackend stack (compiler and runtime) to target different\naccelerators. Whether you are [writing a new domain-specific library\nbuilt with JAX](#upstack), or looking to [support\nnew hardware](#downstack), you can often\ncontribute these with *minimal to no modifications* to the JAX core\ncodebase.\n\nMany of JAX's core contributors have roots in open-source software and\nin research, in fields spanning computer science and the natural\nsciences. We strive to continuously enable the cutting edge of machine\nlearning and numerical computing---across all compute platforms and\naccelerators---and to discover the truths of array programming at\nscale.\n\n(development)=\n## Open development\n\nJAX's day-to-day development takes place in the open on GitHub, using\npull requests, the issue tracker, discussions, and [JAX Enhancement\nProposals\n(JEPs)](https://docs.jax.dev/en/latest/jep/index.html). Reading\nand participating in these is a good way to get involved. We also\nmaintain [developer\nnotes](https://docs.jax.dev/en/latest/contributor_guide.html)\nthat cover JAX's internal design.\n\nThe JAX core team determines whether to accept changes and\nenhancements. Maintaining a simple decision-making structure currently\nhelps us develop at the speed of the research frontier. Open\ndevelopment is a core value of ours, and we may adapt to a more\nintricate decision structure over time (e.g. with designated area\nowners) if/when it becomes useful to do so.\n\nFor more see [contributing to\nJAX](https://docs.jax.dev/en/latest/contributing.html).\n\n(components)=\n## A modular stack\n\nTo enable (a) a growing community of users across numerical domains,\nand (b) an advancing hardware landscape, we lean heavily on\n**modularity**.\n\n(upstack)=\n### Libraries built on JAX\n\nWhile the JAX core library focuses on the fundamentals, we want to\nencourage domain-specific libraries and tools to be built on top of\nJAX. Indeed, [many\nlibraries](https://docs.jax.dev/en/latest/#ecosystem) have\nemerged around JAX to offer higher-level features and extensions.\n\nHow do we encourage such decentralized development? We guide it with\nseveral technical choices. First, JAX's main API focuses on basic\nbuilding blocks (e.g. numerical primitives, NumPy operations, arrays,\nand transformations), encouraging auxiliary libraries to develop\nutilities as needed for their domain. In addition, JAX exposes a\nhandful of more advanced APIs for\n[customization](https://docs.jax.dev/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html)\nand\n[extensibility](https://docs.jax.dev/en/latest/jax.extend.html). Libraries\ncan [lean on these\nAPIs](https://docs.jax.dev/en/latest/building_on_jax.html) in\norder to use JAX as an internal means of implementation, to integrate\nmore with its transformations like autodiff, and more.\n\nProjects across the JAX ecosystem are developed in a distributed and\noften open fashion. They are not governed by the JAX core team, even\nthough sometimes team members contribute to them or maintain contact\nwith their developers.\n\n(downstack)=\n### A pluggable backend\n\nWe want JAX to run on CPUs, GPUs, TPUs, and other hardware platforms\nas they emerge. To encourage unhindered support of JAX on new\nplatforms, the JAX core emphasizes modularity in its backend too.\n\nTo manage hardware devices and memory, and for compilation to such\ndevices, JAX calls out to the open [XLA\ncompiler](https://openxla.org/) and the [PJRT\nruntime](https://github.com/openxla/xla/tree/main/xla/pjrt/c#pjrt---uniform-device-api). Both\nof these are projects external to the JAX core, governed and\nmaintained by OpenXLA (again, with frequent contributions from and\ndiscussion with the JAX core developers).\n\nXLA aims for interoperability across accelerators (e.g. by ingesting\n[StableHLO](https://openxla.org/stablehlo) as input) and PJRT offers\nextensibility through a plug-in device API. Adding support for new\ndevices is done by implementing a backend lowering for XLA, and\nimplementing a plug-in device API defined by PJRT. If you're looking\nto contribute to compilation, or to supporting new hardware, we\nencourage you to contribute at the XLA and PJRT layers.\n\nThese open system components allow third parties to support JAX on new\naccelerator platforms, *without requiring changes in the JAX\ncore*. There are several plug-ins in development today. For example, a\nteam at Apple is working on a PJRT plug-in to get [JAX running on\nApple Metal](https://developer.apple.com/metal/jax/).\n", "docs/advanced-autodiff.md": "---\njupytext:\n  formats: md:myst\n  text_representation:\n    extension: .md\n    format_name: myst\n    format_version: 0.13\n    jupytext_version: 1.16.4\nkernelspec:\n  display_name: Python 3\n  language: python\n  name: python3\n---\n\n(advanced-autodiff)=\n# Advanced automatic differentiation\n\n<!--* freshness: { reviewed: '2024-05-14' } *-->\n\nIn this tutorial, you will learn about complex applications of automatic differentiation (autodiff) in JAX and gain a better understanding of how taking derivatives in JAX can be both easy and powerful.\n\nMake sure to check out the {ref}`automatic-differentiation` tutorial to go over the JAX autodiff basics, if you haven't already.\n\n## Setup\n\n```{code-cell}\nimport jax\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random\n\nkey = random.key(0)\n```\n\n## Taking gradients (part 2)\n\n### Higher-order derivatives\n\nJAX's autodiff makes it easy to compute higher-order derivatives, because the functions that compute derivatives are themselves differentiable. Thus, higher-order derivatives are as easy as stacking transformations.\n\nThe single-variable case was covered in the {ref}`automatic-differentiation` tutorial, where the example showed how to use {func}`jax.grad` to compute the derivative of $f(x) = x^3 + 2x^2 - 3x + 1$.\n\nIn the multivariable case, higher-order derivatives are more complicated. The second-order derivative of a function is represented by its [Hessian matrix](https://en.wikipedia.org/wiki/Hessian_matrix), defined according to:\n\n$$(\\mathbf{H}f)_{i,j} = \\frac{\\partial^2 f}{\\partial_i\\partial_j}.$$\n\nThe Hessian of a real-valued function of several variables, $f: \\mathbb R^n\\to\\mathbb R$, can be identified with the [Jacobian](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant) of its gradient.\n\nJAX provides two transformations for computing the Jacobian of a function, {func}`jax.jacfwd` and {func}`jax.jacrev`, corresponding to forward- and reverse-mode autodiff. They give the same answer, but one can be more efficient than the other in different circumstances \u2013 refer to the [video about autodiff](https://www.youtube.com/watch?v=wG_nF1awSSY).\n\n```{code-cell}\ndef hessian(f):\n  return jax.jacfwd(jax.grad(f))\n```\n\nLet's double check this is correct on the dot-product $f: \\mathbf{x} \\mapsto \\mathbf{x} ^\\top \\mathbf{x}$.\n\nif $i=j$, $\\frac{\\partial^2 f}{\\partial_i\\partial_j}(\\mathbf{x}) = 2$. Otherwise, $\\frac{\\partial^2 f}{\\partial_i\\partial_j}(\\mathbf{x}) = 0$.\n\n```{code-cell}\ndef f(x):\n  return jnp.dot(x, x)\n\nhessian(f)(jnp.array([1., 2., 3.]))\n```\n\n## Higher-order optimization\n\nSome meta-learning techniques, such as Model-Agnostic Meta-Learning ([MAML](https://arxiv.org/abs/1703.03400)), require differentiating through gradient updates. In other frameworks this can be quite cumbersome, but in JAX it's much easier:\n\n```python\ndef meta_loss_fn(params, data):\n  \"\"\"Computes the loss after one step of SGD.\"\"\"\n  grads = jax.grad(loss_fn)(params, data)\n  return loss_fn(params - lr * grads, data)\n\nmeta_grads = jax.grad(meta_loss_fn)(params, data)\n```\n\n(stopping-gradients)=\n### Stopping gradients\n\nAutodiff enables automatic computation of the gradient of a function with respect to its inputs. Sometimes, however, you might want some additional control: for instance, you might want to avoid backpropagating gradients through some subset of the computational graph.\n\nConsider for instance the TD(0) ([temporal difference](https://en.wikipedia.org/wiki/Temporal_difference_learning)) reinforcement learning update. This is used to learn to estimate the *value* of a state in an environment from experience of interacting with the environment. Let's assume the value estimate $v_{\\theta}(s_{t-1}$) in a state $s_{t-1}$ is parameterised by a linear function.\n\n```{code-cell}\n# Value function and initial parameters\nvalue_fn = lambda theta, state: jnp.dot(theta, state)\ntheta = jnp.array([0.1, -0.1, 0.])\n```\n\nConsider a transition from a state $s_{t-1}$ to a state $s_t$ during which you observed the reward $r_t$\n\n```{code-cell}\n# An example transition.\ns_tm1 = jnp.array([1., 2., -1.])\nr_t = jnp.array(1.)\ns_t = jnp.array([2., 1., 0.])\n```\n\nThe TD(0) update to the network parameters is:\n\n$$\n\\Delta \\theta = (r_t + v_{\\theta}(s_t) - v_{\\theta}(s_{t-1})) \\nabla v_{\\theta}(s_{t-1})\n$$\n\nThis update is not the gradient of any loss function.\n\nHowever, it can be **written** as the gradient of the pseudo loss function\n\n$$\nL(\\theta) = - \\frac{1}{2} [r_t + v_{\\theta}(s_t) - v_{\\theta}(s_{t-1})]^2\n$$\n\nif the dependency of the target $r_t + v_{\\theta}(s_t)$ on the parameter $\\theta$ is ignored.\n\nHow can you implement this in JAX? If you write the pseudo loss naively, you get:\n\n```{code-cell}\ndef td_loss(theta, s_tm1, r_t, s_t):\n  v_tm1 = value_fn(theta, s_tm1)\n  target = r_t + value_fn(theta, s_t)\n  return -0.5 * ((target - v_tm1) ** 2)\n\ntd_update = jax.grad(td_loss)\ndelta_theta = td_update(theta, s_tm1, r_t, s_t)\n\ndelta_theta\n```\n\nBut `td_update` will **not** compute a TD(0) update, because the gradient computation will include the dependency of `target` on $\\theta$.\n\nYou can use {func}`jax.lax.stop_gradient` to force JAX to ignore the dependency of the target on $\\theta$:\n\n```{code-cell}\ndef td_loss(theta, s_tm1, r_t, s_t):\n  v_tm1 = value_fn(theta, s_tm1)\n  target = r_t + value_fn(theta, s_t)\n  return -0.5 * ((jax.lax.stop_gradient(target) - v_tm1) ** 2)\n\ntd_update = jax.grad(td_loss)\ndelta_theta = td_update(theta, s_tm1, r_t, s_t)\n\ndelta_theta\n```\n\nThis will treat `target` as if it did **not** depend on the parameters $\\theta$ and compute the correct update to the parameters.\n\nNow, let's also calculate $\\Delta \\theta$ using the original TD(0) update expression, to cross-check our work. You may wish to try and implement this yourself using {func}`jax.grad` and your knowledge so far. Here's our solution:\n\n```{code-cell}\ns_grad = jax.grad(value_fn)(theta, s_tm1)\ndelta_theta_original_calculation = (r_t + value_fn(theta, s_t) - value_fn(theta, s_tm1)) * s_grad\n\ndelta_theta_original_calculation # [1.2, 2.4, -1.2], same as `delta_theta`\n```\n\n`jax.lax.stop_gradient` may also be useful in other settings, for instance if you want the gradient from some loss to only affect a subset of the parameters of the neural network (because, for instance, the other parameters are trained using a different loss).\n\n\n### Straight-through estimator using `stop_gradient`\n\nThe straight-through estimator is a trick for defining a 'gradient' of a function that is otherwise non-differentiable. Given a non-differentiable function $f : \\mathbb{R}^n \\to \\mathbb{R}^n$ that is used as part of a larger function that we wish to find a gradient of, we simply pretend during the backward pass that $f$ is the identity function. This can be implemented neatly using `jax.lax.stop_gradient`:\n\n```{code-cell}\ndef f(x):\n  return jnp.round(x)  # non-differentiable\n\ndef straight_through_f(x):\n  # Create an exactly-zero expression with Sterbenz lemma that has\n  # an exactly-one gradient.\n  zero = x - jax.lax.stop_gradient(x)\n  return zero + jax.lax.stop_gradient(f(x))\n\nprint(\"f(x): \", f(3.2))\nprint(\"straight_through_f(x):\", straight_through_f(3.2))\n\nprint(\"grad(f)(x):\", jax.grad(f)(3.2))\nprint(\"grad(straight_through_f)(x):\", jax.grad(straight_through_f)(3.2))\n```\n\n### Per-example gradients\n\nWhile most ML systems compute gradients and updates from batches of data, for reasons of computational efficiency and/or variance reduction, it is sometimes necessary to have access to the gradient/update associated with each specific sample in the batch.\n\nFor instance, this is needed to prioritize data based on gradient magnitude, or to apply clipping / normalisations on a sample by sample basis.\n\nIn many frameworks (PyTorch, TF, Theano) it is often not trivial to compute per-example gradients, because the library directly accumulates the gradient over the batch. Naive workarounds, such as computing a separate loss per example and then aggregating the resulting gradients are typically very inefficient.\n\nIn JAX, you can define the code to compute the gradient per-sample in an easy but efficient way.\n\nJust combine the {func}`jax.jit`, {func}`jax.vmap` and {func}`jax.grad` transformations together:\n\n```{code-cell}\nperex_grads = jax.jit(jax.vmap(jax.grad(td_loss), in_axes=(None, 0, 0, 0)))\n\n# Test it:\nbatched_s_tm1 = jnp.stack([s_tm1, s_tm1])\nbatched_r_t = jnp.stack([r_t, r_t])\nbatched_s_t = jnp.stack([s_t, s_t])\n\nperex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t)\n```\n\nLet's go through this one transformation at a time.\n\nFirst, you apply {func}`jax.grad` to `td_loss` to obtain a function that computes the gradient of the loss w.r.t. the parameters on single (unbatched) inputs:\n\n```{code-cell}\ndtdloss_dtheta = jax.grad(td_loss)\n\ndtdloss_dtheta(theta, s_tm1, r_t, s_t)\n```\n\nThis function computes one row of the array above.\n\nThen, you vectorise this function using {func}`jax.vmap`. This adds a batch dimension to all inputs and outputs. Now, given a batch of inputs, you produce a batch of outputs \u2014 each output in the batch corresponds to the gradient for the corresponding member of the input batch.\n\n```{code-cell}\nalmost_perex_grads = jax.vmap(dtdloss_dtheta)\n\nbatched_theta = jnp.stack([theta, theta])\nalmost_perex_grads(batched_theta, batched_s_tm1, batched_r_t, batched_s_t)\n```\n\nThis isn't quite what we want, because we have to manually feed this function a batch of `theta`s, whereas we actually want to use a single `theta`. We fix this by adding `in_axes` to the {func}`jax.vmap`, specifying theta as `None`, and the other args as `0`. This makes the resulting function add an extra axis only to the other arguments, leaving `theta` unbatched, as we want:\n\n```{code-cell}\ninefficient_perex_grads = jax.vmap(dtdloss_dtheta, in_axes=(None, 0, 0, 0))\n\ninefficient_perex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t)\n```\n\nThis does what we want, but is slower than it has to be. Now, you wrap the whole thing in a {func}`jax.jit` to get the compiled, efficient version of the same function:\n\n```{code-cell}\nperex_grads = jax.jit(inefficient_perex_grads)\n\nperex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t)\n```\n\n```{code-cell}\n%timeit inefficient_perex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t).block_until_ready()\n%timeit perex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t).block_until_ready()\n```\n\n### Hessian-vector products with `jax.grad`-of-`jax.grad`\n\nOne thing you can do with higher-order {func}`jax.grad` is build a Hessian-vector product function. (Later on you'll write an even more efficient implementation that mixes both forward- and reverse-mode, but this one will use pure reverse-mode.)\n\nA Hessian-vector product function can be useful in a [truncated Newton Conjugate-Gradient algorithm](https://en.wikipedia.org/wiki/Truncated_Newton_method) for minimizing smooth convex functions, or for studying the curvature of neural network training objectives (e.g. [1](https://arxiv.org/abs/1406.2572), [2](https://arxiv.org/abs/1811.07062), [3](https://arxiv.org/abs/1706.04454), [4](https://arxiv.org/abs/1802.03451)).\n\nFor a scalar-valued function $f : \\mathbb{R}^n \\to \\mathbb{R}$ with continuous second derivatives (so that the Hessian matrix is symmetric), the Hessian at a point $x \\in \\mathbb{R}^n$ is written as $\\partial^2 f(x)$. A Hessian-vector product function is then able to evaluate\n\n$\\qquad v \\mapsto \\partial^2 f(x) \\cdot v$\n\nfor any $v \\in \\mathbb{R}^n$.\n\nThe trick is not to instantiate the full Hessian matrix: if $n$ is large, perhaps in the millions or billions in the context of neural networks, then that might be impossible to store.\n\nLuckily, {func}`jax.grad` already gives us a way to write an efficient Hessian-vector product function. You just have to use the identity:\n\n$\\qquad \\partial^2 f (x) v = \\partial [x \\mapsto \\partial f(x) \\cdot v] = \\partial g(x)$,\n\nwhere $g(x) = \\partial f(x) \\cdot v$ is a new scalar-valued function that dots the gradient of $f$ at $x$ with the vector $v$. Notice that you're only ever differentiating scalar-valued functions of vector-valued arguments, which is exactly where you know {func}`jax.grad` is efficient.\n\nIn JAX code, you can just write this:\n\n```{code-cell}\ndef hvp(f, x, v):\n    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)\n```\n\nThis example shows that you can freely use lexical closure, and JAX will never get perturbed or confused.\n\nYou will check this implementation a few cells down, once you learn how to compute dense Hessian matrices. You'll also write an even better version that uses both forward-mode and reverse-mode.\n\n\n### Jacobians and Hessians using `jax.jacfwd` and `jax.jacrev`\n\nYou can compute full Jacobian matrices using the {func}`jax.jacfwd` and {func}`jax.jacrev` functions:\n\n```{code-cell}\nfrom jax import jacfwd, jacrev\n\n# Define a sigmoid function.\ndef sigmoid(x):\n    return 0.5 * (jnp.tanh(x / 2) + 1)\n\n# Outputs probability of a label being true.\ndef predict(W, b, inputs):\n    return sigmoid(jnp.dot(inputs, W) + b)\n\n# Build a toy dataset.\ninputs = jnp.array([[0.52, 1.12,  0.77],\n                   [0.88, -1.08, 0.15],\n                   [0.52, 0.06, -1.30],\n                   [0.74, -2.49, 1.39]])\n\n# Initialize random model coefficients\nkey, W_key, b_key = random.split(key, 3)\nW = random.normal(W_key, (3,))\nb = random.normal(b_key, ())\n\n# Isolate the function from the weight matrix to the predictions\nf = lambda W: predict(W, b, inputs)\n\nJ = jacfwd(f)(W)\nprint(\"jacfwd result, with shape\", J.shape)\nprint(J)\n\nJ = jacrev(f)(W)\nprint(\"jacrev result, with shape\", J.shape)\nprint(J)\n```\n\nThese two functions compute the same values (up to machine numerics), but differ in their implementation: {func}`jax.jacfwd` uses forward-mode automatic differentiation, which is more efficient for \"tall\" Jacobian matrices (more outputs than inputs), while {func}`jax.jacrev` uses reverse-mode, which is more efficient for \"wide\" Jacobian matrices (more inputs than outputs). For matrices that are near-square, {func}`jax.jacfwd` probably has an edge over {func}`jax.jacrev`.\n\nYou can also use {func}`jax.jacfwd` and {func}`jax.jacrev` with container types:\n\n```{code-cell}\ndef predict_dict(params, inputs):\n    return predict(params['W'], params['b'], inputs)\n\nJ_dict = jacrev(predict_dict)({'W': W, 'b': b}, inputs)\nfor k, v in J_dict.items():\n    print(\"Jacobian from {} to logits is\".format(k))\n    print(v)\n```\n\nFor more details on forward- and reverse-mode, as well as how to implement {func}`jax.jacfwd` and {func}`jax.jacrev` as efficiently as possible, read on!\n\nUsing a composition of two of these functions gives us a way to compute dense Hessian matrices:\n\n```{code-cell}\ndef hessian(f):\n    return jacfwd(jacrev(f))\n\nH = hessian(f)(W)\nprint(\"hessian, with shape\", H.shape)\nprint(H)\n```\n\nThis shape makes sense: if you start with a function $f : \\mathbb{R}^n \\to \\mathbb{R}^m$, then at a point $x \\in \\mathbb{R}^n$ you expect to get the shapes:\n\n* $f(x) \\in \\mathbb{R}^m$, the value of $f$ at $x$,\n* $\\partial f(x) \\in \\mathbb{R}^{m \\times n}$, the Jacobian matrix at $x$,\n* $\\partial^2 f(x) \\in \\mathbb{R}^{m \\times n \\times n}$, the Hessian at $x$,\n\nand so on.\n\nTo implement `hessian`, you could have used `jacfwd(jacrev(f))` or `jacrev(jacfwd(f))` or any other composition of these two. But forward-over-reverse is typically the most efficient. That's because in the inner Jacobian computation we're often differentiating a function wide Jacobian (maybe like a loss function $f : \\mathbb{R}^n \\to \\mathbb{R}$), while in the outer Jacobian computation we're differentiating a function with a square Jacobian (since $\\nabla f : \\mathbb{R}^n \\to \\mathbb{R}^n$), which is where forward-mode wins out.\n\n\n## How it's made: Two foundational autodiff functions\n\n### Jacobian-Vector products (JVPs, a.k.a. forward-mode autodiff)\n\nJAX includes efficient and general implementations of both forward- and reverse-mode automatic differentiation. The familiar {func}`jax.grad` function is built on reverse-mode, but to explain the difference between the two modes, and when each can be useful, you need a bit of math background.\n\n\n#### JVPs in math\n\nMathematically, given a function $f : \\mathbb{R}^n \\to \\mathbb{R}^m$, the Jacobian of $f$ evaluated at an input point $x \\in \\mathbb{R}^n$, denoted $\\partial f(x)$, is often thought of as a matrix in $\\mathbb{R}^m \\times \\mathbb{R}^n$:\n\n$\\qquad \\partial f(x) \\in \\mathbb{R}^{m \\times n}$.\n\nBut you can also think of $\\partial f(x)$ as a linear map, which maps the tangent space of the domain of $f$ at the point $x$ (which is just another copy of $\\mathbb{R}^n$) to the tangent space of the codomain of $f$ at the point $f(x)$ (a copy of $\\mathbb{R}^m$):\n\n$\\qquad \\partial f(x) : \\mathbb{R}^n \\to \\mathbb{R}^m$.\n\nThis map is called the [pushforward map](https://en.wikipedia.org/wiki/Pushforward_(differential)) of $f$ at $x$. The Jacobian matrix is just the matrix for this linear map on a standard basis.\n\nIf you don't commit to one specific input point $x$, then you can think of the function $\\partial f$ as first taking an input point and returning the Jacobian linear map at that input point:\n\n$\\qquad \\partial f : \\mathbb{R}^n \\to \\mathbb{R}^n \\to \\mathbb{R}^m$.\n\nIn particular, you can uncurry things so that given input point $x \\in \\mathbb{R}^n$ and a tangent vector $v \\in \\mathbb{R}^n$, you get back an output tangent vector in $\\mathbb{R}^m$. We call that mapping, from $(x, v)$ pairs to output tangent vectors, the *Jacobian-vector product*, and write it as:\n\n$\\qquad (x, v) \\mapsto \\partial f(x) v$\n\n\n#### JVPs in JAX code\n\nBack in Python code, JAX's {func}`jax.jvp` function models this transformation. Given a Python function that evaluates $f$, JAX's {func}`jax.jvp` is a way to get a Python function for evaluating $(x, v) \\mapsto (f(x), \\partial f(x) v)$.\n\n```{code-cell}\nfrom jax import jvp\n\n# Isolate the function from the weight matrix to the predictions\nf = lambda W: predict(W, b, inputs)\n\nkey, subkey = random.split(key)\nv = random.normal(subkey, W.shape)\n\n# Push forward the vector `v` along `f` evaluated at `W`\ny, u = jvp(f, (W,), (v,))\n```\n\nIn terms of [Haskell-like type signatures](https://wiki.haskell.org/Type_signature), you could write:\n\n```haskell\njvp :: (a -> b) -> a -> T a -> (b, T b)\n```\n\nwhere `T a` is used to denote the type of the tangent space for `a`.\n\nIn other words, `jvp` takes as arguments a function of type `a -> b`, a value of type `a`, and a tangent vector value of type `T a`. It gives back a pair consisting of a value of type `b` and an output tangent vector of type `T b`.\n\nThe `jvp`-transformed function is evaluated much like the original function, but paired up with each primal value of type `a` it pushes along tangent values of type `T a`. For each primitive numerical operation that the original function would have applied, the `jvp`-transformed function executes a \"JVP rule\" for that primitive that both evaluates the primitive on the primals and applies the primitive's JVP at those primal values.\n\nThat evaluation strategy has some immediate implications about computational complexity. Since we evaluate JVPs as we go, we don't need to store anything for later, and so the memory cost is independent of the depth of the computation. In addition, the FLOP cost of the `jvp`-transformed function is about 3x the cost of just evaluating the function (one unit of work for evaluating the original function, for example `sin(x)`; one unit for linearizing, like `cos(x)`; and one unit for applying the linearized function to a vector, like `cos_x * v`). Put another way, for a fixed primal point $x$, we can evaluate $v \\mapsto \\partial f(x) \\cdot v$ for about the same marginal cost as evaluating $f$.\n\nThat memory complexity sounds pretty compelling! So why don't we see forward-mode very often in machine learning?\n\nTo answer that, first think about how you could use a JVP to build a full Jacobian matrix. If we apply a JVP to a one-hot tangent vector, it reveals one column of the Jacobian matrix, corresponding to the nonzero entry we fed in. So we can build a full Jacobian one column at a time, and to get each column costs about the same as one function evaluation. That will be efficient for functions with \"tall\" Jacobians, but inefficient for \"wide\" Jacobians.\n\nIf you're doing gradient-based optimization in machine learning, you probably want to minimize a loss function from parameters in $\\mathbb{R}^n$ to a scalar loss value in $\\mathbb{R}$. That means the Jacobian of this function is a very wide matrix: $\\partial f(x) \\in \\mathbb{R}^{1 \\times n}$, which we often identify with the Gradient vector $\\nabla f(x) \\in \\mathbb{R}^n$. Building that matrix one column at a time, with each call taking a similar number of FLOPs to evaluate the original function, sure seems inefficient! In particular, for training neural networks, where $f$ is a training loss function and $n$ can be in the millions or billions, this approach just won't scale.\n\nTo do better for functions like this, you just need to use reverse-mode.\n\n\n### Vector-Jacobian products (VJPs, a.k.a. reverse-mode autodiff)\n\nWhere forward-mode gives us back a function for evaluating Jacobian-vector products, which we can then use to build Jacobian matrices one column at a time, reverse-mode is a way to get back a function for evaluating vector-Jacobian products (equivalently Jacobian-transpose-vector products), which we can use to build Jacobian matrices one row at a time.\n\n\n#### VJPs in math\n\nLet's again consider a function $f : \\mathbb{R}^n \\to \\mathbb{R}^m$.\nStarting from our notation for JVPs, the notation for VJPs is pretty simple:\n\n$\\qquad (x, v) \\mapsto v \\partial f(x)$,\n\nwhere $v$ is an element of the cotangent space of $f$ at $x$ (isomorphic to another copy of $\\mathbb{R}^m$). When being rigorous, we should think of $v$ as a linear map $v : \\mathbb{R}^m \\to \\mathbb{R}$, and when we write $v \\partial f(x)$ we mean function composition $v \\circ \\partial f(x)$, where the types work out because $\\partial f(x) : \\mathbb{R}^n \\to \\mathbb{R}^m$. But in the common case we can identify $v$ with a vector in $\\mathbb{R}^m$ and use the two almost interchangeably, just like we might sometimes flip between \"column vectors\" and \"row vectors\" without much comment.\n\nWith that identification, we can alternatively think of the linear part of a VJP as the transpose (or adjoint conjugate) of the linear part of a JVP:\n\n$\\qquad (x, v) \\mapsto \\partial f(x)^\\mathsf{T} v$.\n\nFor a given point $x$, we can write the signature as\n\n$\\qquad \\partial f(x)^\\mathsf{T} : \\mathbb{R}^m \\to \\mathbb{R}^n$.\n\nThe corresponding map on cotangent spaces is often called the [pullback](https://en.wikipedia.org/wiki/Pullback_(differential_geometry))\nof $f$ at $x$. The key for our purposes is that it goes from something that looks like the output of $f$ to something that looks like the input of $f$, just like we might expect from a transposed linear function.\n\n#### VJPs in JAX code\n\nSwitching from math back to Python, the JAX function `vjp` can take a Python function for evaluating $f$ and give us back a Python function for evaluating the VJP $(x, v) \\mapsto (f(x), v^\\mathsf{T} \\partial f(x))$.\n\n```{code-cell}\nfrom jax import vjp\n\n# Isolate the function from the weight matrix to the predictions\nf = lambda W: predict(W, b, inputs)\n\ny, vjp_fun = vjp(f, W)\n\nkey, subkey = random.split(key)\nu = random.normal(subkey, y.shape)\n\n# Pull back the covector `u` along `f` evaluated at `W`\nv = vjp_fun(u)\n```\n\nIn terms of [Haskell-like type signatures](https://wiki.haskell.org/Type_signature), we could write\n\n```haskell\nvjp :: (a -> b) -> a -> (b, CT b -> CT a)\n```\n\nwhere we use `CT a` to denote the type for the cotangent space for `a`. In words, `vjp` takes as arguments a function of type `a -> b` and a point of type `a`, and gives back a pair consisting of a value of type `b` and a linear map of type `CT b -> CT a`.\n\nThis is great because it lets us build Jacobian matrices one row at a time, and the FLOP cost for evaluating $(x, v) \\mapsto (f(x), v^\\mathsf{T} \\partial f(x))$ is only about three times the cost of evaluating $f$. In particular, if we want the gradient of a function $f : \\mathbb{R}^n \\to \\mathbb{R}$, we can do it in just one call. That's how {func}`jax.grad` is efficient for gradient-based optimization, even for objectives like neural network training loss functions on millions or billions of parameters.\n\nThere's a cost, though the FLOPs are friendly, memory scales with the depth of the computation. Also, the implementation is traditionally more complex than that of forward-mode, though JAX has some tricks up its sleeve (that's a story for a future notebook!).\n\nFor more on how reverse-mode works, check out [this tutorial video from the Deep Learning Summer School in 2017](http://videolectures.net/deeplearning2017_johnson_automatic_differentiation/).\n\n\n### Vector-valued gradients with VJPs\n\nIf you're interested in taking vector-valued gradients (like `tf.gradients`):\n\n```{code-cell}\ndef vgrad(f, x):\n  y, vjp_fn = vjp(f, x)\n  return vjp_fn(jnp.ones(y.shape))[0]\n\nprint(vgrad(lambda x: 3*x**2, jnp.ones((2, 2))))\n```\n\n### Hessian-vector products using both forward- and reverse-mode\n\nIn a previous section, you implemented a Hessian-vector product function just using reverse-mode (assuming continuous second derivatives):\n\n```{code-cell}\ndef hvp(f, x, v):\n    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)\n```\n\nThat's efficient, but you can do even better and save some memory by using forward-mode together with reverse-mode.\n\nMathematically, given a function $f : \\mathbb{R}^n \\to \\mathbb{R}$ to differentiate, a point $x \\in \\mathbb{R}^n$ at which to linearize the function, and a vector $v \\in \\mathbb{R}^n$, the Hessian-vector product function we want is:\n\n$(x, v) \\mapsto \\partial^2 f(x) v$\n\nConsider the helper function $g : \\mathbb{R}^n \\to \\mathbb{R}^n$ defined to be the derivative (or gradient) of $f$, namely $g(x) = \\partial f(x)$. All you need is its JVP, since that will give us:\n\n$(x, v) \\mapsto \\partial g(x) v = \\partial^2 f(x) v$.\n\nWe can translate that almost directly into code:\n\n```{code-cell}\n# forward-over-reverse\ndef hvp(f, primals, tangents):\n  return jvp(grad(f), primals, tangents)[1]\n```\n\nEven better, since you didn't have to call {func}`jnp.dot` directly, this `hvp` function works with arrays of any shape and with arbitrary container types (like vectors stored as nested lists/dicts/tuples), and doesn't even have a dependence on {mod}`jax.numpy`.\n\nHere's an example of how to use it:\n\n```{code-cell}\ndef f(X):\n  return jnp.sum(jnp.tanh(X)**2)\n\nkey, subkey1, subkey2 = random.split(key, 3)\nX = random.normal(subkey1, (30, 40))\nV = random.normal(subkey2, (30, 40))\n\nans1 = hvp(f, (X,), (V,))\nans2 = jnp.tensordot(hessian(f)(X), V, 2)\n\nprint(jnp.allclose(ans1, ans2, 1e-4, 1e-4))\n```\n\nAnother way you might consider writing this is using reverse-over-forward:\n\n```{code-cell}\n# Reverse-over-forward\ndef hvp_revfwd(f, primals, tangents):\n  g = lambda primals: jvp(f, primals, tangents)[1]\n  return grad(g)(primals)\n```\n\nThat's not quite as good, though, because forward-mode has less overhead than reverse-mode, and since the outer differentiation operator here has to differentiate a larger computation than the inner one, keeping forward-mode on the outside works best:\n\n```{code-cell}\n# Reverse-over-reverse, only works for single arguments\ndef hvp_revrev(f, primals, tangents):\n  x, = primals\n  v, = tangents\n  return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)\n\n\nprint(\"Forward over reverse\")\n%timeit -n10 -r3 hvp(f, (X,), (V,))\nprint(\"Reverse over forward\")\n%timeit -n10 -r3 hvp_revfwd(f, (X,), (V,))\nprint(\"Reverse over reverse\")\n%timeit -n10 -r3 hvp_revrev(f, (X,), (V,))\n\nprint(\"Naive full Hessian materialization\")\n%timeit -n10 -r3 jnp.tensordot(hessian(f)(X), V, 2)\n```\n\n## Composing VJPs, JVPs, and `jax.vmap`\n\n### Jacobian-Matrix and Matrix-Jacobian products\n\nNow that you have {func}`jax.jvp` and {func}`jax.vjp` transformations that give you functions to push-forward or pull-back single vectors at a time, you can use JAX's {func}`jax.vmap` [transformation](https://github.com/jax-ml/jax#auto-vectorization-with-vmap) to push and pull entire bases at once. In particular, you can use that to write fast matrix-Jacobian and Jacobian-matrix products:\n\n```{code-cell}\n# Isolate the function from the weight matrix to the predictions\nf = lambda W: predict(W, b, inputs)\n\n# Pull back the covectors `m_i` along `f`, evaluated at `W`, for all `i`.\n# First, use a list comprehension to loop over rows in the matrix M.\ndef loop_mjp(f, x, M):\n    y, vjp_fun = vjp(f, x)\n    return jnp.vstack([vjp_fun(mi) for mi in M])\n\n# Now, use vmap to build a computation that does a single fast matrix-matrix\n# multiply, rather than an outer loop over vector-matrix multiplies.\ndef vmap_mjp(f, x, M):\n    y, vjp_fun = vjp(f, x)\n    outs, = vmap(vjp_fun)(M)\n    return outs\n\nkey = random.key(0)\nnum_covecs = 128\nU = random.normal(key, (num_covecs,) + y.shape)\n\nloop_vs = loop_mjp(f, W, M=U)\nprint('Non-vmapped Matrix-Jacobian product')\n%timeit -n10 -r3 loop_mjp(f, W, M=U)\n\nprint('\\nVmapped Matrix-Jacobian product')\nvmap_vs = vmap_mjp(f, W, M=U)\n%timeit -n10 -r3 vmap_mjp(f, W, M=U)\n\nassert jnp.allclose(loop_vs, vmap_vs), 'Vmap and non-vmapped Matrix-Jacobian Products should be identical'\n```\n\n```{code-cell}\ndef loop_jmp(f, W, M):\n    # jvp immediately returns the primal and tangent values as a tuple,\n    # so we'll compute and select the tangents in a list comprehension\n    return jnp.vstack([jvp(f, (W,), (mi,))[1] for mi in M])\n\ndef vmap_jmp(f, W, M):\n    _jvp = lambda s: jvp(f, (W,), (s,))[1]\n    return vmap(_jvp)(M)\n\nnum_vecs = 128\nS = random.normal(key, (num_vecs,) + W.shape)\n\nloop_vs = loop_jmp(f, W, M=S)\nprint('Non-vmapped Jacobian-Matrix product')\n%timeit -n10 -r3 loop_jmp(f, W, M=S)\nvmap_vs = vmap_jmp(f, W, M=S)\nprint('\\nVmapped Jacobian-Matrix product')\n%timeit -n10 -r3 vmap_jmp(f, W, M=S)\n\nassert jnp.allclose(loop_vs, vmap_vs), 'Vmap and non-vmapped Jacobian-Matrix products should be identical'\n```\n\n### The implementation of `jax.jacfwd` and `jax.jacrev`\n\nNow that we've seen fast Jacobian-matrix and matrix-Jacobian products, it's not hard to guess how to write {func}`jax.jacfwd` and {func}`jax.jacrev`. We just use the same technique to push-forward or pull-back an entire standard basis (isomorphic to an identity matrix) at once.\n\n```{code-cell}\nfrom jax import jacrev as builtin_jacrev\n\ndef our_jacrev(f):\n    def jacfun(x):\n        y, vjp_fun = vjp(f, x)\n        # Use vmap to do a matrix-Jacobian product.\n        # Here, the matrix is the Euclidean basis, so we get all\n        # entries in the Jacobian at once.\n        J, = vmap(vjp_fun, in_axes=0)(jnp.eye(len(y)))\n        return J\n    return jacfun\n\nassert jnp.allclose(builtin_jacrev(f)(W), our_jacrev(f)(W)), 'Incorrect reverse-mode Jacobian results!'\n```\n\n```{code-cell}\nfrom jax import jacfwd as builtin_jacfwd\n\ndef our_jacfwd(f):\n    def jacfun(x):\n        _jvp = lambda s: jvp(f, (x,), (s,))[1]\n        Jt = vmap(_jvp, in_axes=1)(jnp.eye(len(x)))\n        return jnp.transpose(Jt)\n    return jacfun\n\nassert jnp.allclose(builtin_jacfwd(f)(W), our_jacfwd(f)(W)), 'Incorrect forward-mode Jacobian results!'\n```\n\nInterestingly, the [Autograd](https://github.com/hips/autograd) library couldn't do this. The [implementation](https://github.com/HIPS/autograd/blob/96a03f44da43cd7044c61ac945c483955deba957/autograd/differential_operators.py#L60) of reverse-mode `jacobian` in Autograd had to pull back one vector at a time with an outer-loop `map`. Pushing one vector at a time through the computation is much less efficient than batching it all together with {func}`jax.vmap`.\n\nAnother thing that Autograd couldn't do is {func}`jax.jit`. Interestingly, no matter how much Python dynamism you use in your function to be differentiated, we could always use {func}`jax.jit` on the linear part of the computation. For example:\n\n```{code-cell}\ndef f(x):\n    try:\n        if x < 3:\n            return 2 * x ** 3\n        else:\n            raise ValueError\n    except ValueError:\n        return jnp.pi * x\n\ny, f_vjp = vjp(f, 4.)\nprint(jit(f_vjp)(1.))\n```\n\n## Complex numbers and differentiation\n\nJAX is great at complex numbers and differentiation. To support both [holomorphic and non-holomorphic differentiation](https://en.wikipedia.org/wiki/Holomorphic_function), it helps to think in terms of JVPs and VJPs.\n\nConsider a complex-to-complex function $f: \\mathbb{C} \\to \\mathbb{C}$ and identify it with a corresponding function $g: \\mathbb{R}^2 \\to \\mathbb{R}^2$,\n\n```{code-cell}\ndef f(z):\n  x, y = jnp.real(z), jnp.imag(z)\n  return u(x, y) + v(x, y) * 1j\n\ndef g(x, y):\n  return (u(x, y), v(x, y))\n```\n\nThat is, we've decomposed $f(z) = u(x, y) + v(x, y) i$ where $z = x + y i$, and identified $\\mathbb{C}$ with $\\mathbb{R}^2$ to get $g$.\n\nSince $g$ only involves real inputs and outputs, we already know how to write a Jacobian-vector product for it, say given a tangent vector $(c, d) \\in \\mathbb{R}^2$, namely:\n\n$\\begin{bmatrix} \\partial_0 u(x, y) & \\partial_1 u(x, y) \\\\ \\partial_0 v(x, y) & \\partial_1 v(x, y) \\end{bmatrix}\n\\begin{bmatrix} c \\\\ d \\end{bmatrix}$.\n\nTo get a JVP for the original function $f$ applied to a tangent vector $c + di \\in \\mathbb{C}$, we just use the same definition and identify the result as another complex number, \n\n$\\partial f(x + y i)(c + d i) =\n\\begin{matrix} \\begin{bmatrix} 1 & i \\end{bmatrix} \\\\ ~ \\end{matrix}\n\\begin{bmatrix} \\partial_0 u(x, y) & \\partial_1 u(x, y) \\\\ \\partial_0 v(x, y) & \\partial_1 v(x, y) \\end{bmatrix}\n\\begin{bmatrix} c \\\\ d \\end{bmatrix}$.\n\nThat's our definition of the JVP of a $\\mathbb{C} \\to \\mathbb{C}$ function! Notice it doesn't matter whether or not $f$ is holomorphic: the JVP is unambiguous.\n\nHere's a check:\n\n```{code-cell}\ndef check(seed):\n  key = random.key(seed)\n\n  # random coeffs for u and v\n  key, subkey = random.split(key)\n  a, b, c, d = random.uniform(subkey, (4,))\n\n  def fun(z):\n    x, y = jnp.real(z), jnp.imag(z)\n    return u(x, y) + v(x, y) * 1j\n\n  def u(x, y):\n    return a * x + b * y\n\n  def v(x, y):\n    return c * x + d * y\n\n  # primal point\n  key, subkey = random.split(key)\n  x, y = random.uniform(subkey, (2,))\n  z = x + y * 1j\n\n  # tangent vector\n  key, subkey = random.split(key)\n  c, d = random.uniform(subkey, (2,))\n  z_dot = c + d * 1j\n\n  # check jvp\n  _, ans = jvp(fun, (z,), (z_dot,))\n  expected = (grad(u, 0)(x, y) * c +\n              grad(u, 1)(x, y) * d +\n              grad(v, 0)(x, y) * c * 1j+\n              grad(v, 1)(x, y) * d * 1j)\n  print(jnp.allclose(ans, expected))\n```\n\n```{code-cell}\ncheck(0)\ncheck(1)\ncheck(2)\n```\n\nWhat about VJPs? We do something pretty similar: for a cotangent vector $c + di \\in \\mathbb{C}$ we define the VJP of $f$ as\n\n$(c + di)^* \\; \\partial f(x + y i) =\n\\begin{matrix} \\begin{bmatrix} c & -d \\end{bmatrix} \\\\ ~ \\end{matrix}\n\\begin{bmatrix} \\partial_0 u(x, y) & \\partial_1 u(x, y) \\\\ \\partial_0 v(x, y) & \\partial_1 v(x, y) \\end{bmatrix}\n\\begin{bmatrix} 1 \\\\ -i \\end{bmatrix}$.\n\nWhat's with the negatives? They're just to take care of complex conjugation, and the fact that we're working with covectors.\n\nHere's a check of the VJP rules:\n\n```{code-cell}\ndef check(seed):\n  key = random.key(seed)\n\n  # random coeffs for u and v\n  key, subkey = random.split(key)\n  a, b, c, d = random.uniform(subkey, (4,))\n\n  def fun(z):\n    x, y = jnp.real(z), jnp.imag(z)\n    return u(x, y) + v(x, y) * 1j\n\n  def u(x, y):\n    return a * x + b * y\n\n  def v(x, y):\n    return c * x + d * y\n\n  # primal point\n  key, subkey = random.split(key)\n  x, y = random.uniform(subkey, (2,))\n  z = x + y * 1j\n\n  # cotangent vector\n  key, subkey = random.split(key)\n  c, d = random.uniform(subkey, (2,))\n  z_bar = jnp.array(c + d * 1j)  # for dtype control\n\n  # check vjp\n  _, fun_vjp = vjp(fun, z)\n  ans, = fun_vjp(z_bar)\n  expected = (grad(u, 0)(x, y) * c +\n              grad(v, 0)(x, y) * (-d) +\n              grad(u, 1)(x, y) * c * (-1j) +\n              grad(v, 1)(x, y) * (-d) * (-1j))\n  assert jnp.allclose(ans, expected, atol=1e-5, rtol=1e-5)\n```\n\n```{code-cell}\ncheck(0)\ncheck(1)\ncheck(2)\n```\n\nWhat about convenience wrappers like {func}`jax.grad`, {func}`jax.jacfwd`, and {func}`jax.jacrev`?\n\nFor $\\mathbb{R} \\to \\mathbb{R}$ functions, recall we defined `grad(f)(x)` as being `vjp(f, x)[1](1.0)`, which works because applying a VJP to a `1.0` value reveals the gradient (i.e. Jacobian, or derivative). We can do the same thing for $\\mathbb{C} \\to \\mathbb{R}$ functions: we can still use `1.0` as the cotangent vector, and we just get out a complex number result summarizing the full Jacobian:\n\n```{code-cell}\ndef f(z):\n  x, y = jnp.real(z), jnp.imag(z)\n  return x**2 + y**2\n\nz = 3. + 4j\ngrad(f)(z)\n```\n\nFor general $\\mathbb{C} \\to \\mathbb{C}$ functions, the Jacobian has 4 real-valued degrees of freedom (as in the 2x2 Jacobian matrices above), so we can't hope to represent all of them within a complex number. But we can for holomorphic functions! A holomorphic function is precisely a $\\mathbb{C} \\to \\mathbb{C}$ function with the special property that its derivative can be represented as a single complex number. (The [Cauchy-Riemann equations](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Riemann_equations) ensure that the above 2x2 Jacobians have the special form of a scale-and-rotate matrix in the complex plane, i.e. the action of a single complex number under multiplication.) And we can reveal that one complex number using a single call to `vjp` with a covector of `1.0`.\n\nBecause this only works for holomorphic functions, to use this trick we need to promise JAX that our function is holomorphic; otherwise, JAX will raise an error when {func}`jax.grad` is used for a complex-output function:\n\n```{code-cell}\ndef f(z):\n  return jnp.sin(z)\n\nz = 3. + 4j\ngrad(f, holomorphic=True)(z)\n```\n\nAll the `holomorphic=True` promise does is disable the error when the output is complex-valued. We can still write `holomorphic=True` when the function isn't holomorphic, but the answer we get out won't represent the full Jacobian. Instead, it'll be the Jacobian of the function where we just discard the imaginary part of the output:\n\n```{code-cell}\ndef f(z):\n  return jnp.conjugate(z)\n\nz = 3. + 4j\ngrad(f, holomorphic=True)(z)  # f is not actually holomorphic!\n```\n\nThere are some useful upshots for how {func}`jax.grad` works here:\n\n1. We can use {func}`jax.grad` on holomorphic $\\mathbb{C} \\to \\mathbb{C}$ functions.\n2. We can use {func}`jax.grad` to optimize $f : \\mathbb{C} \\to \\mathbb{R}$ functions, like real-valued loss functions of complex parameters `x`, by taking steps in the direction of the conjugate of `grad(f)(x)`.\n3. If we have an $\\mathbb{R} \\to \\mathbb{R}$ function that just happens to use some complex-valued operations internally (some of which must be non-holomorphic, e.g. FFTs used in convolutions) then {func}`jax.grad` still works and we get the same result that an implementation using only real values would have given.\n\nIn any case, JVPs and VJPs are always unambiguous. And if we wanted to compute the full Jacobian matrix of a non-holomorphic $\\mathbb{C} \\to \\mathbb{C}$ function, we can do it with JVPs or VJPs!\n\n\nYou should expect complex numbers to work everywhere in JAX. Here's differentiating through a Cholesky decomposition of a complex matrix:\n\n```{code-cell}\nA = jnp.array([[5.,    2.+3j,    5j],\n              [2.-3j,   7.,  1.+7j],\n              [-5j,  1.-7j,    12.]])\n\ndef f(X):\n    L = jnp.linalg.cholesky(X)\n    return jnp.sum((L - jnp.sin(L))**2)\n\ngrad(f, holomorphic=True)(A)\n```\n\n(advanced-autodiff-custom-derivative-rules)=\n## Custom derivative rules for JAX-transformable Python functions\n\nThere are two ways to define differentiation rules in JAX:\n\n1. Using {func}`jax.custom_jvp` and {func}`jax.custom_vjp` to define custom differentiation rules for Python functions that are already JAX-transformable; and\n2. Defining new `core.Primitive` instances along with all their transformation rules, for example to call into functions from other systems like solvers, simulators, or general numerical computing systems.\n\nThis notebook is about #1. To read instead about #2, refer to the [notebook on adding primitives](https://docs.jax.dev/en/latest/notebooks/How_JAX_primitives_work.html).\n\n\n### TL;DR: Custom JVPs with {func}`jax.custom_jvp`\n\n```{code-cell}\nfrom jax import custom_jvp\n\n@custom_jvp\ndef f(x, y):\n  return jnp.sin(x) * y\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n  x, y = primals\n  x_dot, y_dot = tangents\n  primal_out = f(x, y)\n  tangent_out = jnp.cos(x) * x_dot * y + jnp.sin(x) * y_dot\n  return primal_out, tangent_out\n```\n\n```{code-cell}\nprint(f(2., 3.))\ny, y_dot = jvp(f, (2., 3.), (1., 0.))\nprint(y)\nprint(y_dot)\nprint(grad(f)(2., 3.))\n```\n\n```{code-cell}\n# Equivalent alternative using the `defjvps` convenience wrapper\n\n@custom_jvp\ndef f(x, y):\n  return jnp.sin(x) * y\n\nf.defjvps(lambda x_dot, primal_out, x, y: jnp.cos(x) * x_dot * y,\n          lambda y_dot, primal_out, x, y: jnp.sin(x) * y_dot)\n```\n\n```{code-cell}\nprint(f(2., 3.))\ny, y_dot = jvp(f, (2., 3.), (1., 0.))\nprint(y)\nprint(y_dot)\nprint(grad(f)(2., 3.))\n```\n\n### TL;DR: Custom VJPs with `jax.custom_vjp`\n\n```{code-cell}\nfrom jax import custom_vjp\n\n@custom_vjp\ndef f(x, y):\n  return jnp.sin(x) * y\n\ndef f_fwd(x, y):\n# Returns primal output and residuals to be used in backward pass by `f_bwd`.\n  return f(x, y), (jnp.cos(x), jnp.sin(x), y)\n\ndef f_bwd(res, g):\n  cos_x, sin_x, y = res # Gets residuals computed in `f_fwd`\n  return (cos_x * g * y, sin_x * g)\n\nf.defvjp(f_fwd, f_bwd)\n```\n\n```{code-cell}\nprint(grad(f)(2., 3.))\n```\n\n### Example problems\n\nTo get an idea of what problems {func}`jax.custom_jvp` and {func}`jax.custom_vjp` are meant to solve, let's go over a few examples. A more thorough introduction to the {func}`jax.custom_jvp` and {func}`jax.custom_vjp` APIs is in the next section.\n\n\n#### Example: Numerical stability\n\nOne application of {func}`jax.custom_jvp` is to improve the numerical stability of differentiation.\n\nSay we want to write a function called `log1pexp`, which computes $x \\mapsto \\log ( 1 + e^x )$. We can write that using `jax.numpy`:\n\n```{code-cell}\ndef log1pexp(x):\n  return jnp.log(1. + jnp.exp(x))\n\nlog1pexp(3.)\n```\n\nSince it's written in terms of `jax.numpy`, it's JAX-transformable:\n\n```{code-cell}\nprint(jit(log1pexp)(3.))\nprint(jit(grad(log1pexp))(3.))\nprint(vmap(jit(grad(log1pexp)))(jnp.arange(3.)))\n```\n\nBut there's a numerical stability problem lurking here:\n\n```{code-cell}\nprint(grad(log1pexp)(100.))\n```\n\nThat doesn't seem right! After all, the derivative of $x \\mapsto \\log (1 + e^x)$ is $x \\mapsto \\frac{e^x}{1 + e^x}$, and so for large values of $x$ we'd expect the value to be about 1.\n\nWe can get a bit more insight into what's going on by looking at the jaxpr for the gradient computation:\n\n```{code-cell}\nfrom jax import make_jaxpr\n\nmake_jaxpr(grad(log1pexp))(100.)\n```\n\nStepping through how the jaxpr would be evaluated, notice that the last line would involve multiplying values that floating point math will round to 0 and $\\infty$, respectively, which is never a good idea. That is, we're effectively evaluating `lambda x: (1 / (1 + jnp.exp(x))) * jnp.exp(x)` for large `x`, which effectively turns into `0. * jnp.inf`.\n\nInstead of generating such large and small values, hoping for a cancellation that floats can't always provide, we'd rather just express the derivative function as a more numerically stable program. In particular, we can write a program that more closely evaluates the equal mathematical expression $1 - \\frac{1}{1 + e^x}$, with no cancellation in sight.\n\nThis problem is interesting because even though our definition of `log1pexp` could already be JAX-differentiated (and transformed with {func}`jax.jit`, {func}`jax.vmap`, ...), we're not happy with the result of applying standard autodiff rules to the primitives comprising `log1pexp` and composing the result. Instead, we'd like to specify how the whole function `log1pexp` should be differentiated, as a unit, and thus arrange those exponentials better.\n\nThis is one application of custom derivative rules for Python functions that are already JAX transformable: specifying how a composite function should be differentiated, while still using its original Python definition for other transformations (like {func}`jax.jit`, {func}`jax.vmap`, ...).\n\nHere's a solution using {func}`jax.custom_jvp`:\n\n```{code-cell}\n@custom_jvp\ndef log1pexp(x):\n  return jnp.log(1. + jnp.exp(x))\n\n@log1pexp.defjvp\ndef log1pexp_jvp(primals, tangents):\n  x, = primals\n  x_dot, = tangents\n  ans = log1pexp(x)\n  ans_dot = (1 - 1/(1 + jnp.exp(x))) * x_dot\n  return ans, ans_dot\n```\n\n```{code-cell}\nprint(grad(log1pexp)(100.))\n```\n\n```{code-cell}\nprint(jit(log1pexp)(3.))\nprint(jit(grad(log1pexp))(3.))\nprint(vmap(jit(grad(log1pexp)))(jnp.arange(3.)))\n```\n\nHere's a `defjvps` convenience wrapper to express the same thing:\n\n```{code-cell}\n@custom_jvp\ndef log1pexp(x):\n  return jnp.log(1. + jnp.exp(x))\n\nlog1pexp.defjvps(lambda t, ans, x: (1 - 1/(1 + jnp.exp(x))) * t)\n```\n\n```{code-cell}\nprint(grad(log1pexp)(100.))\nprint(jit(log1pexp)(3.))\nprint(jit(grad(log1pexp))(3.))\nprint(vmap(jit(grad(log1pexp)))(jnp.arange(3.)))\n```\n\n#### Example: Enforcing a differentiation convention\n\nA related application is to enforce a differentiation convention, perhaps at a boundary.\n\nConsider the function $f : \\mathbb{R}_+ \\to \\mathbb{R}_+$ with $f(x) = \\frac{x}{1 + \\sqrt{x}}$, where we take $\\mathbb{R}_+ = [0, \\infty)$. We might implement $f$ as a program like this:\n\n```{code-cell}\ndef f(x):\n  return x / (1 + jnp.sqrt(x))\n```\n\nAs a mathematical function on $\\mathbb{R}$ (the full real line), $f$ is not differentiable at zero (because the limit defining the derivative doesn't exist from the left). Correspondingly, autodiff produces a `nan` value:\n\n```{code-cell}\nprint(grad(f)(0.))\n```\n\nBut mathematically if we think of $f$ as a function on $\\mathbb{R}_+$ then it is differentiable at 0 [Rudin's Principles of Mathematical Analysis Definition 5.1, or Tao's Analysis I 3rd ed. Definition 10.1.1 and Example 10.1.6]. Alternatively, we might say as a convention we want to consider the directional derivative from the right. So there is a sensible value for the Python function `grad(f)` to return at `0.0`, namely `1.0`. By default, JAX's machinery for differentiation assumes all functions are defined over $\\mathbb{R}$ and thus doesn't produce `1.0` here.\n\nWe can use a custom JVP rule! In particular, we can define the JVP rule in terms of the derivative function $x \\mapsto \\frac{\\sqrt{x} + 2}{2(\\sqrt{x} + 1)^2}$ on $\\mathbb{R}_+$,\n\n```{code-cell}\n@custom_jvp\ndef f(x):\n  return x / (1 + jnp.sqrt(x))\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n  x, = primals\n  x_dot, = tangents\n  ans = f(x)\n  ans_dot = ((jnp.sqrt(x) + 2) / (2 * (jnp.sqrt(x) + 1)**2)) * x_dot\n  return ans, ans_dot\n```\n\n```{code-cell}\nprint(grad(f)(0.))\n```\n\nHere's the convenience wrapper version:\n\n```{code-cell}\n@custom_jvp\ndef f(x):\n  return x / (1 + jnp.sqrt(x))\n\nf.defjvps(lambda t, ans, x: ((jnp.sqrt(x) + 2) / (2 * (jnp.sqrt(x) + 1)**2)) * t)\n```\n\n```{code-cell}\nprint(grad(f)(0.))\n```\n\n#### Example: Gradient clipping\n\nWhile in some cases we want to express a mathematical differentiation computation, in other cases we may even want to take a step away from mathematics to adjust the computation autodiff performs. One canonical example is reverse-mode gradient clipping.\n\nFor gradient clipping, we can use {func}`jnp.clip` together with a {func}`jax.custom_vjp` reverse-mode-only rule:\n\n```{code-cell}\nfrom functools import partial\n\n@custom_vjp\ndef clip_gradient(lo, hi, x):\n  return x  # identity function\n\ndef clip_gradient_fwd(lo, hi, x):\n  return x, (lo, hi)  # save bounds as residuals\n\ndef clip_gradient_bwd(res, g):\n  lo, hi = res\n  return (None, None, jnp.clip(g, lo, hi))  # use None to indicate zero cotangents for lo and hi\n\nclip_gradient.defvjp(clip_gradient_fwd, clip_gradient_bwd)\n```\n\n```{code-cell}\nimport matplotlib.pyplot as plt\n\nt = jnp.linspace(0, 10, 1000)\n\nplt.plot(jnp.sin(t))\nplt.plot(vmap(grad(jnp.sin))(t))\n```\n\n```{code-cell}\ndef clip_sin(x):\n  x = clip_gradient(-0.75, 0.75, x)\n  return jnp.sin(x)\n\nplt.plot(clip_sin(t))\nplt.plot(vmap(grad(clip_sin))(t))\n```\n\n#### Example: Python debugging\n\nAnother application that is motivated by development workflow rather than numerics is to set a `pdb` debugger trace in the backward pass of reverse-mode autodiff.\n\nWhen trying to track down the source of a `nan` runtime error, or just examine carefully the cotangent (gradient) values being propagated, it can be useful to insert a debugger at a point in the backward pass that corresponds to a specific point in the primal computation. You can do that with {func}`jax.custom_vjp`.\n\nWe'll defer an example until the next section.\n\n\n\n#### Example: Implicit function differentiation of iterative implementations\n\nThis example gets pretty deep in the mathematical weeds!\n\nAnother application for {func}`jax.custom_vjp` is reverse-mode differentiation of functions that are JAX-transformable (by {func}`jax.jit`, {func}`jax.vmap`, ...) but not efficiently JAX-differentiable for some reason, perhaps because they involve {func}`jax.lax.while_loop`. (It's not possible to produce an XLA HLO program that efficiently computes the reverse-mode derivative of an XLA HLO While loop because that would require a program with unbounded memory use, which isn't possible to express in XLA HLO, at least without \"side-effecting\" interactions through infeed/outfeed.)\n\nFor example, consider this `fixed_point` routine which computes a fixed point by iteratively applying a function in a `while_loop`:\n\n```{code-cell}\nfrom jax.lax import while_loop\n\ndef fixed_point(f, a, x_guess):\n  def cond_fun(carry):\n    x_prev, x = carry\n    return jnp.abs(x_prev - x) > 1e-6\n\n  def body_fun(carry):\n    _, x = carry\n    return x, f(a, x)\n\n  _, x_star = while_loop(cond_fun, body_fun, (x_guess, f(a, x_guess)))\n  return x_star\n```\n\nThis is an iterative procedure for numerically solving the equation $x = f(a, x)$ for $x$, by iterating $x_{t+1} = f(a, x_t)$ until $x_{t+1}$ is sufficiently close to $x_t$. The result $x^*$ depends on the parameters $a$, and so we can think of there being a function $a \\mapsto x^*(a)$ that is implicitly defined by equation $x = f(a, x)$.\n\nWe can use `fixed_point` to run iterative procedures to convergence, for example running Newton's method to calculate square roots while only executing adds, multiplies, and divides:\n\n```{code-cell}\ndef newton_sqrt(a):\n  update = lambda a, x: 0.5 * (x + a / x)\n  return fixed_point(update, a, a)\n```\n\n```{code-cell}\nprint(newton_sqrt(2.))\n```\n\nWe can {func}`jax.vmap` or {func}`jax.jit` the function as well:\n\n```{code-cell}\nprint(jit(vmap(newton_sqrt))(jnp.array([1., 2., 3., 4.])))\n```\n\nWe can't apply reverse-mode automatic differentiation because of the `while_loop`, but it turns out we wouldn't want to anyway: instead of differentiating through the implementation of `fixed_point` and all its iterations, we can exploit the mathematical structure to do something that is much more memory-efficient (and FLOP-efficient in this case, too!). We can instead use the implicit function theorem [Prop A.25 of Bertsekas's Nonlinear Programming, 2nd ed.], which guarantees (under some conditions) the existence of the mathematical objects we're about to use. In essence, we linearize the solution and solve those linear equations iteratively to compute the derivatives we want.\n\nConsider again the equation $x = f(a, x)$ and the function $x^*$. We want to evaluate vector-Jacobian products like $v^\\mathsf{T} \\mapsto v^\\mathsf{T} \\partial x^*(a_0)$.\n\nAt least in an open neighborhood around the point $a_0$ at which we want to differentiate, let's assume that the equation $x^*(a) = f(a, x^*(a))$ holds for all $a$. Since the two sides are equal as functions of $a$, their derivatives must be equal as well, so let's differentiate both sides:\n\n$\\qquad \\partial x^*(a) = \\partial_0 f(a, x^*(a)) + \\partial_1 f(a, x^*(a))  \\partial x^*(a)$.\n\nSetting $A = \\partial_1 f(a_0, x^*(a_0))$ and $B = \\partial_0 f(a_0, x^*(a_0))$, we can write the quantity we're after more simply as:\n\n$\\qquad \\partial x^*(a_0) = B + A \\partial x^*(a_0)$,\n\nor, by rearranging,\n\n$\\qquad \\partial x^*(a_0) = (I - A)^{-1} B$.\n\nThat means we can evaluate vector-Jacobian products, such as:\n\n$\\qquad v^\\mathsf{T} \\partial x^*(a_0) = v^\\mathsf{T} (I - A)^{-1} B = w^\\mathsf{T} B$,\n\nwhere $w^\\mathsf{T} = v^\\mathsf{T} (I - A)^{-1}$, or equivalently $w^\\mathsf{T} = v^\\mathsf{T} + w^\\mathsf{T} A$, or equivalently $w^\\mathsf{T}$ is the fixed point of the map $u^\\mathsf{T} \\mapsto v^\\mathsf{T} + u^\\mathsf{T} A$. That last characterization gives us a way to write the VJP for `fixed_point` in terms of a call to `fixed_point`! Moreover, after expanding $A$ and $B$ back out, you can conclude you need only to evaluate VJPs of $f$ at $(a_0, x^*(a_0))$.\n\nHere's the upshot:\n\n```{code-cell}\n@partial(custom_vjp, nondiff_argnums=(0,))\ndef fixed_point(f, a, x_guess):\n  def cond_fun(carry):\n    x_prev, x = carry\n    return jnp.abs(x_prev - x) > 1e-6\n\n  def body_fun(carry):\n    _, x = carry\n    return x, f(a, x)\n\n  _, x_star = while_loop(cond_fun, body_fun, (x_guess, f(a, x_guess)))\n  return x_star\n\ndef fixed_point_fwd(f, a, x_init):\n  x_star = fixed_point(f, a, x_init)\n  return x_star, (a, x_star)\n\ndef fixed_point_rev(f, res, x_star_bar):\n  a, x_star = res\n  _, vjp_a = vjp(lambda a: f(a, x_star), a)\n  a_bar, = vjp_a(fixed_point(partial(rev_iter, f),\n                             (a, x_star, x_star_bar),\n                             x_star_bar))\n  return a_bar, jnp.zeros_like(x_star)\n  \ndef rev_iter(f, packed, u):\n  a, x_star, x_star_bar = packed\n  _, vjp_x = vjp(lambda x: f(a, x), x_star)\n  return x_star_bar + vjp_x(u)[0]\n\nfixed_point.defvjp(fixed_point_fwd, fixed_point_rev)\n```\n\n```{code-cell}\nprint(newton_sqrt(2.))\n```\n\n```{code-cell}\nprint(grad(newton_sqrt)(2.))\nprint(grad(grad(newton_sqrt))(2.))\n```\n\nWe can check our answers by differentiating {func}`jnp.sqrt`, which uses a totally different implementation:\n\n```{code-cell}\nprint(grad(jnp.sqrt)(2.))\nprint(grad(grad(jnp.sqrt))(2.))\n```\n\nA limitation to this approach is that the argument `f` can't close over any values involved in differentiation. That is, you might notice that we kept the parameter `a` explicit in the argument list of `fixed_point`. For this use case, consider using the low-level primitive `lax.custom_root`, which allows for derivatives in closed-over variables with custom root-finding functions.\n\n\n### Basic usage of `jax.custom_jvp` and `jax.custom_vjp` APIs\n\n#### Use `jax.custom_jvp` to define forward-mode (and, indirectly, reverse-mode) rules\n\nHere's a canonical basic example of using {func}`jax.custom_jvp`, where the comments use\n[Haskell-like type signatures](https://wiki.haskell.org/Type_signature):\n\n```{code-cell}\n# f :: a -> b\n@custom_jvp\ndef f(x):\n  return jnp.sin(x)\n\n# f_jvp :: (a, T a) -> (b, T b)\ndef f_jvp(primals, tangents):\n  x, = primals\n  t, = tangents\n  return f(x), jnp.cos(x) * t\n\nf.defjvp(f_jvp)\n```\n\n```{code-cell}\nprint(f(3.))\n\ny, y_dot = jvp(f, (3.,), (1.,))\nprint(y)\nprint(y_dot)\n```\n\nIn other words, we start with a primal function `f` that takes inputs of type `a` and produces outputs of type `b`. We associate with it a JVP rule function `f_jvp` that takes a pair of inputs representing the primal inputs of type `a` and the corresponding tangent inputs of type `T a`, and produces a pair of outputs representing the primal outputs of type `b` and tangent outputs of type `T b`. The tangent outputs should be a linear function of the tangent inputs.\n\nYou can also use `f.defjvp` as a decorator, as in\n\n```python\n@custom_jvp\ndef f(x):\n  ...\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n  ...\n```\n\nEven though we defined only a JVP rule and no VJP rule, we can use both forward- and reverse-mode differentiation on `f`. JAX will automatically transpose the linear computation on tangent values from our custom JVP rule, computing the VJP as efficiently as if we had written the rule by hand:\n\n```{code-cell}\nprint(grad(f)(3.))\nprint(grad(grad(f))(3.))\n```\n\nFor automatic transposition to work, the JVP rule's output tangents must be linear as a function of the input tangents. Otherwise a transposition error is raised.\n\nMultiple arguments work like this:\n\n```{code-cell}\n@custom_jvp\ndef f(x, y):\n  return x ** 2 * y\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n  x, y = primals\n  x_dot, y_dot = tangents\n  primal_out = f(x, y)\n  tangent_out = 2 * x * y * x_dot + x ** 2 * y_dot\n  return primal_out, tangent_out\n```\n\n```{code-cell}\nprint(grad(f)(2., 3.))\n```\n\nThe `defjvps` convenience wrapper lets us define a JVP for each argument separately, and the results are computed separately then summed:\n\n```{code-cell}\n@custom_jvp\ndef f(x):\n  return jnp.sin(x)\n\nf.defjvps(lambda t, ans, x: jnp.cos(x) * t)\n```\n\n```{code-cell}\nprint(grad(f)(3.))\n```\n\nHere's a `defjvps` example with multiple arguments:\n\n```{code-cell}\n@custom_jvp\ndef f(x, y):\n  return x ** 2 * y\n\nf.defjvps(lambda x_dot, primal_out, x, y: 2 * x * y * x_dot,\n          lambda y_dot, primal_out, x, y: x ** 2 * y_dot)\n```\n\n```{code-cell}\nprint(grad(f)(2., 3.))\nprint(grad(f, 0)(2., 3.))  # same as above\nprint(grad(f, 1)(2., 3.))\n```\n\nAs a shorthand, with `defjvps` you can pass a `None` value to indicate that the JVP for a particular argument is zero:\n\n```{code-cell}\n@custom_jvp\ndef f(x, y):\n  return x ** 2 * y\n\nf.defjvps(lambda x_dot, primal_out, x, y: 2 * x * y * x_dot,\n          None)\n```\n\n```{code-cell}\nprint(grad(f)(2., 3.))\nprint(grad(f, 0)(2., 3.))  # same as above\nprint(grad(f, 1)(2., 3.))\n```\n\nCalling a {func}`jax.custom_jvp` function with keyword arguments, or writing a {func}`jax.custom_jvp` function definition with default arguments, are both allowed so long as they can be unambiguously mapped to positional arguments based on the function signature retrieved by the standard library `inspect.signature` mechanism.\n\nWhen you're not performing differentiation, the function `f` is called just as if it weren't decorated by {func}`jax.custom_jvp`:\n\n```{code-cell}\n@custom_jvp\ndef f(x):\n  print('called f!')  # a harmless side-effect\n  return jnp.sin(x)\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n  print('called f_jvp!')  # a harmless side-effect\n  x, = primals\n  t, = tangents\n  return f(x), jnp.cos(x) * t\n```\n\n```{code-cell}\nprint(f(3.))\n```\n\n```{code-cell}\nprint(vmap(f)(jnp.arange(3.)))\nprint(jit(f)(3.))\n```\n\nThe custom JVP rule is invoked during differentiation, whether forward or reverse:\n\n```{code-cell}\ny, y_dot = jvp(f, (3.,), (1.,))\nprint(y_dot)\n```\n\n```{code-cell}\nprint(grad(f)(3.))\n```\n\nNotice that `f_jvp` calls `f` to compute the primal outputs. In the context of higher-order differentiation, each application of a differentiation transform will use the custom JVP rule if and only if the rule calls the original `f` to compute the primal outputs. (This represents a kind of fundamental tradeoff, where we can't make use of intermediate values from the evaluation of `f` in our rule _and also_ have the rule apply in all orders of higher-order differentiation.)\n\n```{code-cell}\ngrad(grad(f))(3.)\n```\n\nYou can use Python control flow with {func}`jax.custom_jvp`:\n\n```{code-cell}\n@custom_jvp\ndef f(x):\n  if x > 0:\n    return jnp.sin(x)\n  else:\n    return jnp.cos(x)\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n  x, = primals\n  x_dot, = tangents\n  ans = f(x)\n  if x > 0:\n    return ans, 2 * x_dot\n  else:\n    return ans, 3 * x_dot\n```\n\n```{code-cell}\nprint(grad(f)(1.))\nprint(grad(f)(-1.))\n```\n\n#### Use `jax.custom_vjp` to define custom reverse-mode-only rules\n\nWhile {func}`jax.custom_jvp` suffices for controlling both forward- and, via JAX's automatic transposition, reverse-mode differentiation behavior, in some cases we may want to directly control a VJP rule, for example in the latter two example problems presented above. We can do that with {func}`jax.custom_vjp`:\n\n```{code-cell}\nfrom jax import custom_vjp\n\n# f :: a -> b\n@custom_vjp\ndef f(x):\n  return jnp.sin(x)\n\n# f_fwd :: a -> (b, c)\ndef f_fwd(x):\n  return f(x), jnp.cos(x)\n\n# f_bwd :: (c, CT b) -> CT a\ndef f_bwd(cos_x, y_bar):\n  return (cos_x * y_bar,)\n\nf.defvjp(f_fwd, f_bwd)\n```\n\n```{code-cell}\nprint(f(3.))\nprint(grad(f)(3.))\n```\n\nIn other words, we again start with a primal function `f` that takes inputs of type `a` and produces outputs of type `b`. We associate with it two functions, `f_fwd` and `f_bwd`, which describe how to perform the forward- and backward-passes of reverse-mode autodiff, respectively.\n\nThe function `f_fwd` describes the forward pass, not only the primal computation but also what values to save for use on the backward pass. Its input signature is just like that of the primal function `f`, in that it takes a primal input of type `a`. But as output it produces a pair, where the first element is the primal output `b` and the second element is any \"residual\" data of type `c` to be stored for use by the backward pass. (This second output is analogous to [PyTorch's save_for_backward mechanism](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html).)\n\nThe function `f_bwd` describes the backward pass. It takes two inputs, where the first is the residual data of type `c` produced by `f_fwd` and the second is the output cotangents of type `CT b` corresponding to the output of the primal function. It produces an output of type `CT a` representing the cotangents corresponding to the input of the primal function. In particular, the output of `f_bwd` must be a sequence (e.g. a tuple) of length equal to the number of arguments to the primal function.\n\nSo multiple arguments work like this:\n\n```{code-cell}\n@custom_vjp\ndef f(x, y):\n  return jnp.sin(x) * y\n\ndef f_fwd(x, y):\n  return f(x, y), (jnp.cos(x), jnp.sin(x), y)\n\ndef f_bwd(res, g):\n  cos_x, sin_x, y = res\n  return (cos_x * g * y, sin_x * g)\n\nf.defvjp(f_fwd, f_bwd)\n```\n\n```{code-cell}\nprint(grad(f)(2., 3.))\n```\n\nCalling a {func}`jax.custom_vjp` function with keyword arguments, or writing a {func}`jax.custom_vjp` function definition with default arguments, are both allowed so long as they can be unambiguously mapped to positional arguments based on the function signature retrieved by the standard library `inspect.signature` mechanism.\n\nAs with {func}`jax.custom_jvp`, the custom VJP rule composed of `f_fwd` and `f_bwd` is not invoked if differentiation is not applied. If the function is evaluated, or transformed with {func}`jax.jit`, {func}`jax.vmap`, or other non-differentiation transformations, then only `f` is called.\n\n```{code-cell}\n@custom_vjp\ndef f(x):\n  print(\"called f!\")\n  return jnp.sin(x)\n\ndef f_fwd(x):\n  print(\"called f_fwd!\")\n  return f(x), jnp.cos(x)\n\ndef f_bwd(cos_x, y_bar):\n  print(\"called f_bwd!\")\n  return (cos_x * y_bar,)\n\nf.defvjp(f_fwd, f_bwd)\n```\n\n```{code-cell}\nprint(f(3.))\n```\n\n```{code-cell}\nprint(grad(f)(3.))\n```\n\n```{code-cell}\ny, f_vjp = vjp(f, 3.)\nprint(y)\n```\n\n```{code-cell}\nprint(f_vjp(1.))\n```\n\n**Forward-mode autodiff cannot be used on the** {func}`jax.custom_vjp` **function** and will raise an error:\n\n```{code-cell}\n:tags: [raises-exception]\n\nfrom jax import jvp\n\ntry:\n  jvp(f, (3.,), (1.,))\nexcept TypeError as e:\n  print('ERROR! {}'.format(e))\n```\n\nIf you want to use both forward- and reverse-mode, use {func}`jax.custom_jvp` instead.\n\nWe can use {func}`jax.custom_vjp` together with `pdb` to insert a debugger trace in the backward pass:\n\n```{code-cell}\nimport pdb\n\n@custom_vjp\ndef debug(x):\n  return x  # acts like identity\n\ndef debug_fwd(x):\n  return x, x\n\ndef debug_bwd(x, g):\n  import pdb; pdb.set_trace()\n  return g\n\ndebug.defvjp(debug_fwd, debug_bwd)\n```\n\n```{code-cell}\ndef foo(x):\n  y = x ** 2\n  y = debug(y)  # insert pdb in corresponding backward pass step\n  return jnp.sin(y)\n```\n\n```python\njax.grad(foo)(3.)\n\n> <ipython-input-113-b19a2dc1abf7>(12)debug_bwd()\n-> return g\n(Pdb) p x\nArray(9., dtype=float32)\n(Pdb) p g\nArray(-0.91113025, dtype=float32)\n(Pdb) q\n```\n\n\n### More features and details\n\n#### Working with `list` / `tuple` / `dict` containers (and other pytrees)\n\nYou should expect standard Python containers like lists, tuples, namedtuples, and dicts to just work, along with nested versions of those. In general, any [pytrees](https://docs.jax.dev/en/latest/pytrees.html) are permissible, so long as their structures are consistent according to the type constraints. \n\nHere's a contrived example with {func}`jax.custom_jvp`:\n\n```{code-cell}\nfrom collections import namedtuple\nPoint = namedtuple(\"Point\", [\"x\", \"y\"])\n\n@custom_jvp\ndef f(pt):\n  x, y = pt.x, pt.y\n  return {'a': x ** 2,\n          'b': (jnp.sin(x), jnp.cos(y))}\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n  pt, = primals\n  pt_dot, =  tangents\n  ans = f(pt)\n  ans_dot = {'a': 2 * pt.x * pt_dot.x,\n             'b': (jnp.cos(pt.x) * pt_dot.x, -jnp.sin(pt.y) * pt_dot.y)}\n  return ans, ans_dot\n\ndef fun(pt):\n  dct = f(pt)\n  return dct['a'] + dct['b'][0]\n```\n\n```{code-cell}\npt = Point(1., 2.)\n\nprint(f(pt))\n```\n\n```{code-cell}\nprint(grad(fun)(pt))\n```\n\nAnd an analogous contrived example with {func}`jax.custom_vjp`:\n\n```{code-cell}\n@custom_vjp\ndef f(pt):\n  x, y = pt.x, pt.y\n  return {'a': x ** 2,\n          'b': (jnp.sin(x), jnp.cos(y))}\n\ndef f_fwd(pt):\n  return f(pt), pt\n\ndef f_bwd(pt, g):\n  a_bar, (b0_bar, b1_bar) = g['a'], g['b']\n  x_bar = 2 * pt.x * a_bar + jnp.cos(pt.x) * b0_bar\n  y_bar = -jnp.sin(pt.y) * b1_bar\n  return (Point(x_bar, y_bar),)\n\nf.defvjp(f_fwd, f_bwd)\n\ndef fun(pt):\n  dct = f(pt)\n  return dct['a'] + dct['b'][0]\n```\n\n```{code-cell}\npt = Point(1., 2.)\n\nprint(f(pt))\n```\n\n```{code-cell}\nprint(grad(fun)(pt))\n```\n\n#### Handling  non-differentiable arguments\n\nSome use cases, like the final example problem, call for non-differentiable arguments like function-valued arguments to be passed to functions with custom differentiation rules, and for those arguments to also be passed to the rules themselves. In the case of `fixed_point`, the function argument `f` was such a non-differentiable argument. A similar situation arises with `jax.experimental.odeint`.\n\n##### `jax.custom_jvp` with `nondiff_argnums`\n\nUse the optional `nondiff_argnums` parameter to {func}`jax.custom_jvp` to indicate arguments like these. Here's an example with {func}`jax.custom_jvp`:\n\n```{code-cell}\nfrom functools import partial\n\n@partial(custom_jvp, nondiff_argnums=(0,))\ndef app(f, x):\n  return f(x)\n\n@app.defjvp\ndef app_jvp(f, primals, tangents):\n  x, = primals\n  x_dot, = tangents\n  return f(x), 2. * x_dot\n```\n\n```{code-cell}\nprint(app(lambda x: x ** 3, 3.))\n```\n\n```{code-cell}\nprint(grad(app, 1)(lambda x: x ** 3, 3.))\n```\n\nNotice the gotcha here: no matter where in the argument list these parameters appear, they're placed at the *start* of the signature of the corresponding JVP rule. Here's another example:\n\n```{code-cell}\n@partial(custom_jvp, nondiff_argnums=(0, 2))\ndef app2(f, x, g):\n  return f(g((x)))\n\n@app2.defjvp\ndef app2_jvp(f, g, primals, tangents):\n  x, = primals\n  x_dot, = tangents\n  return f(g(x)), 3. * x_dot\n```\n\n```{code-cell}\nprint(app2(lambda x: x ** 3, 3., lambda y: 5 * y))\n```\n\n```{code-cell}\nprint(grad(app2, 1)(lambda x: x ** 3, 3., lambda y: 5 * y))\n```\n\n##### `jax.custom_vjp` with `nondiff_argnums`\n\nA similar option exists for {func}`jax.custom_vjp`, and, similarly, the convention is that the non-differentiable arguments are passed as the first arguments to the `_bwd` rule, no matter where they appear in the signature of the original function. The signature of the `_fwd` rule remains unchanged - it is the same as the signature of the primal function. Here's an example:\n\n```{code-cell}\n@partial(custom_vjp, nondiff_argnums=(0,))\ndef app(f, x):\n  return f(x)\n\ndef app_fwd(f, x):\n  return f(x), x\n\ndef app_bwd(f, x, g):\n  return (5 * g,)\n\napp.defvjp(app_fwd, app_bwd)\n```\n\n```{code-cell}\nprint(app(lambda x: x ** 2, 4.))\n```\n\n```{code-cell}\nprint(grad(app, 1)(lambda x: x ** 2, 4.))\n```\n\nRefer to `fixed_point` above for another usage example.\n\n**You don't need to use** `nondiff_argnums` **with array-valued arguments**, such as, for example, ones with the integer dtype. Instead, `nondiff_argnums` should only be used for argument values that don't correspond to JAX types (essentially don't correspond to array types), like Python callables or strings. If JAX detects that an argument indicated by `nondiff_argnums` contains a JAX Tracer, then an error is raised. The `clip_gradient` function above is a good example of not using `nondiff_argnums` for integer-dtype array arguments.\n\n## Next steps\n\nThere's a whole world of other autodiff tricks and functionality out there. Topics that weren't covered in this tutorial but can be worth pursuing include:\n\n - Gauss-Newton Vector Products, linearizing once\n - Custom VJPs and JVPs\n - Efficient derivatives at fixed-points\n - Estimating the trace of a Hessian using random Hessian-vector products\n - Forward-mode autodiff using only reverse-mode autodiff\n - Taking derivatives with respect to custom data types\n - Checkpointing (binomial checkpointing for efficient reverse-mode, not model snapshotting)\n - Optimizing VJPs with Jacobian pre-accumulation\n", "docs/advanced_guides.rst": ".. _advanced_guides:\n\nResources and Advanced Guides\n=============================\n\nThis section contains examples and tutorials on more advanced topics,\nsuch as multi-core computation, automatic differentiation, and custom\noperations.\n\n.. toctree::\n   :caption: Parallel computation\n   :maxdepth: 1\n\n   notebooks/Distributed_arrays_and_automatic_parallelization\n   notebooks/explicit-sharding\n   notebooks/shard_map\n   notebooks/layout\n   notebooks/host-offloading\n   multi_process\n   distributed_data_loading\n   notebooks/colocated-python\n\n.. toctree::\n   :caption: Machine learning\n   :maxdepth: 1\n\n   the-training-cookbook\n\n.. toctree::\n   :caption: Automatic differentiation\n   :maxdepth: 1\n\n   notebooks/autodiff_cookbook\n   notebooks/Custom_derivative_rules_for_Python_code\n   notebooks/autodiff_remat\n   advanced-autodiff\n\n.. toctree::\n   :maxdepth: 1\n   :caption: Errors and debugging\n\n   errors\n   debugging\n   debugging/index\n   debugging/flags\n   transfer_guard\n\n.. toctree::\n   :maxdepth: 1\n   :caption: Pytrees\n\n   pytrees\n\n.. toctree::\n   :maxdepth: 1\n   :caption: Performance optimizations\n\n   persistent_compilation_cache\n   gpu_performance_tips\n\n.. toctree::\n   :maxdepth: 1\n   :caption: Performance benchmarking and profiling\n\n   profiling\n   device_memory_profiling\n\n.. toctree::\n   :caption: Non-functional programming\n   :maxdepth: 1\n\n   array_refs\n\n.. toctree::\n   :caption: External Callbacks\n   :maxdepth: 1\n\n   external-callbacks\n\n.. toctree::\n   :caption: FFI\n   :maxdepth: 1\n\n   ffi\n\n.. toctree::\n   :caption: Modeling workflows\n   :maxdepth: 1\n\n   gradient-checkpointing\n   aot\n   export/index\n\n.. toctree::\n   :caption: Pallas\n   :maxdepth: 1\n\n   pallas/index\n\n.. toctree::\n   :caption: Example applications\n   :maxdepth: 1\n\n   notebooks/neural_network_with_tfds_data\n   notebooks/Neural_Network_and_Data_Loading\n   notebooks/vmapped_log_probs\n\n.. toctree::\n   :caption: Deep dives\n   :maxdepth: 1\n\n   notebooks/convolutions\n   xla_flags\n   jax-primitives\n   jaxpr\n", "docs/aot.md": "(ahead-of-time-lowering)=\n\n# Ahead-of-time lowering and compilation\n\n<!--* freshness: { reviewed: '2024-06-12' } *-->\n\nJAX's `jax.jit` transformation returns a function that, when called,\ncompiles a computation and runs it on accelerators (or the CPU). As\nthe JIT acronym indicates, all compilation happens _just-in-time_ for\nexecution.\n\nSome situations call for _ahead-of-time_ (AOT) compilation instead. When you\nwant to fully compile prior to execution time, or you want control over when\ndifferent parts of the compilation process take place, JAX has some options for\nyou.\n\nFirst, let's review the stages of compilation. Suppose that `f` is a\nfunction/callable output by {func}`jax.jit`, say `f = jax.jit(F)` for some input\ncallable `F`. When it is invoked with arguments, say `f(x, y)` where `x` and `y`\nare arrays, JAX does the following in order:\n\n1. **Stage out** a specialized version of the original Python callable\n   `F` to an internal representation. The specialization reflects a\n   restriction of `F` to input types inferred from properties of the\n   arguments `x` and `y` (usually their shape and element type). JAX\n   carries out this specialization by a process that we call\n   _tracing_. During tracing, JAX stages the specialization of `F` to\n   a jaxpr, which is a function in the [Jaxpr intermediate\n   language](https://docs.jax.dev/en/latest/jaxpr.html).\n\n2. **Lower** this specialized, staged-out computation to the XLA compiler's\n   input language, StableHLO.\n\n3. **Compile** the lowered HLO program to produce an optimized executable for\n   the target device (CPU, GPU, or TPU).\n\n4. **Execute** the compiled executable with the arrays `x` and `y` as arguments.\n\nJAX's AOT API gives you direct control over each of these steps, plus\nsome other features along the way. An example:\n\n```python\n>>> import jax\n\n>>> def f(x, y): return 2 * x + y\n>>> x, y = 3, 4\n\n>>> traced = jax.jit(f).trace(x, y)\n\n>>> # Print the specialized, staged-out representation (as Jaxpr IR)\n>>> print(traced.jaxpr)\n{ lambda ; a:i32[] b:i32[]. let\n    c:i32[] = mul 2:i32[] a\n    d:i32[] = add c b\n  in (d,) }\n\n>>> lowered = traced.lower()\n\n>>> # Print lowered HLO\n>>> print(lowered.as_text())\nmodule @jit_f attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<i32>, %arg1: tensor<i32>) -> (tensor<i32> {jax.result_info = \"result\"}) {\n    %c = stablehlo.constant dense<2> : tensor<i32>\n    %0 = stablehlo.multiply %c, %arg0 : tensor<i32>\n    %1 = stablehlo.add %0, %arg1 : tensor<i32>\n    return %1 : tensor<i32>\n  }\n}\n\n>>> compiled = lowered.compile()\n\n>>> # Query for cost analysis, print FLOP estimate\n>>> compiled.cost_analysis()['flops']\n2.0\n\n>>> # Execute the compiled function!\n>>> compiled(x, y)\nArray(10, dtype=int32, weak_type=True)\n\n```\n\nNote that the lowered objects can be used only in the same process\nin which they were lowered. For exporting use cases, see the {ref}`export` APIs.\n\nSee the {mod}`jax.stages` documentation for more details on what functionality\nthe lowering and compiled functions provide.\n\nAll optional arguments to `jit`---such as `static_argnums`---are respected in\nthe corresponding tracing, lowering, compilation, and execution.\n\nIn the example above, we can replace the arguments to `trace` with any objects\nthat have `shape` and `dtype` attributes:\n\n```python\n>>> i32_scalar = jax.ShapeDtypeStruct((), jnp.dtype('int32'))\n>>> jax.jit(f).trace(i32_scalar, i32_scalar).lower().compile()(x, y)\nArray(10, dtype=int32)\n\n```\n\nMore generally, `trace` only needs its arguments to structurally supply what JAX\nmust know for specialization and lowering. For typical array arguments like the\nones above, this means `shape` and `dtype` fields. For static arguments, by\ncontrast, JAX needs actual array values (more on this\n[below](#tracing-with-static-arguments)).\n\nInvoking an AOT-compiled function with arguments that are incompatible with its\ntracing raises an error:\n\n```python\n>>> x_1d = y_1d = jnp.arange(3)\n>>> jax.jit(f).trace(i32_scalar, i32_scalar).lower().compile()(x_1d, y_1d)  # doctest: +IGNORE_EXCEPTION_DETAIL\n...\nTraceback (most recent call last):\nTypeError: Argument types differ from the types for which this computation was compiled. The mismatches are:\nArgument 'x' compiled with int32[] and called with int32[3]\nArgument 'y' compiled with int32[] and called with int32[3]\n\n>>> x_f = y_f = jnp.float32(72.)\n>>> jax.jit(f).trace(i32_scalar, i32_scalar).lower().compile()(x_f, y_f)  # doctest: +IGNORE_EXCEPTION_DETAIL\n...\nTraceback (most recent call last):\nTypeError: Argument types differ from the types for which this computation was compiled. The mismatches are:\nArgument 'x' compiled with int32[] and called with float32[]\nArgument 'y' compiled with int32[] and called with float32[]\n\n```\n\nRelatedly, AOT-compiled functions [cannot be transformed by JAX's just-in-time\ntransformations](#aot-compiled-functions-cannot-be-transformed) such as\n`jax.jit`, {func}`jax.grad`, and {func}`jax.vmap`.\n\n\n## Tracing with static arguments\n\nTracing with static arguments underscores the interaction between options\npassed to `jax.jit`, the arguments passed to `trace`, and the arguments needed\nto invoke the resulting compiled function. Continuing with our example above:\n\n```python\n>>> lowered_with_x = jax.jit(f, static_argnums=0).trace(7, 8).lower()\n\n>>> # Lowered HLO, specialized to the *value* of the first argument (7)\n>>> print(lowered_with_x.as_text())\nmodule @jit_f attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<i32>) -> (tensor<i32> {jax.result_info = \"result\"}) {\n    %c = stablehlo.constant dense<14> : tensor<i32>\n    %0 = stablehlo.add %c, %arg0 : tensor<i32>\n    return %0 : tensor<i32>\n  }\n}\n\n>>> lowered_with_x.compile()(5)\nArray(19, dtype=int32, weak_type=True)\n\n```\n\nNote that `trace` here takes two arguments as usual, but the subsequent compiled\nfunction accepts only the remaining non-static second argument. The static first\nargument (value 7) is taken as a constant at lowering time and built into the\nlowered computation, where it is possibly folded in with other constants. In\nthis case, its multiplication by 2 is simplified, resulting in the constant 14.\n\nAlthough the second argument to `trace` above can be replaced by a hollow\nshape/dtype structure, it is necessary that the static first argument be a\nconcrete value. Otherwise, tracing errs:\n\n```python\n>>> jax.jit(f, static_argnums=0).trace(i32_scalar, i32_scalar)  # doctest: +SKIP\nTraceback (most recent call last):\nTypeError: unsupported operand type(s) for *: 'int' and 'ShapeDtypeStruct'\n\n>>> jax.jit(f, static_argnums=0).trace(10, i32_scalar).lower().compile()(5)\nArray(25, dtype=int32)\n\n```\n\nThe results of `trace` and of `lower` are not safe to serialize directly for use\nin a different process. See {ref}`export` for additional APIs for this purpose.\n\n## AOT-compiled functions cannot be transformed\n\nCompiled functions are specialized to a particular set of argument \"types,\" such\nas arrays with a specific shape and element type in our running example. From\nJAX's internal point of view, transformations such as {func}`jax.vmap` alter the\ntype signature of functions in a way that invalidates the compiled-for type\nsignature. As a policy, JAX simply disallows compiled functions to be involved\nin transformations. Example:\n\n```python\n>>> def g(x):\n...   assert x.shape == (3, 2)\n...   return x @ jnp.ones(2)\n\n>>> def make_z(*shape):\n...   return jnp.arange(np.prod(shape)).reshape(shape)\n\n>>> z, zs = make_z(3, 2), make_z(4, 3, 2)\n\n>>> g_jit = jax.jit(g)\n>>> g_aot = jax.jit(g).trace(z).lower().compile()\n\n>>> jax.vmap(g_jit)(zs)\nArray([[ 1.,  5.,  9.],\n       [13., 17., 21.],\n       [25., 29., 33.],\n       [37., 41., 45.]], dtype=float32)\n\n>>> jax.vmap(g_aot)(zs)  # doctest: +SKIP\nTraceback (most recent call last):\nTypeError: Cannot apply JAX transformations to a function lowered and compiled for a particular signature. Detected argument of Tracer type <class 'jax._src.interpreters.batching.BatchTracer'>\n\n```\n\nA similar error is raised when `g_aot` is involved in autodiff\n(e.g. {func}`jax.grad`). For consistency, transformation by `jax.jit` is\ndisallowed as well, even though `jit` does not meaningfully modify its\nargument's type signature.\n\n\n## Debug information and analyses, when available\n\nIn addition to the primary AOT functionality (separate and explicit lowering,\ncompilation, and execution), JAX's various AOT stages also offer some additional\nfeatures to help with debugging and gathering compiler feedback.\n\nFor instance, as the initial example above shows, lowered functions often offer\na text representation. Compiled functions do the same, and also offer cost and\nmemory analyses from the compiler. All of these are provided via methods on the\n{class}`jax.stages.Lowered` and {class}`jax.stages.Compiled` objects (e.g.,\n`lowered.as_text()` and `compiled.cost_analysis()` above).\nYou can obtain more debugging information, e.g., source location,\nby using the `debug_info` parameter to `lowered.as_text()`.\n\nThese methods are meant as an aid for manual inspection and debugging, not as a\nreliably programmable API. Their availability and output vary by compiler,\nplatform, and runtime. This makes for two important caveats:\n\n1. If some functionality is unavailable on JAX's current backend, then the\n   method for it returns something trivial (and `False`-like). For example, if\n   the compiler underlying JAX does not provide a cost analysis, then\n   `compiled.cost_analysis()` will be `None`.\n\n2. If some functionality is available, there are still very limited guarantees\n   on what the corresponding method provides. The return value is not required\n   to be consistent---in type, structure, or value---across JAX configurations,\n   backends/platforms, versions, or even invocations of the method. JAX cannot\n   guarantee that the output of `compiled.cost_analysis()` on one day will\n   remain the same on the following day.\n\nWhen in doubt, see the package API documentation for {mod}`jax.stages`.\n", "docs/api_compatibility.md": "(api-compatibility)=\n\n# API compatibility\n\n<!--* freshness: { reviewed: '2023-07-18' } *-->\n\nJAX is constantly evolving, and we want to be able to make improvements to its\nAPIs. That said, we want to minimize churn for the JAX user community, and we\ntry to make breaking changes rarely.\n\n## JAX Versioning\nJAX uses [Effort-based versioning](https://jacobtomlinson.dev/effver/) (see\n{ref}`jep-effver`), and is currently in the Zero version phase.\nThis means that for version `0.X.Y`, incrementing `Y` will introduce minor\nbreaking changes, and incrementing `X` will introduce major breaking changes.\n\nFor any breaking change, JAX currently follows a 3 month deprecation policy.\nWhen an incompatible change is made to an API, we will make our best effort\nto obey the following procedure:\n* the change will be announced in `CHANGELOG.md` and in the doc string for the\n  deprecated API, and the old API will issue a `DeprecationWarning`.\n* three months after the `jax` release that deprecated an API, we may remove the\n  deprecated API at any time. Note that three months is a *lower* bound, and is\n  intentionally chosen to be faster than that of many more mature projects. In\n  practice, deprecations may take considerably longer, particularly if there are\n  many users of a feature. If a three month deprecation period becomes\n  problematic, please raise this with us.\n\nWe reserve the right to change this policy at any time.\n\n## What is covered?\n\nOnly public JAX APIs are covered, which includes the following modules:\n\n* `jax`\n* `jax.dlpack`\n* `jax.image`\n* `jax.lax`\n* `jax.nn`\n* `jax.numpy`\n* `jax.ops`\n* `jax.profiler`\n* `jax.random` (see [details below](#numerics-and-randomness))\n* `jax.scipy`\n* `jax.tree`\n* `jax.tree_util`\n* `jax.test_util`\n\nNot everything in these modules is intended to be public, and over time, we\nare working to separate public and private APIs. Public APIs are documented\nin the JAX documentation.\nAdditionally, our goal is that all non-public APIs should have names\nprefixed with underscores, although we do not entirely comply with this yet.\n\n## What is not covered?\n\n### Explicitly private APIs\nAny API or import path prefixed with an underscore is explicitly private,\nand may change without warning between JAX releases. We are working to move\nall private APIs into `jax._src` to make these expectations more clear.\n\n### jaxlib\nAny import path in the `jaxlib` package is considered private, and may change\nwithout warning between releases. Some APIs defined in `jaxlib` have public\naliases in the `jax` package.\n\n### Legacy internal APIs\nIn addition, there are several legacy modules that currently expose some\nprivate APIs without an underscore, including:\n\n- `jax.core`\n- `jax.interpreters`\n- `jax.lib`\n- `jax.util`\n\nWe are actively working on deprecating these modules and the APIs they contain.\nIn most cases, such deprecations will follow the 3 month deprecation period,\nbut this may not always be possible. If you use any such APIs, please expect\nthem to be deprecated soon, and seek alternatives.\n\n### Experimental and example libraries\nThe following modules include code for experimental or demonstration purposes,\nand API may change between releases without warning:\n\n* `jax.experimental`\n* `jax.example_libraries`\n\nWe understand that some users depend on `jax.experimental`, and so in most cases\nwe follow the 3 month deprecation period for changes, but this may not always be\npossible.\n\n### JAX extend\nThe {mod}`jax.extend` module includes semi-public JAX internal APIs that are\nmeant for use by downstream projects, but do not have the same stability\nguarantees of the main JAX package. If you have code that uses `jax.extend`,\nwe would strongly recommend CI tests against JAX's nightly releases, so as to\ncatch potential changes before they are released.\n\nFor details on `jax.extend`, see the [`jax.extend` module documentation](https://docs.jax.dev/en/latest/jax.extend.html), or the design document, {ref}`jax-extend-jep`.\n\n## Numerics and randomness\n\nThe *exact* values of numerical operations are not guaranteed to be\nstable across JAX releases. In fact, exact numerics are not\nnecessarily stable at a given JAX version, across accelerator\nplatforms, within or without `jax.jit`, and more.\n\nFor a fixed PRNG key input, the outputs of pseudorandom functions in\n`jax.random` may vary across JAX versions. The compatibility policy\napplies only to the output *distribution*. For example, the expression\n`jax.random.gumbel(jax.random.key(72))` may return a different value\nacross JAX releases, but `jax.random.gumbel` will remain a\npseudorandom generator for the Gumbel distribution.\n\nWe try to make such changes to pseudorandom values infrequently. When\nthey happen, the changes are announced in the changelog, but do not\nfollow a deprecation cycle. In some situations, JAX might expose a\ntransient configuration flag that reverts the new behavior, to help\nusers diagnose and update affected code. Such flags will last a\ndeprecation window's amount of time.\n", "docs/array_refs.md": "---\njupytext:\n  cell_metadata_filter: -all\n  formats: ipynb,md:myst,py\n  main_language: python\n  text_representation:\n    extension: .md\n    format_name: myst\n    format_version: 0.13\n    jupytext_version: 1.16.4\n---\n\n```{raw-cell}\n\n---\nCopyright 2025 The JAX Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n---\n```\n\n# `Ref`: mutable arrays for data plumbing and memory control\n\nJAX `Array`s are immutable, representing mathematical values. Immutability can\nmake code easier to reason about, and is useful for optimized compilation,\nparallelization, rematerialization, and transformations like autodiff.\n\nBut immutability is constraining too:\n* **expressiveness** --- plumbing out intermediate data or maintaining state,\n  e.g. for normalization statistics or metrics, can feel heavyweight;\n* **performance** --- it's more difficult to reason about performance, like\n  memory lifetimes and in-place updates.\n\n`Ref`s can help! They represent mutable arrays that can be read and written\nin-place. These array references are compatible with JAX transformations, like\n`jax.jit` and `jax.grad`:\n\n```{code-cell}\nimport jax\nimport jax.numpy as jnp\n\nx_ref = jax.new_ref(jnp.zeros(3))  # new array ref, with initial value [0., 0., 0.]\n\n@jax.jit\ndef f():\n  x_ref[1] += 1.  # indexed add-update\n\nprint(x_ref)  # Ref([0., 0., 0.])\nf()\nf()\nprint(x_ref)  # Ref([0., 2., 0.])\n```\n\nThe indexing syntax follows NumPy's. For a `Ref` called `x_ref`, we can\nread its entire value into an `Array` by writing `x_ref[...]`, and write its\nentire value using `x_ref[...] = A` for some `Array`-valued expression `A`:\n\n```{code-cell}\ndef g(x):\n  x_ref = jax.new_ref(0.)\n  x_ref[...] = jnp.sin(x)\n  return x_ref[...]\n\nprint(jax.grad(g)(1.0))  # 0.54\n```\n\n`Ref` is a distinct type from `Array`, and it comes with some important\nconstraints and limitations. In particular, indexed reading and writing is just\nabout the *only* thing you can do with an `Ref`. References can't be passed\nwhere `Array`s are expected:\n\n```{code-cell}\nx_ref = jax.new_ref(1.0)\ntry:\n  jnp.sin(x_ref)  # error! can't do math on refs\nexcept Exception as e:\n  print(e)\n```\n\nTo do math, you need to read the ref's value first, like `jnp.sin(x_ref[...])`.\n\nSo what _can_ you do with `Ref`? Read on for the details, and some useful\nrecipes.\n\n### API\n\nIf you've ever used\n[Pallas](https://docs.jax.dev/en/latest/pallas/quickstart.html), then `Ref`\nshould look familiar. A big difference is that you can create new `Ref`s\nyourself directly using `jax.new_ref`:\n\n```{code-cell}\nfrom jax import Array, Ref\n\ndef array_ref(init_val: Array) -> Ref:\n  \"\"\"Introduce a new reference with given initial value.\"\"\"\n```\n\n`jax.freeze` is its antithesis, invalidating the given ref (so that accessing it\nafterwards is an error) and producing its final value:\n\n```{code-cell}\ndef freeze(ref: Ref) -> Array:\n  \"\"\"Invalidate given reference and produce its final value.\"\"\"\n```\n\nIn between creating and destroying them, you can perform indexed reads and\nwrites on refs. You can read and write using the functions `jax.ref.get` and\n`jax.ref.swap`, but usually you'd just use NumPy-style array indexing syntax:\n\n```{code-cell}\nimport types\nIndex = int | slice | Array | types.EllipsisType\nIndexer = Index | tuple[Index, ...]\n\ndef get(ref: Ref, idx: Indexer) -> Array:\n  \"\"\"Returns `ref[idx]` for NumPy-style indexer `idx`.\"\"\"\n\ndef swap(ref: Ref, idx: Indexer, val: Array) -> Array:\n  \"\"\"Performs `newval, ref[idx] = ref[idx], val` and returns `newval`.\"\"\"\n```\n\nHere, `Indexer` can be any NumPy indexing expression:\n\n```{code-cell}\nx_ref = jax.new_ref(jnp.arange(12.).reshape(3, 4))\n\n# int indexing\nrow = x_ref[0]\nx_ref[1] = row\n\n# tuple indexing\nval = x_ref[1, 2]\nx_ref[2, 3] = val\n\n# slice indexing\ncol = x_ref[:, 1]\nx_ref[0, :3] = col\n\n# advanced int array indexing\nvals = x_ref[jnp.array([0, 0, 1]), jnp.array([1, 2, 3])]\nx_ref[jnp.array([1, 2, 1]), jnp.array([0, 0, 1])] = vals\n```\n\nAs with `Array`s, indexing mostly follows NumPy behavior, except for\nout-of-bounds indexing which [behaves in the usual way for JAX\n`Array`s](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html#out-of-bounds-indexing).\n\n### Pure and impure functions\n\nA function that takes a ref as an argument (either explicitly or by lexical\nclosure) is considered _impure_. For example:\n\n```{code-cell}\n# takes ref as an argument => impure\n@jax.jit\ndef impure1(x_ref, y_ref):\n  x_ref[...] = y_ref[...]\n\n# closes over ref => impure\ny_ref = jax.new_ref(0)\n\n@jax.jit\ndef impure2(x):\n  y_ref[...] = x\n```\n\nIf a function only uses refs internally, it is still considered _pure_. Purity\nis in the eye of the caller. For example:\n\n```{code-cell}\n# internal refs => still pure\n@jax.jit\ndef pure1(x):\n  ref = jax.new_ref(x)\n  ref[...] = ref[...] + ref[...]\n  return ref[...]\n```\n\nPure functions, even those that use refs internally, are familiar: for example,\nthey work with transformations like `jax.grad`, `jax.vmap`, `jax.shard_map`, and\nothers in the usual way.\n\nImpure functions are sequenced in Python program order.\n\n### Restrictions\n\n`Ref`s are second-class, in the sense that there are restrictions on their\nuse:\n\n* **Can't return refs** from `jit`\\-decorated functions or the bodies of\n  higher-order primitives like `jax.lax.scan`, `jax.lax.while_loop`, or\n  `jax.lax.cond`\n* **Can't pass a ref as an argument more than once** to `jit`\\-decorated\n  functions or higher-order primitives\n* **Can only `freeze` in creation scope**\n* **No higher-order refs** (refs-to-refs)\n\nFor example, these are errors:\n\n```{code-cell}\nx_ref = jax.new_ref(0.)\n\n# can't return refs\n@jax.jit\ndef err1(x_ref):\n  x_ref[...] = 5.\n  return x_ref  # error!\ntry:\n  err1(x_ref)\nexcept Exception as e:\n  print(e)\n\n# can't pass a ref as an argument more than once\n@jax.jit\ndef err2(x_ref, y_ref):\n  ...\ntry:\n  err2(x_ref, x_ref)  # error!\nexcept Exception as e:\n  print(e)\n\n# can't pass and close over the same ref\n@jax.jit\ndef err3(y_ref):\n  y_ref[...] = x_ref[...]\ntry:\n  err3(x_ref)  # error!\nexcept Exception as e:\n  print(e)\n\n# can only freeze in creation scope\n@jax.jit\ndef err4(x_ref):\n  jax.freeze(x_ref)\ntry:\n  err4(x_ref)  # error!\nexcept Exception as e:\n  print(e)\n```\n\nThese restrictions exist to rule out aliasing, where two refs might refer to the\nsame mutable memory, making programs harder to reason about and transform.\nWeaker restrictions would also suffice, so some of these restrictions may be\nlifted as we improve JAX's ability to verify that no aliasing is present.\n\nThere are also restrictions stemming from undefined semantics, e.g. in the\npresence of parallelism or rematerialization:\n\n* **Can't `vmap` or `shard_map` a function that closes over refs**\n* **Can't apply `jax.remat`/`jax.checkpoint` to an impure function**\n\nFor example, here are ways you can and can't use `vmap` with impure functions:\n\n```{code-cell}\n# vmap over ref args is okay\ndef dist(x, y, out_ref):\n  assert x.ndim == y.ndim == 1\n  assert out_ref.ndim == 0\n  out_ref[...] = jnp.sum((x - y) ** 2)\n\nvecs = jnp.arange(12.).reshape(3, 4)\nout_ref = jax.new_ref(jnp.zeros((3, 3)))\njax.vmap(jax.vmap(dist, (0, None, 0)), (None, 0, 0))(vecs, vecs, out_ref)  # ok!\nprint(out_ref)\n```\n\n```{code-cell}\n# vmap with a closed-over ref is not\nx_ref = jax.new_ref(0.)\n\ndef err5(x):\n  x_ref[...] = x\n\ntry:\n  jax.vmap(err5)(jnp.arange(3.))  # error!\nexcept Exception as e:\n  print(e)\n```\n\nThe latter is an error because it's not clear which value `x_ref` should be\nafter we run `jax.vmap(err5)`.\n\n### `Ref`s and automatic differentiation\n\nAutodiff can be applied to pure functions as before, even if they use array refs\ninternally. For example:\n\n```{code-cell}\n@jax.jit\ndef pure2(x):\n  ref = jax.new_ref(x)\n  ref[...] = ref[...] + ref[...]\n  return ref[...]\n\nprint(jax.grad(pure1)(3.0))  # 2.0\n```\n\nAutodiff can also be applied to functions that take array refs as arguments, if\nthose arguments are only used for plumbing and not involved in differentiation:\n\n```{code-cell}\n# error\ndef err6(x, some_plumbing_ref):\n  y = x + x\n  some_plumbing_ref[...] += y\n  return y\n\n# fine\ndef foo(x, some_plumbing_ref):\n  y = x + x\n  some_plumbing_ref[...] += jax.lax.stop_gradient(y)\n  return y\n```\n\nYou can combine plumbing refs with `custom_vjp` to plumb data out of the\nbackward pass of a differentiated function:\n\n```{code-cell}\n# First, define the helper `stash_grads`:\n\n@jax.custom_vjp\ndef stash_grads(grads_ref, x):\n  return x\n\ndef stash_grads_fwd(grads_ref, x):\n  return x, grads_ref\n\ndef stash_grads_bwd(grads_ref, g):\n  grads_ref[...] = g\n  return None, g\n\nstash_grads.defvjp(stash_grads_fwd, stash_grads_bwd)\n```\n\n```{code-cell}\n# Now, use `stash_grads` to stash intermediate gradients:\n\ndef f(x, grads_ref):\n  x = jnp.sin(x)\n  x = stash_grads(grads_ref, x)\n  return x\n\ngrads_ref = jax.new_ref(0.)\nf(1., grads_ref)\nprint(grads_ref)\n```\n\nNotice `stash_grads_fwd` is returning a `Ref` here. That's a special\nallowance for `custom_vjp` fwd rules: it's really syntax for indicating which\nref arguments should be shared by both the fwd and bwd rules. So any refs\nreturned by a fwd rule must be arguments to that fwd rule.\n\n### `Ref`s and performance\n\nAt the top level, when calling `jit`\\-decorated functions, `Ref`s obviate\nthe need for donation, since they are effectively always donated:\n\n```{code-cell}\n@jax.jit\ndef sin_inplace(x_ref):\n  x_ref[...] = jnp.sin(x_ref[...])\n\nx_ref = jax.new_ref(jnp.arange(3.))\nprint(x_ref.unsafe_buffer_pointer(), x_ref)\nsin_inplace(x_ref)\nprint(x_ref.unsafe_buffer_pointer(), x_ref)\n```\n\nHere `sin_inplace` operates in-place, updating the buffer backing `x_ref` so\nthat its address stays the same.\n\nUnder a `jit`, you should expect array references to point to fixed buffer\naddresses, and for indexed updates to be performed in-place.\n\n**Temporary caveat:** dispatch from Python to impure `jit`\\-compiled functions\nthat take `Ref` inputs is currently slower than dispatch to pure\n`jit`\\-compiled functions, since it takes a less optimized path.\n\n### `foreach`, a new way to write `scan`\n\nAs you may know, `jax.lax.scan` is a loop construct with a built-in fixed access\npattern for scanned-over inputs and outputs. The access pattern is built in for\nautodiff reasons: if we were instead to slice into immutable inputs directly,\nreverse-mode autodiff would end up creating one-hot gradients and summing them\nup, which can be asymptotically inefficient. See [Sec 5.3.3 of the Dex\npaper](https://arxiv.org/pdf/2104.05372).\n\nBut reading slices of `Ref`s doesn't have this efficiency problem: when we\napply reverse-mode autodiff, we always generate in-place accumulation\noperations. As a result, we no longer need to be constrained by `scan`'s fixed\naccess pattern. We can write more flexible loops, e.g. with non-sequential\naccess.\n\nMoreover, having mutation available allows for some syntax tricks, like in this\nrecipe for a `foreach` decorator:\n\n```{code-cell}\nimport jax\nimport jax.numpy as jnp\nfrom jax.lax import scan\n\ndef foreach(*args):\n  def decorator(body):\n    return scan(lambda _, elts: (None, body(*elts)), None, args)[1]\n  return decorator\n```\n\n```{code-cell}\nr = jax.new_ref(0)\nxs = jnp.arange(10)\n\n@foreach(xs)\ndef ys(x):\n  r[...] += x\n  return x * 2\n\nprint(r)   # Ref(45, dtype=int32)\nprint(ys)  # [ 0  2  4  6  8 10 12 14 16 18]\n```\n\nHere, the loop runs immediately, updating `r` in-place and binding `ys` to be\nthe mapped result.\n"}, "files_index": [{"path": ".bazelrc", "type": "blob", "size": 25671}, {"path": ".bazelversion", "type": "blob", "size": 6}, {"path": ".editorconfig", "type": "blob", "size": 176}, {"path": ".github", "type": "tree", "size": null}, {"path": ".github/ISSUE_TEMPLATE", "type": "tree", "size": null}, {"path": ".github/ISSUE_TEMPLATE/Feature_request.md", "type": "blob", "size": 243}, {"path": ".github/ISSUE_TEMPLATE/bug-report.yml", "type": "blob", "size": 1385}, {"path": ".github/ISSUE_TEMPLATE/config.yml", "type": "blob", "size": 191}, {"path": ".github/actionlint.yaml", "type": "blob", "size": 1633}, {"path": ".github/actions", "type": "tree", "size": null}, {"path": ".github/actions/download-jax-cpu-wheels", "type": "tree", "size": null}, {"path": ".github/actions/download-jax-cpu-wheels/action.yml", "type": "blob", "size": 4865}, {"path": ".github/actions/download-jax-cuda-wheels", "type": "tree", "size": null}, {"path": ".github/actions/download-jax-cuda-wheels/action.yml", "type": "blob", "size": 4692}, {"path": ".github/dependabot.yml", "type": "blob", "size": 517}, {"path": ".github/workflows", "type": "tree", "size": null}, {"path": ".github/workflows/README.md", "type": "blob", "size": 586}, {"path": ".github/workflows/asan.yaml", "type": "blob", "size": 3596}, {"path": ".github/workflows/bazel_cpu.yml", "type": "blob", "size": 4024}, {"path": ".github/workflows/bazel_cpu_presubmit.yml", "type": "blob", "size": 1812}, {"path": ".github/workflows/bazel_cuda.yml", "type": "blob", "size": 5797}, {"path": ".github/workflows/bazel_cuda_h100_b200.yml", "type": "blob", "size": 6575}, {"path": ".github/workflows/bazel_cuda_presubmit.yml", "type": "blob", "size": 2376}, {"path": ".github/workflows/bazel_test_tpu.yml", "type": "blob", "size": 6238}, {"path": ".github/workflows/build_artifacts.yml", "type": "blob", "size": 6376}, {"path": ".github/workflows/ci-build.yaml", "type": "blob", "size": 8800}, {"path": ".github/workflows/cloud-tpu-ci-nightly.yml", "type": "blob", "size": 7250}, {"path": ".github/workflows/cloud-tpu-ci-presubmit.yml", "type": "blob", "size": 1360}, {"path": ".github/workflows/community_release_actions.yml", "type": "blob", "size": 1053}, {"path": ".github/workflows/jax-array-api.yml", "type": "blob", "size": 1407}, {"path": ".github/workflows/k8s.yaml", "type": "blob", "size": 3903}, {"path": ".github/workflows/metal_plugin_ci.yml", "type": "blob", "size": 1727}, {"path": ".github/workflows/numpy_nightly.yml", "type": "blob", "size": 2940}, {"path": ".github/workflows/oldest_supported_numpy.yml", "type": "blob", "size": 1899}, {"path": ".github/workflows/pytest_cpu.yml", "type": "blob", "size": 3538}, {"path": ".github/workflows/pytest_cuda.yml", "type": "blob", "size": 4862}, {"path": ".github/workflows/pytest_tpu.yml", "type": "blob", "size": 5623}, {"path": ".github/workflows/release-notification.yml", "type": "blob", "size": 734}, {"path": ".github/workflows/rocm-ci.yml", "type": "blob", "size": 1991}, {"path": ".github/workflows/self_hosted_runner_utils", "type": "tree", "size": null}, {"path": ".github/workflows/self_hosted_runner_utils/README.md", "type": "blob", "size": 176}, {"path": ".github/workflows/self_hosted_runner_utils/runner.env", "type": "blob", "size": 101}, {"path": ".github/workflows/self_hosted_runner_utils/setup_runner.sh", "type": "blob", "size": 3589}, {"path": ".github/workflows/self_hosted_runner_utils/start_github_runner.sh", "type": "blob", "size": 781}, {"path": ".github/workflows/self_hosted_runner_utils/validate_job.sh", "type": "blob", "size": 1364}, {"path": ".github/workflows/tsan-suppressions_3.13.txt", "type": "blob", "size": 1362}, {"path": ".github/workflows/tsan-suppressions_3.14.txt", "type": "blob", "size": 686}, {"path": ".github/workflows/tsan.yaml", "type": "blob", "size": 9545}, {"path": ".github/workflows/upstream-nightly.yml", "type": "blob", "size": 3115}, {"path": ".github/workflows/wheel_tests_continuous.yml", "type": "blob", "size": 13128}, {"path": ".github/workflows/wheel_tests_nightly_release.yml", "type": "blob", "size": 15739}, {"path": ".gitignore", "type": "blob", "size": 381}, {"path": ".pre-commit-config.yaml", "type": "blob", "size": 1481}, {"path": ".readthedocs.yml", "type": "blob", "size": 1132}, {"path": "AUTHORS", "type": "blob", "size": 313}, {"path": "BUILD.bazel", "type": "blob", "size": 4660}, {"path": "CHANGELOG.md", "type": "blob", "size": 176444}, {"path": "CITATION.bib", "type": "blob", "size": 408}, {"path": "CONTRIBUTING.md", "type": "blob", "size": 144}, {"path": "LICENSE", "type": "blob", "size": 11358}, {"path": "README.md", "type": "blob", "size": 10954}, {"path": "WORKSPACE", "type": "blob", "size": 4834}, {"path": "benchmarks", "type": "tree", "size": null}, {"path": "benchmarks/api_benchmark.py", "type": "blob", "size": 25068}, {"path": "benchmarks/linalg_benchmark.py", "type": "blob", "size": 1196}, {"path": "benchmarks/math_benchmark.py", "type": "blob", "size": 4147}, {"path": "benchmarks/mosaic", "type": "tree", "size": null}, {"path": "benchmarks/mosaic/BUILD", "type": "blob", "size": 1212}, {"path": "benchmarks/mosaic/matmul_bench.py", "type": "blob", "size": 3334}, {"path": "benchmarks/random_benchmark.py", "type": "blob", "size": 3064}, {"path": "benchmarks/shape_poly_benchmark.py", "type": "blob", "size": 2441}, {"path": "benchmarks/sparse_benchmark.py", "type": "blob", "size": 5138}, {"path": "benchmarks/tracing_benchmark.py", "type": "blob", "size": 5041}, {"path": "build", "type": "tree", "size": null}, {"path": "build/BUILD.bazel", "type": "blob", "size": 2839}, {"path": "build/build.py", "type": "blob", "size": 24943}, {"path": "build/collect-profile-requirements.txt", "type": "blob", "size": 149}, {"path": "build/freethreading-requirements.txt", "type": "blob", "size": 151}, {"path": "build/nonfreethreading-requirements.txt", "type": "blob", "size": 421}, {"path": "build/nvidia-requirements.txt", "type": "blob", "size": 1602}, {"path": "build/parallel_accelerator_execute.sh", "type": "blob", "size": 3408}, {"path": "build/requirements.in", "type": "blob", "size": 740}, {"path": "build/requirements_lock_3_11.txt", "type": "blob", "size": 80734}, {"path": "build/requirements_lock_3_12.txt", "type": "blob", "size": 80734}, {"path": "build/requirements_lock_3_13.txt", "type": "blob", "size": 81514}, {"path": "build/requirements_lock_3_13_ft.txt", "type": "blob", "size": 71078}, {"path": "build/requirements_lock_3_14.txt", "type": "blob", "size": 44954}, {"path": "build/requirements_lock_3_14_ft.txt", "type": "blob", "size": 44954}, {"path": "build/rocm", "type": "tree", "size": null}, {"path": "build/rocm/Dockerfile.ms", "type": "blob", "size": 3178}, {"path": "build/rocm/README.md", "type": "blob", "size": 7075}, {"path": "build/rocm/build_common.sh", "type": "blob", "size": 1738}, {"path": "build/rocm/build_rocm.sh", "type": "blob", "size": 1051}, {"path": "build/rocm/build_wheels", "type": "tree", "size": null}, {"path": "build/rocm/build_wheels/Dockerfile.manylinux_2_28_x86_64.rocm", "type": "blob", "size": 1811}, {"path": "build/rocm/build_wheels/clang.cfg", "type": "blob", "size": 129}, {"path": "build/rocm/ci_build", "type": "blob", "size": 9047}, {"path": "build/rocm/ci_build.sh", "type": "blob", "size": 4945}, {"path": "build/rocm/custom_install.sh", "type": "blob", "size": 996}, {"path": "build/rocm/dev_build_rocm.py", "type": "blob", "size": 5032}, {"path": "build/rocm/docker", "type": "tree", "size": null}, {"path": "build/rocm/docker/Dockerfile.jax-ubu22", "type": "blob", "size": 2228}, {"path": "build/rocm/docker/Dockerfile.jax-ubu24", "type": "blob", "size": 2185}, {"path": "build/rocm/docker/Makefile", "type": "blob", "size": 441}, {"path": "build/rocm/run_multi_gpu.sh", "type": "blob", "size": 2866}, {"path": "build/rocm/run_single_gpu.py", "type": "blob", "size": 7310}, {"path": "build/rocm/setup.rocm.sh", "type": "blob", "size": 3032}, {"path": "build/rocm/tools", "type": "tree", "size": null}, {"path": "build/rocm/tools/blacken.sh", "type": "blob", "size": 69}, {"path": "build/rocm/tools/build_wheels.py", "type": "blob", "size": 10526}, {"path": "build/rocm/tools/fixwheel.py", "type": "blob", "size": 2944}, {"path": "build/rocm/tools/get_rocm.py", "type": "blob", "size": 9245}, {"path": "build/rocm/tools/libc.py", "type": "blob", "size": 1642}, {"path": "build/rocm/tools/symbols.py", "type": "blob", "size": 1670}, {"path": "build/test-requirements.txt", "type": "blob", "size": 324}, {"path": "build/tools", "type": "tree", "size": null}, {"path": "build/tools/command.py", "type": "blob", "size": 3282}, {"path": "build/tools/utils.py", "type": "blob", "size": 9521}, {"path": "build_wheel.py", "type": "blob", "size": 3407}, {"path": "ci", "type": "tree", "size": null}, {"path": "ci/README.md", "type": "blob", "size": 15310}, {"path": "ci/build_artifacts.sh", "type": "blob", "size": 5775}, {"path": "ci/envs", "type": "tree", "size": null}, {"path": "ci/envs/README.md", "type": "blob", "size": 12723}, {"path": "ci/envs/default.env", "type": "blob", "size": 4037}, {"path": "ci/envs/docker.env", "type": "blob", "size": 1893}, {"path": "ci/jax_ci_system.png", "type": "blob", "size": 535880}, {"path": "ci/k8s", "type": "tree", "size": null}, {"path": "ci/k8s/indexed-job.yaml", "type": "blob", "size": 926}, {"path": "ci/k8s/jobset.yaml", "type": "blob", "size": 913}, {"path": "ci/run_bazel_test_cpu_rbe.sh", "type": "blob", "size": 4944}, {"path": "ci/run_bazel_test_cuda_non_rbe.sh", "type": "blob", "size": 6496}, {"path": "ci/run_bazel_test_cuda_rbe.sh", "type": "blob", "size": 2806}, {"path": "ci/run_bazel_test_tpu.sh", "type": "blob", "size": 8735}, {"path": "ci/run_pytest_cpu.sh", "type": "blob", "size": 2008}, {"path": "ci/run_pytest_cuda.sh", "type": "blob", "size": 4804}, {"path": "ci/run_pytest_tpu.sh", "type": "blob", "size": 5030}, {"path": "ci/utilities", "type": "tree", "size": null}, {"path": "ci/utilities/README.md", "type": "blob", "size": 836}, {"path": "ci/utilities/convert_msys_paths_to_win_paths.py", "type": "blob", "size": 2710}, {"path": "ci/utilities/install_wheels_locally.sh", "type": "blob", "size": 2220}, {"path": "ci/utilities/run_auditwheel.sh", "type": "blob", "size": 2300}, {"path": "ci/utilities/run_docker_container.sh", "type": "blob", "size": 3607}, {"path": "ci/utilities/setup_build_environment.sh", "type": "blob", "size": 3953}, {"path": "cloud_tpu_colabs", "type": "tree", "size": null}, {"path": "cloud_tpu_colabs/JAX_NeurIPS_2020_demo.ipynb", "type": "blob", "size": 10739}, {"path": "cloud_tpu_colabs/JAX_demo.ipynb", "type": "blob", "size": 19329}, {"path": "cloud_tpu_colabs/Lorentz_ODE_Solver.ipynb", "type": "blob", "size": 14334}, {"path": "cloud_tpu_colabs/Pmap_Cookbook.ipynb", "type": "blob", "size": 16760}, {"path": "cloud_tpu_colabs/README.md", "type": "blob", "size": 4374}, {"path": "cloud_tpu_colabs/Wave_Equation.ipynb", "type": "blob", "size": 15752}, {"path": "cloud_tpu_colabs/images", "type": "tree", "size": null}, {"path": "cloud_tpu_colabs/images/lorentz.png", "type": "blob", "size": 574004}, {"path": "cloud_tpu_colabs/images/nested_pmap.png", "type": "blob", "size": 23866}, {"path": "cloud_tpu_colabs/images/wave_movie.gif", "type": "blob", "size": 5913790}, {"path": "conftest.py", "type": "blob", "size": 3187}, {"path": "docs", "type": "tree", "size": null}, {"path": "docs/README.md", "type": "blob", "size": 127}, {"path": "docs/_static", "type": "tree", "size": null}, {"path": "docs/_static/debugger.gif", "type": "blob", "size": 434368}, {"path": "docs/_static/device_memory_profile.svg", "type": "blob", "size": 20209}, {"path": "docs/_static/device_memory_profile_leak1.svg", "type": "blob", "size": 14499}, {"path": "docs/_static/device_memory_profile_leak2.svg", "type": "blob", "size": 12707}, {"path": "docs/_static/distributed_data_loading", "type": "tree", "size": null}, {"path": "docs/_static/distributed_data_loading/1.svg", "type": "blob", "size": 62562}, {"path": "docs/_static/distributed_data_loading/10.svg", "type": "blob", "size": 52335}, {"path": "docs/_static/distributed_data_loading/11.svg", "type": "blob", "size": 74936}, {"path": "docs/_static/distributed_data_loading/12.svg", "type": "blob", "size": 83761}, {"path": "docs/_static/distributed_data_loading/13.svg", "type": "blob", "size": 95338}, {"path": "docs/_static/distributed_data_loading/14.svg", "type": "blob", "size": 111778}, {"path": "docs/_static/distributed_data_loading/15.svg", "type": "blob", "size": 118777}, {"path": "docs/_static/distributed_data_loading/16.svg", "type": "blob", "size": 93252}, {"path": "docs/_static/distributed_data_loading/17.svg", "type": "blob", "size": 94837}, {"path": "docs/_static/distributed_data_loading/18.svg", "type": "blob", "size": 91001}, {"path": "docs/_static/distributed_data_loading/19.svg", "type": "blob", "size": 98193}, {"path": "docs/_static/distributed_data_loading/2.svg", "type": "blob", "size": 60319}, {"path": "docs/_static/distributed_data_loading/20.svg", "type": "blob", "size": 104317}, {"path": "docs/_static/distributed_data_loading/21.svg", "type": "blob", "size": 98389}, {"path": "docs/_static/distributed_data_loading/22.svg", "type": "blob", "size": 77482}, {"path": "docs/_static/distributed_data_loading/3.svg", "type": "blob", "size": 45653}, {"path": "docs/_static/distributed_data_loading/4.svg", "type": "blob", "size": 125834}, {"path": "docs/_static/distributed_data_loading/5.svg", "type": "blob", "size": 85813}, {"path": "docs/_static/distributed_data_loading/6.svg", "type": "blob", "size": 81757}, {"path": "docs/_static/distributed_data_loading/7.svg", "type": "blob", "size": 111077}, {"path": "docs/_static/distributed_data_loading/8.svg", "type": "blob", "size": 96519}, {"path": "docs/_static/distributed_data_loading/9.svg", "type": "blob", "size": 68467}, {"path": "docs/_static/favicon.png", "type": "blob", "size": 6644}, {"path": "docs/_static/jax-hero.svg", "type": "blob", "size": 9524}, {"path": "docs/_static/jax_logo_250px.png", "type": "blob", "size": 34025}, {"path": "docs/_static/mesh.jpg", "type": "blob", "size": 49143}, {"path": "docs/_static/multi_host.jpg", "type": "blob", "size": 38964}, {"path": "docs/_static/multi_process", "type": "tree", "size": null}, {"path": "docs/_static/multi_process/controller_and_local_devices.png", "type": "blob", "size": 132362}, {"path": "docs/_static/multi_process/mcjax_overview.png", "type": "blob", "size": 145901}, {"path": "docs/_static/pallas", "type": "tree", "size": null}, {"path": "docs/_static/pallas/BlockSpec.png", "type": "blob", "size": 20156}, {"path": "docs/_static/pallas/distributed", "type": "tree", "size": null}, {"path": "docs/_static/pallas/distributed/all_gather.svg", "type": "blob", "size": 59420}, {"path": "docs/_static/pallas/distributed/race_condition.svg", "type": "blob", "size": 103133}, {"path": "docs/_static/pallas/distributed/rdma_recv.svg", "type": "blob", "size": 64881}, {"path": "docs/_static/pallas/distributed/rdma_send.svg", "type": "blob", "size": 65189}, {"path": "docs/_static/pallas/distributed/rdma_start.svg", "type": "blob", "size": 64875}, {"path": "docs/_static/pallas/distributed/reduce_scatter_1.svg", "type": "blob", "size": 46856}, {"path": "docs/_static/pallas/distributed/reduce_scatter_2.svg", "type": "blob", "size": 65543}, {"path": "docs/_static/pallas/distributed/reduce_sum_1.svg", "type": "blob", "size": 26417}, {"path": "docs/_static/pallas/distributed/reduce_sum_2.svg", "type": "blob", "size": 71285}, {"path": "docs/_static/pallas/gpu", "type": "tree", "size": null}, {"path": "docs/_static/pallas/gpu/collective_mma.svg", "type": "blob", "size": 4007}, {"path": "docs/_static/pallas/gpu/grid_tiling_off.svg", "type": "blob", "size": 6931}, {"path": "docs/_static/pallas/gpu/grid_tiling_on.svg", "type": "blob", "size": 7275}, {"path": "docs/_static/pallas/gpu/memory_spaces.svg", "type": "blob", "size": 5516}, {"path": "docs/_static/pallas/gpu/nvidia_sm.svg", "type": "blob", "size": 5499}, {"path": "docs/_static/pallas/gpu/pipeline_matmul.svg", "type": "blob", "size": 66725}, {"path": "docs/_static/pallas/gpu/pipeline_matmul_ws.svg", "type": "blob", "size": 35449}, {"path": "docs/_static/pallas/gpu/warp_specialization.svg", "type": "blob", "size": 177976}, {"path": "docs/_static/pallas/grid.png", "type": "blob", "size": 10334}, {"path": "docs/_static/pallas/pallas_flow.png", "type": "blob", "size": 122667}, {"path": "docs/_static/pallas/pipelining_bandwidth_bound.svg", "type": "blob", "size": 217286}, {"path": "docs/_static/pallas/pipelining_compute_bound.svg", "type": "blob", "size": 119759}, {"path": "docs/_static/pallas/pipelining_example.svg", "type": "blob", "size": 217132}, {"path": "docs/_static/pallas/pipelining_latency_multistage.svg", "type": "blob", "size": 119828}, {"path": "docs/_static/pallas/pipelining_mem_hierarchy.svg", "type": "blob", "size": 1725}, {"path": "docs/_static/pallas/sparse", "type": "tree", "size": null}, {"path": "docs/_static/pallas/sparse/block_coo.svg", "type": "blob", "size": 39781}, {"path": "docs/_static/pallas/sparse/prefetch_map.svg", "type": "blob", "size": 54072}, {"path": "docs/_static/pallas/sparse/sparse_matmul.svg", "type": "blob", "size": 26470}, {"path": "docs/_static/pallas/vector_layout_example.svg", "type": "blob", "size": 26385}, {"path": "docs/_static/partition_spec_none_y.png", "type": "blob", "size": 24359}, {"path": "docs/_static/partition_spec_x_none.png", "type": "blob", "size": 13598}, {"path": "docs/_static/partition_spec_x_y.png", "type": "blob", "size": 6994}, {"path": "docs/_static/partition_spec_xy.png", "type": "blob", "size": 7513}, {"path": "docs/_static/partition_spec_y_none.png", "type": "blob", "size": 22334}, {"path": "docs/_static/perfetto.png", "type": "blob", "size": 98811}, {"path": "docs/_static/style.css", "type": "blob", "size": 5547}, {"path": "docs/_static/tensorboard_profiler.png", "type": "blob", "size": 29822}, {"path": "docs/_static/type_lattice.svg", "type": "blob", "size": 22788}, {"path": "docs/_static/vscode-completion.png", "type": "blob", "size": 31521}, {"path": "docs/_static/xla_spmd.jpg", "type": "blob", "size": 42269}, {"path": "docs/_templates", "type": "tree", "size": null}, {"path": "docs/_templates/layout.html", "type": "blob", "size": 85}, {"path": "docs/_tutorials", "type": "tree", "size": null}, {"path": "docs/_tutorials/advanced-compilation.md", "type": "blob", "size": 285}, {"path": "docs/_tutorials/advanced-debugging.md", "type": "blob", "size": 514}, {"path": "docs/_tutorials/index.rst", "type": "blob", "size": 812}, {"path": "docs/_tutorials/parallelism.md", "type": "blob", "size": 341}, {"path": "docs/_tutorials/profiling-and-performance.md", "type": "blob", "size": 334}, {"path": "docs/_tutorials/simple-neural-network.md", "type": "blob", "size": 183}, {"path": "docs/about.md", "type": "blob", "size": 5511}, {"path": "docs/advanced-autodiff.md", "type": "blob", "size": 69206}, {"path": "docs/advanced_guides.rst", "type": "blob", "size": 1994}, {"path": "docs/aot.md", "type": "blob", "size": 10084}, {"path": "docs/api_compatibility.md", "type": "blob", "size": 5005}, {"path": "docs/array_refs.ipynb", "type": "blob", "size": 18791}, {"path": "docs/array_refs.md", "type": "blob", "size": 11823}, {"path": "docs/array_refs.py", "type": "blob", "size": 11857}, {"path": "docs/async_dispatch.rst", "type": "blob", "size": 5397}, {"path": "docs/autodidax.ipynb", "type": "blob", "size": 145197}, {"path": "docs/autodidax.md", "type": "blob", "size": 106469}, {"path": "docs/autodidax.py", "type": "blob", "size": 106360}, {"path": "docs/autodidax2_part1.ipynb", "type": "blob", "size": 35169}, {"path": "docs/autodidax2_part1.md", "type": "blob", "size": 19841}, {"path": "docs/autodidax2_part1.py", "type": "blob", "size": 19695}, {"path": "docs/automatic-differentiation.md", "type": "blob", "size": 8508}, {"path": "docs/automatic-vectorization.md", "type": "blob", "size": 3725}, {"path": "docs/beginner_guide.rst", "type": "blob", "size": 2055}, {"path": "docs/build_custom_gpu.sh", "type": "blob", "size": 1365}, {"path": "docs/building_on_jax.md", "type": "blob", "size": 4698}, {"path": "docs/changelog.md", "type": "blob", "size": 33}, {"path": "docs/concurrency.rst", "type": "blob", "size": 609}, {"path": "docs/conf.py", "type": "blob", "size": 13098}, {"path": "docs/config_options.rst", "type": "blob", "size": 1976}, {"path": "docs/contributing.md", "type": "blob", "size": 7355}, {"path": "docs/contributor_guide.rst", "type": "blob", "size": 587}, {"path": "docs/control-flow.md", "type": "blob", "size": 11893}, {"path": "docs/debugging.md", "type": "blob", "size": 8635}, {"path": "docs/debugging", "type": "tree", "size": null}, {"path": "docs/debugging/checkify_guide.md", "type": "blob", "size": 10348}, {"path": "docs/debugging/flags.md", "type": "blob", "size": 2948}, {"path": "docs/debugging/index.md", "type": "blob", "size": 4343}, {"path": "docs/debugging/print_breakpoint.md", "type": "blob", "size": 11013}, {"path": "docs/debugging/xla_metadata.md", "type": "blob", "size": 5941}, {"path": "docs/default_dtypes.md", "type": "blob", "size": 3602}, {"path": "docs/deprecation.md", "type": "blob", "size": 2470}, {"path": "docs/developer.md", "type": "blob", "size": 33339}, {"path": "docs/device_memory_profiling.md", "type": "blob", "size": 4950}, {"path": "docs/direct_linearize_migration.md", "type": "blob", "size": 1476}, {"path": "docs/distributed_data_loading.md", "type": "blob", "size": 22986}, {"path": "docs/errors.rst", "type": "blob", "size": 504}, {"path": "docs/export", "type": "tree", "size": null}, {"path": "docs/export/export.md", "type": "blob", "size": 35208}, {"path": "docs/export/index.rst", "type": "blob", "size": 157}, {"path": "docs/export/jax2tf.md", "type": "blob", "size": 153}, {"path": "docs/export/shape_poly.md", "type": "blob", "size": 25885}, {"path": "docs/extensions.rst", "type": "blob", "size": 371}, {"path": "docs/external-callbacks.md", "type": "blob", "size": 15541}, {"path": "docs/faq.rst", "type": "blob", "size": 32832}, {"path": "docs/ffi.ipynb", "type": "blob", "size": 43205}, {"path": "docs/ffi.md", "type": "blob", "size": 34052}, {"path": "docs/ffi", "type": "tree", "size": null}, {"path": "docs/ffi/.gitignore", "type": "blob", "size": 7}, {"path": "docs/ffi/CMakeLists.txt", "type": "blob", "size": 601}, {"path": "docs/ffi/rms_norm.cc", "type": "blob", "size": 5342}, {"path": "docs/glossary.rst", "type": "blob", "size": 4587}, {"path": "docs/gpu_memory_allocation.rst", "type": "blob", "size": 4816}, {"path": "docs/gpu_performance_tips.md", "type": "blob", "size": 24221}, {"path": "docs/gradient-checkpointing.md", "type": "blob", "size": 22545}, {"path": "docs/hero.html", "type": "blob", "size": 398}, {"path": "docs/index.rst", "type": "blob", "size": 5177}, {"path": "docs/installation.md", "type": "blob", "size": 14945}, {"path": "docs/internals", "type": "tree", "size": null}, {"path": "docs/internals/constants.md", "type": "blob", "size": 9320}, {"path": "docs/internals/index.rst", "type": "blob", "size": 402}, {"path": "docs/investigating_a_regression.md", "type": "blob", "size": 6750}, {"path": "docs/jax-101.rst", "type": "blob", "size": 587}, {"path": "docs/jax-primitives.md", "type": "blob", "size": 24510}, {"path": "docs/jax.debug.rst", "type": "blob", "size": 630}, {"path": "docs/jax.distributed.rst", "type": "blob", "size": 196}, {"path": "docs/jax.dlpack.rst", "type": "blob", "size": 188}, {"path": "docs/jax.dtypes.rst", "type": "blob", "size": 226}, {"path": "docs/jax.example_libraries.optimizers.rst", "type": "blob", "size": 195}, {"path": "docs/jax.example_libraries.rst", "type": "blob", "size": 943}, {"path": "docs/jax.example_libraries.stax.rst", "type": "blob", "size": 177}, {"path": "docs/jax.experimental.checkify.rst", "type": "blob", "size": 345}, {"path": "docs/jax.experimental.compilation_cache.rst", "type": "blob", "size": 265}, {"path": "docs/jax.experimental.custom_dce.rst", "type": "blob", "size": 210}, {"path": "docs/jax.experimental.custom_partitioning.rst", "type": "blob", "size": 198}, {"path": "docs/jax.experimental.jet.rst", "type": "blob", "size": 134}, {"path": "docs/jax.experimental.key_reuse.rst", "type": "blob", "size": 120}, {"path": "docs/jax.experimental.mesh_utils.rst", "type": "blob", "size": 225}, {"path": "docs/jax.experimental.multihost_utils.rst", "type": "blob", "size": 453}, {"path": "docs/jax.experimental.pallas.mosaic_gpu.rst", "type": "blob", "size": 908}, {"path": "docs/jax.experimental.pallas.rst", "type": "blob", "size": 577}, {"path": "docs/jax.experimental.pallas.tpu.rst", "type": "blob", "size": 248}, {"path": "docs/jax.experimental.pallas.triton.rst", "type": "blob", "size": 448}, {"path": "docs/jax.experimental.pjit.rst", "type": "blob", "size": 138}, {"path": "docs/jax.experimental.rst", "type": "blob", "size": 868}, {"path": "docs/jax.experimental.serialize_executable.rst", "type": "blob", "size": 241}, {"path": "docs/jax.experimental.shard_map.rst", "type": "blob", "size": 185}, {"path": "docs/jax.experimental.sparse.rst", "type": "blob", "size": 2480}, {"path": "docs/jax.export.rst", "type": "blob", "size": 1170}, {"path": "docs/jax.extend.core.rst", "type": "blob", "size": 242}, {"path": "docs/jax.extend.linear_util.rst", "type": "blob", "size": 264}, {"path": "docs/jax.extend.mlir.rst", "type": "blob", "size": 163}, {"path": "docs/jax.extend.random.rst", "type": "blob", "size": 266}, {"path": "docs/jax.extend.rst", "type": "blob", "size": 231}, {"path": "docs/jax.ffi.rst", "type": "blob", "size": 190}, {"path": "docs/jax.flatten_util.rst", "type": "blob", "size": 224}, {"path": "docs/jax.image.rst", "type": "blob", "size": 332}, {"path": "docs/jax.lax.rst", "type": "blob", "size": 4836}, {"path": "docs/jax.nn.initializers.rst", "type": "blob", "size": 841}, {"path": "docs/jax.nn.rst", "type": "blob", "size": 791}, {"path": "docs/jax.numpy.rst", "type": "blob", "size": 10382}, {"path": "docs/jax.ops.rst", "type": "blob", "size": 489}, {"path": "docs/jax.profiler.rst", "type": "blob", "size": 656}, {"path": "docs/jax.random.rst", "type": "blob", "size": 1079}, {"path": "docs/jax.ref.rst", "type": "blob", "size": 258}, {"path": "docs/jax.rst", "type": "blob", "size": 4638}, {"path": "docs/jax.scipy.rst", "type": "blob", "size": 7216}, {"path": "docs/jax.sharding.rst", "type": "blob", "size": 439}, {"path": "docs/jax.stages.rst", "type": "blob", "size": 507}, {"path": "docs/jax.test_util.rst", "type": "blob", "size": 238}, {"path": "docs/jax.tree.rst", "type": "blob", "size": 354}, {"path": "docs/jax.tree_util.rst", "type": "blob", "size": 832}, {"path": "docs/jax.typing.rst", "type": "blob", "size": 178}, {"path": "docs/jax_array_migration.md", "type": "blob", "size": 12108}, {"path": "docs/jaxpr.md", "type": "blob", "size": 13075}, {"path": "docs/jep", "type": "tree", "size": null}, {"path": "docs/jep/10657-sequencing-effects.md", "type": "blob", "size": 13374}, {"path": "docs/jep/11830-new-remat-checkpoint.md", "type": "blob", "size": 6705}, {"path": "docs/jep/12049-type-annotations.md", "type": "blob", "size": 30450}, {"path": "docs/jep/14273-shard-map.md", "type": "blob", "size": 25583}, {"path": "docs/jep/15856-jex.md", "type": "blob", "size": 7811}, {"path": "docs/jep/17111-shmap-transpose.md", "type": "blob", "size": 23200}, {"path": "docs/jep/18137-numpy-scipy-scope.md", "type": "blob", "size": 21005}, {"path": "docs/jep/2026-custom-derivatives.md", "type": "blob", "size": 19574}, {"path": "docs/jep/25516-effver.md", "type": "blob", "size": 5770}, {"path": "docs/jep/263-prng.md", "type": "blob", "size": 11637}, {"path": "docs/jep/28661-jax-array-protocol.md", "type": "blob", "size": 9910}, {"path": "docs/jep/4008-custom-vjp-update.md", "type": "blob", "size": 4989}, {"path": "docs/jep/4410-omnistaging.md", "type": "blob", "size": 14170}, {"path": "docs/jep/9263-typed-keys.md", "type": "blob", "size": 14671}, {"path": "docs/jep/9407-type-promotion.ipynb", "type": "blob", "size": 405037}, {"path": "docs/jep/9407-type-promotion.md", "type": "blob", "size": 43503}, {"path": "docs/jep/9419-jax-versioning.md", "type": "blob", "size": 9315}, {"path": "docs/jep/index.rst", "type": "blob", "size": 2672}, {"path": "docs/jit-compilation.md", "type": "blob", "size": 11153}, {"path": "docs/key-concepts.md", "type": "blob", "size": 7533}, {"path": "docs/multi_process.md", "type": "blob", "size": 29404}, {"path": "docs/notebooks", "type": "tree", "size": null}, {"path": "docs/notebooks/Common_Gotchas_in_JAX.ipynb", "type": "blob", "size": 43564}, {"path": "docs/notebooks/Common_Gotchas_in_JAX.md", "type": "blob", "size": 25754}, {"path": "docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb", "type": "blob", "size": 115639}, {"path": "docs/notebooks/Custom_derivative_rules_for_Python_code.md", "type": "blob", "size": 36123}, {"path": "docs/notebooks/Distributed_arrays_and_automatic_parallelization.ipynb", "type": "blob", "size": 100779}, {"path": "docs/notebooks/Distributed_arrays_and_automatic_parallelization.md", "type": "blob", "size": 22263}, {"path": "docs/notebooks/Neural_Network_and_Data_Loading.ipynb", "type": "blob", "size": 20450}, {"path": "docs/notebooks/Neural_Network_and_Data_Loading.md", "type": "blob", "size": 8974}, {"path": "docs/notebooks/README.md", "type": "blob", "size": 152}, {"path": "docs/notebooks/Writing_custom_interpreters_in_Jax.ipynb", "type": "blob", "size": 16997}, {"path": "docs/notebooks/Writing_custom_interpreters_in_Jax.md", "type": "blob", "size": 11841}, {"path": "docs/notebooks/autodiff_cookbook.ipynb", "type": "blob", "size": 57902}, {"path": "docs/notebooks/autodiff_cookbook.md", "type": "blob", "size": 38666}, {"path": "docs/notebooks/autodiff_remat.ipynb", "type": "blob", "size": 92499}, {"path": "docs/notebooks/autodiff_remat.md", "type": "blob", "size": 19932}, {"path": "docs/notebooks/colocated-python.ipynb", "type": "blob", "size": 13471}, {"path": "docs/notebooks/colocated-python.md", "type": "blob", "size": 9425}, {"path": "docs/notebooks/convolutions.ipynb", "type": "blob", "size": 526605}, {"path": "docs/notebooks/convolutions.md", "type": "blob", "size": 15668}, {"path": "docs/notebooks/explicit-sharding.ipynb", "type": "blob", "size": 28676}, {"path": "docs/notebooks/explicit-sharding.md", "type": "blob", "size": 18063}, {"path": "docs/notebooks/host-offloading.ipynb", "type": "blob", "size": 36748}, {"path": "docs/notebooks/host-offloading.md", "type": "blob", "size": 27377}, {"path": "docs/notebooks/layout.ipynb", "type": "blob", "size": 9915}, {"path": "docs/notebooks/layout.md", "type": "blob", "size": 6805}, {"path": "docs/notebooks/neural_network_with_tfds_data.ipynb", "type": "blob", "size": 14895}, {"path": "docs/notebooks/neural_network_with_tfds_data.md", "type": "blob", "size": 9248}, {"path": "docs/notebooks/shard_map.ipynb", "type": "blob", "size": 2388557}, {"path": "docs/notebooks/shard_map.md", "type": "blob", "size": 2363800}, {"path": "docs/notebooks/thinking_in_jax.ipynb", "type": "blob", "size": 27730}, {"path": "docs/notebooks/thinking_in_jax.md", "type": "blob", "size": 19223}, {"path": "docs/notebooks/vmapped_log_probs.ipynb", "type": "blob", "size": 41984}, {"path": "docs/notebooks/vmapped_log_probs.md", "type": "blob", "size": 6583}, {"path": "docs/notes.rst", "type": "blob", "size": 1238}, {"path": "docs/pallas", "type": "tree", "size": null}, {"path": "docs/pallas/CHANGELOG.md", "type": "blob", "size": 8364}, {"path": "docs/pallas/design", "type": "tree", "size": null}, {"path": "docs/pallas/design/async_note.md", "type": "blob", "size": 27102}, {"path": "docs/pallas/design/design.md", "type": "blob", "size": 23820}, {"path": "docs/pallas/design/index.rst", "type": "blob", "size": 115}, {"path": "docs/pallas/gpu", "type": "tree", "size": null}, {"path": "docs/pallas/gpu/index.rst", "type": "blob", "size": 241}, {"path": "docs/pallas/gpu/pipelining.ipynb", "type": "blob", "size": 23190}, {"path": "docs/pallas/gpu/pipelining.md", "type": "blob", "size": 19664}, {"path": "docs/pallas/gpu/reference.md", "type": "blob", "size": 49038}, {"path": "docs/pallas/grid_blockspec.md", "type": "blob", "size": 12512}, {"path": "docs/pallas/index.rst", "type": "blob", "size": 1158}, {"path": "docs/pallas/pipelining.ipynb", "type": "blob", "size": 39242}, {"path": "docs/pallas/pipelining.md", "type": "blob", "size": 30939}, {"path": "docs/pallas/quickstart.ipynb", "type": "blob", "size": 18725}, {"path": "docs/pallas/quickstart.md", "type": "blob", "size": 12799}, {"path": "docs/pallas/tpu", "type": "tree", "size": null}, {"path": "docs/pallas/tpu/details.rst", "type": "blob", "size": 17452}, {"path": "docs/pallas/tpu/distributed.ipynb", "type": "blob", "size": 84085}, {"path": "docs/pallas/tpu/distributed.md", "type": "blob", "size": 66389}, {"path": "docs/pallas/tpu/index.rst", "type": "blob", "size": 170}, {"path": "docs/pallas/tpu/matmul.ipynb", "type": "blob", "size": 43519}, {"path": "docs/pallas/tpu/matmul.md", "type": "blob", "size": 29914}, {"path": "docs/pallas/tpu/pipelining.ipynb", "type": "blob", "size": 67420}, {"path": "docs/pallas/tpu/pipelining.md", "type": "blob", "size": 62831}, {"path": "docs/pallas/tpu/prng.rst", "type": "blob", "size": 8542}, {"path": "docs/pallas/tpu/sparse.ipynb", "type": "blob", "size": 33028}, {"path": "docs/pallas/tpu/sparse.md", "type": "blob", "size": 26176}, {"path": "docs/persistent_compilation_cache.md", "type": "blob", "size": 10645}, {"path": "docs/profiling.md", "type": "blob", "size": 17490}, {"path": "docs/pytrees.md", "type": "blob", "size": 11050}, {"path": "docs/random-numbers.md", "type": "blob", "size": 9630}, {"path": "docs/rank_promotion_warning.rst", "type": "blob", "size": 1843}, {"path": "docs/requirements.txt", "type": "blob", "size": 702}, {"path": "docs/sharded-computation.ipynb", "type": "blob", "size": 42990}, {"path": "docs/sharded-computation.md", "type": "blob", "size": 13543}, {"path": "docs/shardy_jax_migration.md", "type": "blob", "size": 6471}, {"path": "docs/sphinxext", "type": "tree", "size": null}, {"path": "docs/sphinxext/jax_extensions.py", "type": "blob", "size": 2190}, {"path": "docs/sphinxext/jax_list_config_options.py", "type": "blob", "size": 5561}, {"path": "docs/sphinxext/source_include.py", "type": "blob", "size": 3521}, {"path": "docs/stateful-computations.md", "type": "blob", "size": 8088}, {"path": "docs/the-training-cookbook.py", "type": "blob", "size": 8981}, {"path": "docs/the-training-cookbook.rst", "type": "blob", "size": 26702}, {"path": "docs/tracing.md", "type": "blob", "size": 8114}, {"path": "docs/transfer_guard.rst", "type": "blob", "size": 2814}, {"path": "docs/type_promotion.rst", "type": "blob", "size": 14358}, {"path": "docs/working-with-pytrees.md", "type": "blob", "size": 19269}, {"path": "docs/xla_flags.md", "type": "blob", "size": 2332}, {"path": "examples", "type": "tree", "size": null}, {"path": "examples/__init__.py", "type": "blob", "size": 581}, {"path": "examples/advi.py", "type": "blob", "size": 4888}, {"path": "examples/datasets.py", "type": "blob", "size": 3183}, {"path": "examples/differentially_private_sgd.py", "type": "blob", "size": 9012}, {"path": "examples/examples_test.py", "type": "blob", "size": 2086}, {"path": "examples/ffi", "type": "tree", "size": null}, {"path": "examples/ffi/CMakeLists.txt", "type": "blob", "size": 1603}, {"path": "examples/ffi/README.md", "type": "blob", "size": 1776}, {"path": "examples/ffi/pyproject.toml", "type": "blob", "size": 283}, {"path": "examples/ffi/src", "type": "tree", "size": null}, {"path": "examples/ffi/src/jax_ffi_example", "type": "tree", "size": null}, {"path": "examples/ffi/src/jax_ffi_example/__init__.py", "type": "blob", "size": 581}, {"path": "examples/ffi/src/jax_ffi_example/cpu_examples.cc", "type": "blob", "size": 4971}, {"path": "examples/ffi/src/jax_ffi_example/cpu_examples.py", "type": "blob", "size": 1365}, {"path": "examples/ffi/src/jax_ffi_example/cuda_examples.cu", "type": "blob", "size": 6261}, {"path": "examples/ffi/src/jax_ffi_example/cuda_examples.py", "type": "blob", "size": 2132}, {"path": "examples/ffi/src/jax_ffi_example/gpu_examples.cc", "type": "blob", "size": 2125}, {"path": "examples/ffi/src/jax_ffi_example/gpu_examples.py", "type": "blob", "size": 913}, {"path": "examples/ffi/src/jax_ffi_example/rms_norm.cc", "type": "blob", "size": 8124}, {"path": "examples/ffi/src/jax_ffi_example/rms_norm.py", "type": "blob", "size": 2587}, {"path": "examples/ffi/tests", "type": "tree", "size": null}, {"path": "examples/ffi/tests/cpu_examples_test.py", "type": "blob", "size": 3460}, {"path": "examples/ffi/tests/cuda_examples_test.py", "type": "blob", "size": 2557}, {"path": "examples/ffi/tests/gpu_examples_test.py", "type": "blob", "size": 1280}, {"path": "examples/ffi/tests/rms_norm_test.py", "type": "blob", "size": 1913}, {"path": "examples/gaussian_process_regression.py", "type": "blob", "size": 4553}, {"path": "examples/jax_cpp", "type": "tree", "size": null}, {"path": "examples/jax_cpp/BUILD", "type": "blob", "size": 1460}, {"path": "examples/jax_cpp/main.cc", "type": "blob", "size": 4097}, {"path": "examples/jax_cpp/prog.py", "type": "blob", "size": 690}, {"path": "examples/k8s", "type": "tree", "size": null}, {"path": "examples/k8s/example.yaml", "type": "blob", "size": 1232}, {"path": "examples/k8s/svc-acct.yaml", "type": "blob", "size": 635}, {"path": "examples/kernel_lsq.py", "type": "blob", "size": 2569}, {"path": "examples/mnist_classifier.py", "type": "blob", "size": 3163}, {"path": "examples/mnist_classifier_fromscratch.py", "type": "blob", "size": 3063}, {"path": "examples/mnist_vae.py", "type": "blob", "size": 5030}, {"path": "examples/onnx2xla.py", "type": "blob", "size": 4780}, {"path": "examples/spmd_mnist_classifier_fromscratch.py", "type": "blob", "size": 4829}, {"path": "images", "type": "tree", "size": null}, {"path": "images/jax_logo.png", "type": "blob", "size": 144244}, {"path": "images/jax_logo.svg", "type": "blob", "size": 3976}, {"path": "images/jax_logo_250px.png", "type": "blob", "size": 34025}, {"path": "images/jax_logo_500px.png", "type": "blob", "size": 49222}, {"path": "images/lifecycle.png", "type": "blob", "size": 76284}, {"path": "jax", "type": "tree", "size": null}, {"path": "jax/BUILD", "type": "blob", "size": 12256}, {"path": "jax/__init__.py", "type": "blob", "size": 9896}, {"path": "jax/_src", "type": "tree", "size": null}, {"path": "jax/_src/BUILD", "type": "blob", "size": 32482}, {"path": "jax/_src/__init__.py", "type": "blob", "size": 581}, {"path": "jax/_src/abstract_arrays.py", "type": "blob", "size": 6166}, {"path": "jax/_src/ad_checkpoint.py", "type": "blob", "size": 41901}, {"path": "jax/_src/ad_util.py", "type": "blob", "size": 4454}, {"path": "jax/_src/api.py", "type": "blob", "size": 138634}, {"path": "jax/_src/api_util.py", "type": "blob", "size": 30271}, {"path": "jax/_src/array.py", "type": "blob", "size": 51634}, {"path": "jax/_src/basearray.py", "type": "blob", "size": 6904}, {"path": "jax/_src/basearray.pyi", "type": "blob", "size": 14464}, {"path": "jax/_src/blocked_sampler.py", "type": "blob", "size": 5890}, {"path": "jax/_src/buffer_callback.py", "type": "blob", "size": 10533}, {"path": "jax/_src/cache_key.py", "type": "blob", "size": 12923}, {"path": "jax/_src/callback.py", "type": "blob", "size": 33613}, {"path": "jax/_src/checkify.py", "type": "blob", "size": 57996}, {"path": "jax/_src/cloud_tpu_init.py", "type": "blob", "size": 5141}, {"path": "jax/_src/clusters", "type": "tree", "size": null}, {"path": "jax/_src/clusters/__init__.py", "type": "blob", "size": 1283}, {"path": "jax/_src/clusters/cloud_tpu_cluster.py", "type": "blob", "size": 8488}, {"path": "jax/_src/clusters/cluster.py", "type": "blob", "size": 5858}, {"path": "jax/_src/clusters/k8s_cluster.py", "type": "blob", "size": 9279}, {"path": "jax/_src/clusters/mpi4py_cluster.py", "type": "blob", "size": 2608}, {"path": "jax/_src/clusters/ompi_cluster.py", "type": "blob", "size": 2317}, {"path": "jax/_src/clusters/slurm_cluster.py", "type": "blob", "size": 2321}, {"path": "jax/_src/compilation_cache.py", "type": "blob", "size": 12154}, {"path": "jax/_src/compilation_cache_interface.py", "type": "blob", "size": 865}, {"path": "jax/_src/compiler.py", "type": "blob", "size": 32679}, {"path": "jax/_src/compute_on.py", "type": "blob", "size": 1911}, {"path": "jax/_src/config.py", "type": "blob", "size": 84751}, {"path": "jax/_src/core.py", "type": "blob", "size": 148208}, {"path": "jax/_src/cudnn", "type": "tree", "size": null}, {"path": "jax/_src/cudnn/__init__.py", "type": "blob", "size": 631}, {"path": "jax/_src/cudnn/fused_attention_stablehlo.py", "type": "blob", "size": 81994}, {"path": "jax/_src/cudnn/fusion.py", "type": "blob", "size": 2866}, {"path": "jax/_src/cudnn/scaled_matmul_stablehlo.py", "type": "blob", "size": 25340}, {"path": "jax/_src/custom_api_util.py", "type": "blob", "size": 876}, {"path": "jax/_src/custom_batching.py", "type": "blob", "size": 15238}, {"path": "jax/_src/custom_dce.py", "type": "blob", "size": 17470}, {"path": "jax/_src/custom_derivatives.py", "type": "blob", "size": 81242}, {"path": "jax/_src/custom_partitioning.py", "type": "blob", "size": 28452}, {"path": "jax/_src/custom_partitioning_sharding_rule.py", "type": "blob", "size": 21211}, {"path": "jax/_src/custom_transpose.py", "type": "blob", "size": 9204}, {"path": "jax/_src/debugger", "type": "tree", "size": null}, {"path": "jax/_src/debugger/__init__.py", "type": "blob", "size": 899}, {"path": "jax/_src/debugger/cli_debugger.py", "type": "blob", "size": 4778}, {"path": "jax/_src/debugger/colab_debugger.py", "type": "blob", "size": 7854}, {"path": "jax/_src/debugger/colab_lib.py", "type": "blob", "size": 4298}, {"path": "jax/_src/debugger/core.py", "type": "blob", "size": 8682}, {"path": "jax/_src/debugger/web_debugger.py", "type": "blob", "size": 3433}, {"path": "jax/_src/debugging.py", "type": "blob", "size": 34004}, {"path": "jax/_src/deprecations.py", "type": "blob", "size": 4699}, {"path": "jax/_src/dispatch.py", "type": "blob", "size": 28485}, {"path": "jax/_src/distributed.py", "type": "blob", "size": 14926}, {"path": "jax/_src/dlpack.py", "type": "blob", "size": 10826}, {"path": "jax/_src/dtypes.py", "type": "blob", "size": 40755}, {"path": "jax/_src/earray.py", "type": "blob", "size": 4405}, {"path": "jax/_src/effects.py", "type": "blob", "size": 5041}, {"path": "jax/_src/environment_info.py", "type": "blob", "size": 2031}, {"path": "jax/_src/error_check.py", "type": "blob", "size": 12975}, {"path": "jax/_src/errors.py", "type": "blob", "size": 24710}, {"path": "jax/_src/export", "type": "tree", "size": null}, {"path": "jax/_src/export/__init__.py", "type": "blob", "size": 581}, {"path": "jax/_src/export/_export.py", "type": "blob", "size": 70680}, {"path": "jax/_src/export/serialization.fbs", "type": "blob", "size": 3185}, {"path": "jax/_src/export/serialization.py", "type": "blob", "size": 19466}, {"path": "jax/_src/export/serialization_generated.py", "type": "blob", "size": 27781}, {"path": "jax/_src/export/shape_poly.py", "type": "blob", "size": 86021}, {"path": "jax/_src/export/shape_poly_decision.py", "type": "blob", "size": 20476}, {"path": "jax/_src/extend", "type": "tree", "size": null}, {"path": "jax/_src/extend/__init__.py", "type": "blob", "size": 581}, {"path": "jax/_src/extend/random.py", "type": "blob", "size": 1269}, {"path": "jax/_src/ffi.py", "type": "blob", "size": 29274}, {"path": "jax/_src/flatten_util.py", "type": "blob", "size": 3483}, {"path": "jax/_src/frozen_dict.py", "type": "blob", "size": 1457}, {"path": "jax/_src/hardware_utils.py", "type": "blob", "size": 2349}, {"path": "jax/_src/hashable_array.py", "type": "blob", "size": 1036}, {"path": "jax/_src/hijax.py", "type": "blob", "size": 9868}, {"path": "jax/_src/image", "type": "tree", "size": null}, {"path": "jax/_src/image/__init__.py", "type": "blob", "size": 581}, {"path": "jax/_src/image/scale.py", "type": "blob", "size": 13880}, {"path": "jax/_src/internal_test_util", "type": "tree", "size": null}, {"path": "jax/_src/internal_test_util/__init__.py", "type": "blob", "size": 581}, {"path": "jax/_src/internal_test_util/deprecation_module.py", "type": "blob", "size": 851}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data", "type": "tree", "size": null}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/__init__.py", "type": "blob", "size": 682}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/annotate_data_placement.py", "type": "blob", "size": 27102}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/cpu_cholesky_lapack_potrf.py", "type": "blob", "size": 45965}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/cpu_eig_lapack_geev.py", "type": "blob", "size": 35800}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/cpu_eigh_lapack_syev.py", "type": "blob", "size": 58741}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/cpu_hessenberg_lapack_gehrd.py", "type": "blob", "size": 37276}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/cpu_lu_lapack_getrf.py", "type": "blob", "size": 80518}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/cpu_qr_lapack_geqrf.py", "type": "blob", "size": 28588}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/cpu_schur_lapack_gees.py", "type": "blob", "size": 25050}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/cpu_svd_lapack_gesdd.py", "type": "blob", "size": 44276}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/cpu_triangular_solve_blas_trsm.py", "type": "blob", "size": 19086}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/cpu_tridiagonal_lapack_sytrd_hetrd.py", "type": "blob", "size": 58875}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/cpu_tridiagonal_solve_lapack_gtsv.py", "type": "blob", "size": 31786}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/cuda_eigh_cusolver_syev.py", "type": "blob", "size": 43043}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/cuda_lu_cusolver_getrf.py", "type": "blob", "size": 28728}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/cuda_lu_pivots_to_permutation.py", "type": "blob", "size": 4159}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/cuda_qr_cusolver_geqrf.py", "type": "blob", "size": 31584}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/cuda_svd_cusolver_gesvd.py", "type": "blob", "size": 80048}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/cuda_threefry2x32.py", "type": "blob", "size": 14920}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/cuda_tridiagonal_cusolver_sytrd.py", "type": "blob", "size": 39049}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/cuda_tridiagonal_solve.py", "type": "blob", "size": 7756}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/pallas", "type": "tree", "size": null}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/pallas/__init__.py", "type": "blob", "size": 682}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/pallas/mosaic_gpu_add_one.py", "type": "blob", "size": 158445}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/pallas/mosaic_matmul.py", "type": "blob", "size": 31203}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/pallas/mosaic_semaphore_dma.py", "type": "blob", "size": 16908}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/pallas/triton_add_one.py", "type": "blob", "size": 10043}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/rocm_eigh_hipsolver_syev.py", "type": "blob", "size": 115014}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/shardy_sharding_ops_with_different_meshes.py", "type": "blob", "size": 22351}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/stablehlo_dynamic_approx_top_k.py", "type": "blob", "size": 13062}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/stablehlo_dynamic_rng_bit_generator.py", "type": "blob", "size": 11422}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/stablehlo_dynamic_top_k.py", "type": "blob", "size": 14167}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/tpu_ApproxTopK.py", "type": "blob", "size": 12559}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/tpu_Eigh.py", "type": "blob", "size": 10146}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/tpu_Lu.py", "type": "blob", "size": 6185}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/tpu_Qr.py", "type": "blob", "size": 10228}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/tpu_Sharding.py", "type": "blob", "size": 20023}, {"path": "jax/_src/internal_test_util/export_back_compat_test_data/tpu_stablehlo_dynamic_reduce_window.py", "type": "blob", "size": 22996}, {"path": "jax/_src/internal_test_util/export_back_compat_test_util.py", "type": "blob", "size": 15892}, {"path": "jax/_src/internal_test_util/lax_test_util.py", "type": "blob", "size": 13016}, {"path": "jax/_src/internal_test_util/lazy_loader_module", "type": "tree", "size": null}, {"path": "jax/_src/internal_test_util/lazy_loader_module/__init__.py", "type": "blob", "size": 705}, {"path": "jax/_src/internal_test_util/lazy_loader_module/lazy_test_submodule.py", "type": "blob", "size": 610}, {"path": "jax/_src/internal_test_util/test_harnesses.py", "type": "blob", "size": 126707}, {"path": "jax/_src/interpreters", "type": "tree", "size": null}, {"path": "jax/_src/interpreters/__init__.py", "type": "blob", "size": 690}, {"path": "jax/_src/interpreters/ad.py", "type": "blob", "size": 70200}, {"path": "jax/_src/interpreters/batching.py", "type": "blob", "size": 54902}, {"path": "jax/_src/interpreters/mlir.py", "type": "blob", "size": 139556}, {"path": "jax/_src/interpreters/partial_eval.py", "type": "blob", "size": 129863}, {"path": "jax/_src/interpreters/pxla.py", "type": "blob", "size": 142492}, {"path": "jax/_src/jaxpr_util.py", "type": "blob", "size": 10475}, {"path": "jax/_src/lax", "type": "tree", "size": null}, {"path": "jax/_src/lax/__init__.py", "type": "blob", "size": 8388}, {"path": "jax/_src/lax/ann.py", "type": "blob", "size": 17043}, {"path": "jax/_src/lax/control_flow", "type": "tree", "size": null}, {"path": "jax/_src/lax/control_flow/__init__.py", "type": "blob", "size": 2244}, {"path": "jax/_src/lax/control_flow/common.py", "type": "blob", "size": 7553}, {"path": "jax/_src/lax/control_flow/conditionals.py", "type": "blob", "size": 56318}, {"path": "jax/_src/lax/control_flow/loops.py", "type": "blob", "size": 142155}, {"path": "jax/_src/lax/control_flow/solves.py", "type": "blob", "size": 20364}, {"path": "jax/_src/lax/convolution.py", "type": "blob", "size": 47516}, {"path": "jax/_src/lax/fft.py", "type": "blob", "size": 7277}, {"path": "jax/_src/lax/lax.py", "type": "blob", "size": 367543}, {"path": "jax/_src/lax/linalg.py", "type": "blob", "size": 101628}, {"path": "jax/_src/lax/other.py", "type": "blob", "size": 13874}, {"path": "jax/_src/lax/parallel.py", "type": "blob", "size": 98232}, {"path": "jax/_src/lax/slicing.py", "type": "blob", "size": 151422}, {"path": "jax/_src/lax/special.py", "type": "blob", "size": 28872}, {"path": "jax/_src/lax/utils.py", "type": "blob", "size": 10795}, {"path": "jax/_src/lax/windowed_reductions.py", "type": "blob", "size": 47324}, {"path": "jax/_src/lax_reference.py", "type": "blob", "size": 19642}, {"path": "jax/_src/layout.py", "type": "blob", "size": 5797}, {"path": "jax/_src/lazy_loader.py", "type": "blob", "size": 1842}, {"path": "jax/_src/lib", "type": "tree", "size": null}, {"path": "jax/_src/lib/BUILD", "type": "blob", "size": 1265}, {"path": "jax/_src/lib/__init__.py", "type": "blob", "size": 7672}, {"path": "jax/_src/lib/mlir", "type": "tree", "size": null}, {"path": "jax/_src/lib/mlir/__init__.py", "type": "blob", "size": 676}, {"path": "jax/_src/lib/mlir/dialects", "type": "tree", "size": null}, {"path": "jax/_src/lib/mlir/dialects/__init__.py", "type": "blob", "size": 2136}, {"path": "jax/_src/lib/mosaic_gpu.py", "type": "blob", "size": 1017}, {"path": "jax/_src/lib/triton.py", "type": "blob", "size": 2202}, {"path": "jax/_src/linear_util.py", "type": "blob", "size": 20361}, {"path": "jax/_src/literals.py", "type": "blob", "size": 6909}, {"path": "jax/_src/logging_config.py", "type": "blob", "size": 4195}, {"path": "jax/_src/lru_cache.py", "type": "blob", "size": 6741}, {"path": "jax/_src/memory.py", "type": "blob", "size": 745}, {"path": "jax/_src/mesh.py", "type": "blob", "size": 20919}, {"path": "jax/_src/mesh_utils.py", "type": "blob", "size": 33909}, {"path": "jax/_src/monitoring.py", "type": "blob", "size": 6241}, {"path": "jax/_src/named_sharding.py", "type": "blob", "size": 20945}, {"path": "jax/_src/nn", "type": "tree", "size": null}, {"path": "jax/_src/nn/__init__.py", "type": "blob", "size": 581}, {"path": "jax/_src/nn/functions.py", "type": "blob", "size": 48507}, {"path": "jax/_src/nn/initializers.py", "type": "blob", "size": 27013}, {"path": "jax/_src/numpy", "type": "tree", "size": null}, {"path": "jax/_src/numpy/__init__.py", "type": "blob", "size": 4072}, {"path": "jax/_src/numpy/array_api_metadata.py", "type": "blob", "size": 3617}, {"path": "jax/_src/numpy/array_constructors.py", "type": "blob", "size": 15688}, {"path": "jax/_src/numpy/array_creation.py", "type": "blob", "size": 31028}, {"path": "jax/_src/numpy/array_methods.py", "type": "blob", "size": 50521}, {"path": "jax/_src/numpy/einsum.py", "type": "blob", "size": 24373}, {"path": "jax/_src/numpy/error.py", "type": "blob", "size": 6437}, {"path": "jax/_src/numpy/fft.py", "type": "blob", "size": 52323}, {"path": "jax/_src/numpy/index_tricks.py", "type": "blob", "size": 10034}, {"path": "jax/_src/numpy/indexing.py", "type": "blob", "size": 52755}, {"path": "jax/_src/numpy/lax_numpy.py", "type": "blob", "size": 335774}, {"path": "jax/_src/numpy/linalg.py", "type": "blob", "size": 79562}, {"path": "jax/_src/numpy/polynomial.py", "type": "blob", "size": 28500}, {"path": "jax/_src/numpy/reductions.py", "type": "blob", "size": 109108}, {"path": "jax/_src/numpy/scalar_types.py", "type": "blob", "size": 3836}, {"path": "jax/_src/numpy/setops.py", "type": "blob", "size": 47972}, {"path": "jax/_src/numpy/sorting.py", "type": "blob", "size": 16625}, {"path": "jax/_src/numpy/tensor_contractions.py", "type": "blob", "size": 24633}, {"path": "jax/_src/numpy/ufunc_api.py", "type": "blob", "size": 24376}, {"path": "jax/_src/numpy/ufuncs.py", "type": "blob", "size": 118789}, {"path": "jax/_src/numpy/util.py", "type": "blob", "size": 17828}, {"path": "jax/_src/numpy/vectorize.py", "type": "blob", "size": 14077}, {"path": "jax/_src/numpy/window_functions.py", "type": "blob", "size": 5473}, {"path": "jax/_src/op_shardings.py", "type": "blob", "size": 4388}, {"path": "jax/_src/ops", "type": "tree", "size": null}, {"path": "jax/_src/ops/__init__.py", "type": "blob", "size": 581}, {"path": "jax/_src/ops/scatter.py", "type": "blob", "size": 18383}, {"path": "jax/_src/ops/special.py", "type": "blob", "size": 4380}, {"path": "jax/_src/pallas", "type": "tree", "size": null}, {"path": "jax/_src/pallas/BUILD", "type": "blob", "size": 1561}, {"path": "jax/_src/pallas/__init__.py", "type": "blob", "size": 581}, {"path": "jax/_src/pallas/core.py", "type": "blob", "size": 52527}, {"path": "jax/_src/pallas/cost_estimate.py", "type": "blob", "size": 8334}, {"path": "jax/_src/pallas/fuser", "type": "tree", "size": null}, {"path": "jax/_src/pallas/fuser/BUILD", "type": "blob", "size": 3827}, {"path": "jax/_src/pallas/fuser/__init__.py", "type": "blob", "size": 1265}, {"path": "jax/_src/pallas/fuser/block_spec.py", "type": "blob", "size": 71351}, {"path": "jax/_src/pallas/fuser/custom_evaluate.py", "type": "blob", "size": 2823}, {"path": "jax/_src/pallas/fuser/custom_fusion_lib.py", "type": "blob", "size": 8395}, {"path": "jax/_src/pallas/fuser/fuser_utils.py", "type": "blob", "size": 1270}, {"path": "jax/_src/pallas/fuser/fusible.py", "type": "blob", "size": 2689}, {"path": "jax/_src/pallas/fuser/fusible_dtype.py", "type": "blob", "size": 17990}, {"path": "jax/_src/pallas/fuser/fusion.py", "type": "blob", "size": 1515}, {"path": "jax/_src/pallas/fuser/jaxpr_fusion.py", "type": "blob", "size": 11176}, {"path": "jax/_src/pallas/helpers.py", "type": "blob", "size": 4219}, {"path": "jax/_src/pallas/hlo_interpreter.py", "type": "blob", "size": 19157}, {"path": "jax/_src/pallas/mosaic", "type": "tree", "size": null}, {"path": "jax/_src/pallas/mosaic/BUILD", "type": "blob", "size": 5653}, {"path": "jax/_src/pallas/mosaic/__init__.py", "type": "blob", "size": 581}, {"path": "jax/_src/pallas/mosaic/core.py", "type": "blob", "size": 11742}, {"path": "jax/_src/pallas/mosaic/error_handling.py", "type": "blob", "size": 5542}, {"path": "jax/_src/pallas/mosaic/helpers.py", "type": "blob", "size": 2767}, {"path": "jax/_src/pallas/mosaic/interpret.py", "type": "blob", "size": 102954}, {"path": "jax/_src/pallas/mosaic/lowering.py", "type": "blob", "size": 146660}, {"path": "jax/_src/pallas/mosaic/pallas_call_registration.py", "type": "blob", "size": 14512}, {"path": "jax/_src/pallas/mosaic/pipeline.py", "type": "blob", "size": 82129}, {"path": "jax/_src/pallas/mosaic/primitives.py", "type": "blob", "size": 30930}, {"path": "jax/_src/pallas/mosaic/random.py", "type": "blob", "size": 7952}, {"path": "jax/_src/pallas/mosaic/sc_core.py", "type": "blob", "size": 10853}, {"path": "jax/_src/pallas/mosaic/sc_lowering.py", "type": "blob", "size": 23805}, {"path": "jax/_src/pallas/mosaic/sc_primitives.py", "type": "blob", "size": 32359}, {"path": "jax/_src/pallas/mosaic/verification.py", "type": "blob", "size": 24512}, {"path": "jax/_src/pallas/mosaic_gpu", "type": "tree", "size": null}, {"path": "jax/_src/pallas/mosaic_gpu/BUILD", "type": "blob", "size": 3950}, {"path": "jax/_src/pallas/mosaic_gpu/__init__.py", "type": "blob", "size": 581}, {"path": "jax/_src/pallas/mosaic_gpu/core.py", "type": "blob", "size": 48461}, {"path": "jax/_src/pallas/mosaic_gpu/helpers.py", "type": "blob", "size": 9459}, {"path": "jax/_src/pallas/mosaic_gpu/lowering.py", "type": "blob", "size": 128705}, {"path": "jax/_src/pallas/mosaic_gpu/pallas_call_registration.py", "type": "blob", "size": 5730}, {"path": "jax/_src/pallas/mosaic_gpu/pipeline.py", "type": "blob", "size": 39770}, {"path": "jax/_src/pallas/mosaic_gpu/primitives.py", "type": "blob", "size": 118552}, {"path": "jax/_src/pallas/pallas_call.py", "type": "blob", "size": 70271}, {"path": "jax/_src/pallas/primitives.py", "type": "blob", "size": 45233}, {"path": "jax/_src/pallas/triton", "type": "tree", "size": null}, {"path": "jax/_src/pallas/triton/BUILD", "type": "blob", "size": 2363}, {"path": "jax/_src/pallas/triton/__init__.py", "type": "blob", "size": 581}, {"path": "jax/_src/pallas/triton/core.py", "type": "blob", "size": 1222}, {"path": "jax/_src/pallas/triton/lowering.py", "type": "blob", "size": 97679}, {"path": "jax/_src/pallas/triton/pallas_call_registration.py", "type": "blob", "size": 6884}, {"path": "jax/_src/pallas/triton/primitives.py", "type": "blob", "size": 5816}, {"path": "jax/_src/pallas/utils.py", "type": "blob", "size": 14875}, {"path": "jax/_src/partition_spec.py", "type": "blob", "size": 6220}, {"path": "jax/_src/path.py", "type": "blob", "size": 2162}, {"path": "jax/_src/pickle_util.py", "type": "blob", "size": 2312}, {"path": "jax/_src/pjit.py", "type": "blob", "size": 141467}, {"path": "jax/_src/pretty_printer.py", "type": "blob", "size": 3419}, {"path": "jax/_src/prng.py", "type": "blob", "size": 46742}, {"path": "jax/_src/profiler.py", "type": "blob", "size": 17446}, {"path": "jax/_src/public_test_util.py", "type": "blob", "size": 13073}, {"path": "jax/_src/random.py", "type": "blob", "size": 115956}, {"path": "jax/_src/ref.py", "type": "blob", "size": 1209}, {"path": "jax/_src/scipy", "type": "tree", "size": null}, {"path": "jax/_src/scipy/__init__.py", "type": "blob", "size": 581}, {"path": "jax/_src/scipy/cluster", "type": "tree", "size": null}, {"path": "jax/_src/scipy/cluster/__init__.py", "type": "blob", "size": 581}, {"path": "jax/_src/scipy/cluster/vq.py", "type": "blob", "size": 3039}, {"path": "jax/_src/scipy/fft.py", "type": "blob", "size": 15857}, {"path": "jax/_src/scipy/integrate.py", "type": "blob", "size": 2374}, {"path": "jax/_src/scipy/linalg.py", "type": "blob", "size": 85090}, {"path": "jax/_src/scipy/ndimage.py", "type": "blob", "size": 6945}, {"path": "jax/_src/scipy/optimize", "type": "tree", "size": null}, {"path": "jax/_src/scipy/optimize/__init__.py", "type": "blob", "size": 581}, {"path": "jax/_src/scipy/optimize/_lbfgs.py", "type": "blob", "size": 7874}, {"path": "jax/_src/scipy/optimize/bfgs.py", "type": "blob", "size": 5733}, {"path": "jax/_src/scipy/optimize/line_search.py", "type": "blob", "size": 13184}, {"path": "jax/_src/scipy/optimize/minimize.py", "type": "blob", "size": 4954}, {"path": "jax/_src/scipy/signal.py", "type": "blob", "size": 47551}, {"path": "jax/_src/scipy/sparse", "type": "tree", "size": null}, {"path": "jax/_src/scipy/sparse/__init__.py", "type": "blob", "size": 581}, {"path": "jax/_src/scipy/sparse/linalg.py", "type": "blob", "size": 27246}, {"path": "jax/_src/scipy/spatial", "type": "tree", "size": null}, {"path": "jax/_src/scipy/spatial/__init__.py", "type": "blob", "size": 581}, {"path": "jax/_src/scipy/spatial/transform.py", "type": "blob", "size": 17634}, {"path": "jax/_src/scipy/special.py", "type": "blob", "size": 92834}, {"path": "jax/_src/scipy/stats", "type": "tree", "size": null}, {"path": "jax/_src/scipy/stats/__init__.py", "type": "blob", "size": 581}, {"path": "jax/_src/scipy/stats/_core.py", "type": "blob", "size": 10642}, {"path": "jax/_src/scipy/stats/bernoulli.py", "type": "blob", "size": 4578}, {"path": "jax/_src/scipy/stats/beta.py", "type": "blob", "size": 8026}, {"path": "jax/_src/scipy/stats/betabinom.py", "type": "blob", "size": 3480}, {"path": "jax/_src/scipy/stats/binom.py", "type": "blob", "size": 2859}, {"path": "jax/_src/scipy/stats/cauchy.py", "type": "blob", "size": 8901}, {"path": "jax/_src/scipy/stats/chi2.py", "type": "blob", "size": 8045}, {"path": "jax/_src/scipy/stats/dirichlet.py", "type": "blob", "size": 3237}, {"path": "jax/_src/scipy/stats/expon.py", "type": "blob", "size": 7845}, {"path": "jax/_src/scipy/stats/gamma.py", "type": "blob", "size": 7076}, {"path": "jax/_src/scipy/stats/gennorm.py", "type": "blob", "size": 2994}, {"path": "jax/_src/scipy/stats/geom.py", "type": "blob", "size": 2282}, {"path": "jax/_src/scipy/stats/gumbel_l.py", "type": "blob", "size": 8345}, {"path": "jax/_src/scipy/stats/gumbel_r.py", "type": "blob", "size": 8361}, {"path": "jax/_src/scipy/stats/kde.py", "type": "blob", "size": 10249}, {"path": "jax/_src/scipy/stats/laplace.py", "type": "blob", "size": 3274}, {"path": "jax/_src/scipy/stats/logistic.py", "type": "blob", "size": 6291}, {"path": "jax/_src/scipy/stats/multinomial.py", "type": "blob", "size": 2532}, {"path": "jax/_src/scipy/stats/multivariate_normal.py", "type": "blob", "size": 3569}, {"path": "jax/_src/scipy/stats/nbinom.py", "type": "blob", "size": 2637}, {"path": "jax/_src/scipy/stats/norm.py", "type": "blob", "size": 8421}, {"path": "jax/_src/scipy/stats/pareto.py", "type": "blob", "size": 2630}, {"path": "jax/_src/scipy/stats/poisson.py", "type": "blob", "size": 3387}, {"path": "jax/_src/scipy/stats/t.py", "type": "blob", "size": 3226}, {"path": "jax/_src/scipy/stats/truncnorm.py", "type": "blob", "size": 9270}, {"path": "jax/_src/scipy/stats/uniform.py", "type": "blob", "size": 4372}, {"path": "jax/_src/scipy/stats/vonmises.py", "type": "blob", "size": 2484}, {"path": "jax/_src/scipy/stats/wrapcauchy.py", "type": "blob", "size": 2401}, {"path": "jax/_src/shard_alike.py", "type": "blob", "size": 4001}, {"path": "jax/_src/shard_map.py", "type": "blob", "size": 90622}, {"path": "jax/_src/sharding.py", "type": "blob", "size": 8300}, {"path": "jax/_src/sharding_impls.py", "type": "blob", "size": 46893}, {"path": "jax/_src/sharding_specs.py", "type": "blob", "size": 8617}, {"path": "jax/_src/source_info_util.py", "type": "blob", "size": 10457}, {"path": "jax/_src/sourcemap.py", "type": "blob", "size": 6640}, {"path": "jax/_src/stages.py", "type": "blob", "size": 36115}, {"path": "jax/_src/state", "type": "tree", "size": null}, {"path": "jax/_src/state/__init__.py", "type": "blob", "size": 1038}, {"path": "jax/_src/state/discharge.py", "type": "blob", "size": 34927}, {"path": "jax/_src/state/indexing.py", "type": "blob", "size": 12849}, {"path": "jax/_src/state/primitives.py", "type": "blob", "size": 41314}, {"path": "jax/_src/state/types.py", "type": "blob", "size": 18648}, {"path": "jax/_src/state/utils.py", "type": "blob", "size": 4023}, {"path": "jax/_src/test_loader.py", "type": "blob", "size": 7260}, {"path": "jax/_src/test_multiprocess.py", "type": "blob", "size": 9384}, {"path": "jax/_src/test_util.py", "type": "blob", "size": 85234}, {"path": "jax/_src/test_warning_util.py", "type": "blob", "size": 4074}, {"path": "jax/_src/third_party", "type": "tree", "size": null}, {"path": "jax/_src/third_party/README.md", "type": "blob", "size": 293}, {"path": "jax/_src/third_party/__init__.py", "type": "blob", "size": 0}, {"path": "jax/_src/third_party/scipy", "type": "tree", "size": null}, {"path": "jax/_src/third_party/scipy/LICENSE.txt", "type": "blob", "size": 1536}, {"path": "jax/_src/third_party/scipy/__init__.py", "type": "blob", "size": 0}, {"path": "jax/_src/third_party/scipy/betaln.py", "type": "blob", "size": 2388}, {"path": "jax/_src/third_party/scipy/interpolate.py", "type": "blob", "size": 6360}, {"path": "jax/_src/third_party/scipy/linalg.py", "type": "blob", "size": 3878}, {"path": "jax/_src/third_party/scipy/signal_helper.py", "type": "blob", "size": 3569}, {"path": "jax/_src/third_party/scipy/special.py", "type": "blob", "size": 9118}, {"path": "jax/_src/tpu", "type": "tree", "size": null}, {"path": "jax/_src/tpu/__init__.py", "type": "blob", "size": 581}, {"path": "jax/_src/tpu/linalg", "type": "tree", "size": null}, {"path": "jax/_src/tpu/linalg/__init__.py", "type": "blob", "size": 779}, {"path": "jax/_src/tpu/linalg/eigh.py", "type": "blob", "size": 24665}, {"path": "jax/_src/tpu/linalg/qdwh.py", "type": "blob", "size": 9553}, {"path": "jax/_src/tpu/linalg/stack.py", "type": "blob", "size": 2570}, {"path": "jax/_src/tpu/linalg/svd.py", "type": "blob", "size": 10336}, {"path": "jax/_src/tpu_custom_call.py", "type": "blob", "size": 27600}, {"path": "jax/_src/traceback_util.py", "type": "blob", "size": 8135}, {"path": "jax/_src/tree.py", "type": "blob", "size": 15379}, {"path": "jax/_src/tree_util.py", "type": "blob", "size": 50011}, {"path": "jax/_src/typing.py", "type": "blob", "size": 3529}, {"path": "jax/_src/util.py", "type": "blob", "size": 27138}, {"path": "jax/_src/xla_bridge.py", "type": "blob", "size": 44525}, {"path": "jax/_src/xla_metadata.py", "type": "blob", "size": 3834}, {"path": "jax/_src/xla_metadata_lib.py", "type": "blob", "size": 1474}, {"path": "jax/ad_checkpoint.py", "type": "blob", "size": 932}, {"path": "jax/api_util.py", "type": "blob", "size": 979}, {"path": "jax/cloud_tpu_init.py", "type": "blob", "size": 651}, {"path": "jax/collect_profile.py", "type": "blob", "size": 4860}, {"path": "jax/core.py", "type": "blob", "size": 2945}, {"path": "jax/custom_batching.py", "type": "blob", "size": 691}, {"path": "jax/custom_derivatives.py", "type": "blob", "size": 1879}, {"path": "jax/custom_transpose.py", "type": "blob", "size": 664}, {"path": "jax/debug.py", "type": "blob", "size": 1083}, {"path": "jax/distributed.py", "type": "blob", "size": 710}, {"path": "jax/dlpack.py", "type": "blob", "size": 1177}, {"path": "jax/dtypes.py", "type": "blob", "size": 1188}, {"path": "jax/errors.py", "type": "blob", "size": 1595}, {"path": "jax/example_libraries", "type": "tree", "size": null}, {"path": "jax/example_libraries/BUILD", "type": "blob", "size": 1313}, {"path": "jax/example_libraries/README.md", "type": "blob", "size": 3707}, {"path": "jax/example_libraries/__init__.py", "type": "blob", "size": 581}, {"path": "jax/example_libraries/optimizers.py", "type": "blob", "size": 20426}, {"path": "jax/example_libraries/stax.py", "type": "blob", "size": 13834}, {"path": "jax/experimental", "type": "tree", "size": null}, {"path": "jax/experimental/BUILD", "type": "blob", "size": 17841}, {"path": "jax/experimental/__init__.py", "type": "blob", "size": 2072}, {"path": "jax/experimental/_private_mm", "type": "tree", "size": null}, {"path": "jax/experimental/_private_mm/BUILD", "type": "blob", "size": 1060}, {"path": "jax/experimental/_private_mm/__init__.py", "type": "blob", "size": 775}, {"path": "jax/experimental/_private_mm/examples", "type": "tree", "size": null}, {"path": "jax/experimental/_private_mm/examples/example_basic.py", "type": "blob", "size": 2334}, {"path": "jax/experimental/_private_mm/examples/example_overlap.py", "type": "blob", "size": 6494}, {"path": "jax/experimental/_private_mm/examples/example_pp.py", "type": "blob", "size": 12140}, {"path": "jax/experimental/_private_mm/examples/example_tests.py", "type": "blob", "size": 3696}, {"path": "jax/experimental/_private_mm/examples/launch_utils.py", "type": "blob", "size": 3461}, {"path": "jax/experimental/_private_mm/mini_dime.py", "type": "blob", "size": 9738}, {"path": "jax/experimental/_private_mm/mm.py", "type": "blob", "size": 11435}, {"path": "jax/experimental/_private_mm/profile_utils.py", "type": "blob", "size": 2169}, {"path": "jax/experimental/array_serialization", "type": "tree", "size": null}, {"path": "jax/experimental/array_serialization/BUILD", "type": "blob", "size": 2213}, {"path": "jax/experimental/array_serialization/__init__.py", "type": "blob", "size": 581}, {"path": "jax/experimental/array_serialization/pytree_serialization.py", "type": "blob", "size": 21623}, {"path": "jax/experimental/array_serialization/pytree_serialization_utils.py", "type": "blob", "size": 2798}, {"path": "jax/experimental/array_serialization/serialization.py", "type": "blob", "size": 13434}, {"path": "jax/experimental/array_serialization/serialization_test.py", "type": "blob", "size": 41130}, {"path": "jax/experimental/array_serialization/tensorstore_impl.py", "type": "blob", "size": 25126}, {"path": "jax/experimental/buffer_callback.py", "type": "blob", "size": 765}, {"path": "jax/experimental/checkify.py", "type": "blob", "size": 1213}, {"path": "jax/experimental/colocated_python", "type": "tree", "size": null}, {"path": "jax/experimental/colocated_python/__init__.py", "type": "blob", "size": 980}, {"path": "jax/experimental/colocated_python/api.py", "type": "blob", "size": 6440}, {"path": "jax/experimental/colocated_python/func.py", "type": "blob", "size": 17703}, {"path": "jax/experimental/colocated_python/func_backend.py", "type": "blob", "size": 1382}, {"path": "jax/experimental/colocated_python/obj.py", "type": "blob", "size": 5657}, {"path": "jax/experimental/colocated_python/obj_backend.py", "type": "blob", "size": 2310}, {"path": "jax/experimental/colocated_python/serialization.py", "type": "blob", "size": 8945}, {"path": "jax/experimental/compilation_cache", "type": "tree", "size": null}, {"path": "jax/experimental/compilation_cache/__init__.py", "type": "blob", "size": 581}, {"path": "jax/experimental/compilation_cache/compilation_cache.py", "type": "blob", "size": 1394}, {"path": "jax/experimental/compute_on.py", "type": "blob", "size": 644}, {"path": "jax/experimental/custom_dce.py", "type": "blob", "size": 682}, {"path": "jax/experimental/custom_partitioning.py", "type": "blob", "size": 1054}, {"path": "jax/experimental/fused.py", "type": "blob", "size": 6803}, {"path": "jax/experimental/jax2tf", "type": "tree", "size": null}, {"path": "jax/experimental/jax2tf/BUILD", "type": "blob", "size": 1328}, {"path": "jax/experimental/jax2tf/JAX2TF_getting_started.ipynb", "type": "blob", "size": 814}, {"path": "jax/experimental/jax2tf/README.md", "type": "blob", "size": 84798}, {"path": "jax/experimental/jax2tf/__init__.py", "type": "blob", "size": 944}, {"path": "jax/experimental/jax2tf/call_tf.py", "type": "blob", "size": 28746}, {"path": "jax/experimental/jax2tf/examples", "type": "tree", "size": null}, {"path": "jax/experimental/jax2tf/examples/README.md", "type": "blob", "size": 8727}, {"path": "jax/experimental/jax2tf/examples/__init__.py", "type": "blob", "size": 581}, {"path": "jax/experimental/jax2tf/examples/keras_reuse_main.py", "type": "blob", "size": 2936}, {"path": "jax/experimental/jax2tf/examples/keras_reuse_main_test.py", "type": "blob", "size": 1670}, {"path": "jax/experimental/jax2tf/examples/mnist_lib.py", "type": "blob", "size": 11512}, {"path": "jax/experimental/jax2tf/examples/requirements.txt", "type": "blob", "size": 40}, {"path": "jax/experimental/jax2tf/examples/saved_model_lib.py", "type": "blob", "size": 6883}, {"path": "jax/experimental/jax2tf/examples/saved_model_main.py", "type": "blob", "size": 8003}, {"path": "jax/experimental/jax2tf/examples/saved_model_main_test.py", "type": "blob", "size": 2405}, {"path": "jax/experimental/jax2tf/examples/serving", "type": "tree", "size": null}, {"path": "jax/experimental/jax2tf/examples/serving/README.md", "type": "blob", "size": 5909}, {"path": "jax/experimental/jax2tf/examples/serving/__init__.py", "type": "blob", "size": 581}, {"path": "jax/experimental/jax2tf/examples/serving/model_server_request.py", "type": "blob", "size": 4912}, {"path": "jax/experimental/jax2tf/examples/tf_js", "type": "tree", "size": null}, {"path": "jax/experimental/jax2tf/examples/tf_js/README.md", "type": "blob", "size": 525}, {"path": "jax/experimental/jax2tf/examples/tf_js/quickdraw", "type": "tree", "size": null}, {"path": "jax/experimental/jax2tf/examples/tf_js/quickdraw/README.md", "type": "blob", "size": 2609}, {"path": "jax/experimental/jax2tf/examples/tf_js/quickdraw/input_pipeline.py", "type": "blob", "size": 2778}, {"path": "jax/experimental/jax2tf/examples/tf_js/quickdraw/quickdraw.py", "type": "blob", "size": 4894}, {"path": "jax/experimental/jax2tf/examples/tf_js/quickdraw/third_party", "type": "tree", "size": null}, {"path": "jax/experimental/jax2tf/examples/tf_js/quickdraw/third_party/zaidalyafeai.github.io", "type": "tree", "size": null}, {"path": "jax/experimental/jax2tf/examples/tf_js/quickdraw/third_party/zaidalyafeai.github.io/LICENSE", "type": "blob", "size": 1069}, {"path": "jax/experimental/jax2tf/examples/tf_js/quickdraw/third_party/zaidalyafeai.github.io/class_names.txt", "type": "blob", "size": 760}, {"path": "jax/experimental/jax2tf/g3doc", "type": "tree", "size": null}, {"path": "jax/experimental/jax2tf/g3doc/BUILD", "type": "blob", "size": 980}, {"path": "jax/experimental/jax2tf/g3doc/convert_models_results.md", "type": "blob", "size": 48188}, {"path": "jax/experimental/jax2tf/g3doc/convert_models_results.md.template", "type": "blob", "size": 1948}, {"path": "jax/experimental/jax2tf/g3doc/jax_primitives_coverage.md", "type": "blob", "size": 9426}, {"path": "jax/experimental/jax2tf/g3doc/jax_primitives_coverage.md.template", "type": "blob", "size": 2604}, {"path": "jax/experimental/jax2tf/jax2tf.py", "type": "blob", "size": 38347}, {"path": "jax/experimental/jax2tf/tests", "type": "tree", "size": null}, {"path": "jax/experimental/jax2tf/tests/__init__.py", "type": "blob", "size": 581}, {"path": "jax/experimental/jax2tf/tests/back_compat_testdata", "type": "tree", "size": null}, {"path": "jax/experimental/jax2tf/tests/back_compat_testdata/BUILD", "type": "blob", "size": 956}, {"path": "jax/experimental/jax2tf/tests/back_compat_testdata/tf_call_tf_function.py", "type": "blob", "size": 50036}, {"path": "jax/experimental/jax2tf/tests/back_compat_tf_test.py", "type": "blob", "size": 5431}, {"path": "jax/experimental/jax2tf/tests/call_tf_test.py", "type": "blob", "size": 62107}, {"path": "jax/experimental/jax2tf/tests/control_flow_ops_test.py", "type": "blob", "size": 10041}, {"path": "jax/experimental/jax2tf/tests/converters.py", "type": "blob", "size": 1998}, {"path": "jax/experimental/jax2tf/tests/cross_compilation_check.py", "type": "blob", "size": 7400}, {"path": "jax/experimental/jax2tf/tests/flax_models", "type": "tree", "size": null}, {"path": "jax/experimental/jax2tf/tests/flax_models/BUILD", "type": "blob", "size": 1224}, {"path": "jax/experimental/jax2tf/tests/flax_models/actor_critic.py", "type": "blob", "size": 2285}, {"path": "jax/experimental/jax2tf/tests/flax_models/bilstm_classifier.py", "type": "blob", "size": 14451}, {"path": "jax/experimental/jax2tf/tests/flax_models/cnn.py", "type": "blob", "size": 1234}, {"path": "jax/experimental/jax2tf/tests/flax_models/gnn.py", "type": "blob", "size": 6823}, {"path": "jax/experimental/jax2tf/tests/flax_models/resnet.py", "type": "blob", "size": 4562}, {"path": "jax/experimental/jax2tf/tests/flax_models/seq2seq_lstm.py", "type": "blob", "size": 7110}, {"path": "jax/experimental/jax2tf/tests/flax_models/transformer_lm1b.py", "type": "blob", "size": 12578}, {"path": "jax/experimental/jax2tf/tests/flax_models/transformer_nlp_seq.py", "type": "blob", "size": 6776}, {"path": "jax/experimental/jax2tf/tests/flax_models/transformer_wmt.py", "type": "blob", "size": 19182}, {"path": "jax/experimental/jax2tf/tests/flax_models/vae.py", "type": "blob", "size": 1741}, {"path": "jax/experimental/jax2tf/tests/jax2tf_limitations.py", "type": "blob", "size": 7546}, {"path": "jax/experimental/jax2tf/tests/jax2tf_test.py", "type": "blob", "size": 62517}, {"path": "jax/experimental/jax2tf/tests/jax_primitives_coverage_test.py", "type": "blob", "size": 6645}, {"path": "jax/experimental/jax2tf/tests/model_harness.py", "type": "blob", "size": 13870}, {"path": "jax/experimental/jax2tf/tests/models_test_main.py", "type": "blob", "size": 8928}, {"path": "jax/experimental/jax2tf/tests/primitives_test.py", "type": "blob", "size": 7845}, {"path": "jax/experimental/jax2tf/tests/savedmodel_test.py", "type": "blob", "size": 13067}, {"path": "jax/experimental/jax2tf/tests/shape_poly_test.py", "type": "blob", "size": 47395}, {"path": "jax/experimental/jax2tf/tests/sharding_test.py", "type": "blob", "size": 23543}, {"path": "jax/experimental/jax2tf/tests/tf_test_util.py", "type": "blob", "size": 19525}, {"path": "jax/experimental/jet.py", "type": "blob", "size": 27645}, {"path": "jax/experimental/key_reuse", "type": "tree", "size": null}, {"path": "jax/experimental/key_reuse/__init__.py", "type": "blob", "size": 1688}, {"path": "jax/experimental/key_reuse/_core.py", "type": "blob", "size": 23241}, {"path": "jax/experimental/layout.py", "type": "blob", "size": 738}, {"path": "jax/experimental/mesh_utils.py", "type": "blob", "size": 923}, {"path": "jax/experimental/mosaic", "type": "tree", "size": null}, {"path": "jax/experimental/mosaic/__init__.py", "type": "blob", "size": 883}, {"path": "jax/experimental/mosaic/dialects.py", "type": "blob", "size": 769}, {"path": "jax/experimental/mosaic/gpu", "type": "tree", "size": null}, {"path": "jax/experimental/mosaic/gpu/__init__.py", "type": "blob", "size": 4062}, {"path": "jax/experimental/mosaic/gpu/core.py", "type": "blob", "size": 36974}, {"path": "jax/experimental/mosaic/gpu/dialect_lowering.py", "type": "blob", "size": 76201}, {"path": "jax/experimental/mosaic/gpu/equations.py", "type": "blob", "size": 26781}, {"path": "jax/experimental/mosaic/gpu/examples", "type": "tree", "size": null}, {"path": "jax/experimental/mosaic/gpu/examples/BUILD", "type": "blob", "size": 1665}, {"path": "jax/experimental/mosaic/gpu/examples/__init__.py", "type": "blob", "size": 682}, {"path": "jax/experimental/mosaic/gpu/examples/flash_attention.py", "type": "blob", "size": 22011}, {"path": "jax/experimental/mosaic/gpu/examples/matmul.py", "type": "blob", "size": 14973}, {"path": "jax/experimental/mosaic/gpu/examples/matmul_blackwell.py", "type": "blob", "size": 12481}, {"path": "jax/experimental/mosaic/gpu/fragmented_array.py", "type": "blob", "size": 138751}, {"path": "jax/experimental/mosaic/gpu/inference_utils.py", "type": "blob", "size": 10513}, {"path": "jax/experimental/mosaic/gpu/launch_context.py", "type": "blob", "size": 60133}, {"path": "jax/experimental/mosaic/gpu/layout_inference.py", "type": "blob", "size": 49367}, {"path": "jax/experimental/mosaic/gpu/layouts.py", "type": "blob", "size": 13033}, {"path": "jax/experimental/mosaic/gpu/mma.py", "type": "blob", "size": 7550}, {"path": "jax/experimental/mosaic/gpu/mma_utils.py", "type": "blob", "size": 9687}, {"path": "jax/experimental/mosaic/gpu/profiler.py", "type": "blob", "size": 13510}, {"path": "jax/experimental/mosaic/gpu/tcgen05.py", "type": "blob", "size": 58687}, {"path": "jax/experimental/mosaic/gpu/transform_inference.py", "type": "blob", "size": 19330}, {"path": "jax/experimental/mosaic/gpu/utils.py", "type": "blob", "size": 59430}, {"path": "jax/experimental/mosaic/gpu/wgmma.py", "type": "blob", "size": 17511}, {"path": "jax/experimental/multihost_utils.py", "type": "blob", "size": 22684}, {"path": "jax/experimental/ode.py", "type": "blob", "size": 10903}, {"path": "jax/experimental/pallas", "type": "tree", "size": null}, {"path": "jax/experimental/pallas/__init__.py", "type": "blob", "size": 6818}, {"path": "jax/experimental/pallas/fuser.py", "type": "blob", "size": 1294}, {"path": "jax/experimental/pallas/g3doc", "type": "tree", "size": null}, {"path": "jax/experimental/pallas/g3doc/debugging.md", "type": "blob", "size": 11171}, {"path": "jax/experimental/pallas/mosaic_gpu.py", "type": "blob", "size": 6353}, {"path": "jax/experimental/pallas/ops", "type": "tree", "size": null}, {"path": "jax/experimental/pallas/ops/__init__.py", "type": "blob", "size": 764}, {"path": "jax/experimental/pallas/ops/gpu", "type": "tree", "size": null}, {"path": "jax/experimental/pallas/ops/gpu/BUILD", "type": "blob", "size": 911}, {"path": "jax/experimental/pallas/ops/gpu/__init__.py", "type": "blob", "size": 581}, {"path": "jax/experimental/pallas/ops/gpu/attention.py", "type": "blob", "size": 22855}, {"path": "jax/experimental/pallas/ops/gpu/attention_mgpu.py", "type": "blob", "size": 36425}, {"path": "jax/experimental/pallas/ops/gpu/blackwell_matmul_mgpu.py", "type": "blob", "size": 11828}, {"path": "jax/experimental/pallas/ops/gpu/blackwell_ragged_dot_mgpu.py", "type": "blob", "size": 15803}, {"path": "jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py", "type": "blob", "size": 11114}, {"path": "jax/experimental/pallas/ops/gpu/decode_attention.py", "type": "blob", "size": 17020}, {"path": "jax/experimental/pallas/ops/gpu/hopper_matmul_mgpu.py", "type": "blob", "size": 11019}, {"path": "jax/experimental/pallas/ops/gpu/hopper_mixed_type_matmul_mgpu.py", "type": "blob", "size": 12575}, {"path": "jax/experimental/pallas/ops/gpu/layer_norm.py", "type": "blob", "size": 11271}, {"path": "jax/experimental/pallas/ops/gpu/paged_attention.py", "type": "blob", "size": 16191}, {"path": "jax/experimental/pallas/ops/gpu/ragged_dot_mgpu.py", "type": "blob", "size": 11294}, {"path": "jax/experimental/pallas/ops/gpu/rms_norm.py", "type": "blob", "size": 10091}, {"path": "jax/experimental/pallas/ops/gpu/softmax.py", "type": "blob", "size": 2766}, {"path": "jax/experimental/pallas/ops/tpu", "type": "tree", "size": null}, {"path": "jax/experimental/pallas/ops/tpu/__init__.py", "type": "blob", "size": 581}, {"path": "jax/experimental/pallas/ops/tpu/all_gather.py", "type": "blob", "size": 5573}, {"path": "jax/experimental/pallas/ops/tpu/example_kernel.py", "type": "blob", "size": 802}, {"path": "jax/experimental/pallas/ops/tpu/flash_attention.py", "type": "blob", "size": 49855}, {"path": "jax/experimental/pallas/ops/tpu/matmul.py", "type": "blob", "size": 2717}, {"path": "jax/experimental/pallas/ops/tpu/megablox", "type": "tree", "size": null}, {"path": "jax/experimental/pallas/ops/tpu/megablox/__init__.py", "type": "blob", "size": 650}, {"path": "jax/experimental/pallas/ops/tpu/megablox/common.py", "type": "blob", "size": 1969}, {"path": "jax/experimental/pallas/ops/tpu/megablox/gmm.py", "type": "blob", "size": 27243}, {"path": "jax/experimental/pallas/ops/tpu/megablox/ops.py", "type": "blob", "size": 2903}, {"path": "jax/experimental/pallas/ops/tpu/paged_attention", "type": "tree", "size": null}, {"path": "jax/experimental/pallas/ops/tpu/paged_attention/__init__.py", "type": "blob", "size": 700}, {"path": "jax/experimental/pallas/ops/tpu/paged_attention/paged_attention_kernel.py", "type": "blob", "size": 22026}, {"path": "jax/experimental/pallas/ops/tpu/paged_attention/quantization_utils.py", "type": "blob", "size": 2556}, {"path": "jax/experimental/pallas/ops/tpu/paged_attention/util.py", "type": "blob", "size": 3254}, {"path": "jax/experimental/pallas/ops/tpu/ragged_paged_attention", "type": "tree", "size": null}, {"path": "jax/experimental/pallas/ops/tpu/ragged_paged_attention/__init__.py", "type": "blob", "size": 1036}, {"path": "jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py", "type": "blob", "size": 31879}, {"path": "jax/experimental/pallas/ops/tpu/ragged_paged_attention/tuned_block_sizes.py", "type": "blob", "size": 97758}, {"path": "jax/experimental/pallas/ops/tpu/random", "type": "tree", "size": null}, {"path": "jax/experimental/pallas/ops/tpu/random/__init__.py", "type": "blob", "size": 682}, {"path": "jax/experimental/pallas/ops/tpu/random/philox.py", "type": "blob", "size": 7494}, {"path": "jax/experimental/pallas/ops/tpu/random/prng_utils.py", "type": "blob", "size": 1780}, {"path": "jax/experimental/pallas/ops/tpu/random/threefry.py", "type": "blob", "size": 4339}, {"path": "jax/experimental/pallas/ops/tpu/splash_attention", "type": "tree", "size": null}, {"path": "jax/experimental/pallas/ops/tpu/splash_attention/__init__.py", "type": "blob", "size": 2738}, {"path": "jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py", "type": "blob", "size": 78591}, {"path": "jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_mask.py", "type": "blob", "size": 16307}, {"path": "jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_mask_info.py", "type": "blob", "size": 40291}, {"path": "jax/experimental/pallas/tpu.py", "type": "blob", "size": 5381}, {"path": "jax/experimental/pallas/tpu_sc.py", "type": "blob", "size": 2251}, {"path": "jax/experimental/pallas/triton.py", "type": "blob", "size": 1568}, {"path": "jax/experimental/pjit.py", "type": "blob", "size": 729}, {"path": "jax/experimental/profiler.py", "type": "blob", "size": 1413}, {"path": "jax/experimental/rnn.py", "type": "blob", "size": 20096}, {"path": "jax/experimental/roofline", "type": "tree", "size": null}, {"path": "jax/experimental/roofline/__init__.py", "type": "blob", "size": 1260}, {"path": "jax/experimental/roofline/roofline.py", "type": "blob", "size": 11804}, {"path": "jax/experimental/roofline/rooflines.py", "type": "blob", "size": 28455}, {"path": "jax/experimental/scheduling_groups.py", "type": "blob", "size": 2590}, {"path": "jax/experimental/serialize_executable.py", "type": "blob", "size": 4632}, {"path": "jax/experimental/shard_alike.py", "type": "blob", "size": 646}, {"path": "jax/experimental/shard_map.py", "type": "blob", "size": 4427}, {"path": "jax/experimental/slab", "type": "tree", "size": null}, {"path": "jax/experimental/slab/djax.py", "type": "blob", "size": 5385}, {"path": "jax/experimental/slab/slab.py", "type": "blob", "size": 11798}, {"path": "jax/experimental/source_mapper", "type": "tree", "size": null}, {"path": "jax/experimental/source_mapper/__init__.py", "type": "blob", "size": 1698}, {"path": "jax/experimental/source_mapper/common.py", "type": "blob", "size": 2508}, {"path": "jax/experimental/source_mapper/generate_map.py", "type": "blob", "size": 2129}, {"path": "jax/experimental/source_mapper/hlo.py", "type": "blob", "size": 4124}, {"path": "jax/experimental/source_mapper/jaxpr.py", "type": "blob", "size": 2595}, {"path": "jax/experimental/source_mapper/mlir.py", "type": "blob", "size": 4686}, {"path": "jax/experimental/sparse", "type": "tree", "size": null}, {"path": "jax/experimental/sparse/__init__.py", "type": "blob", "size": 11186}, {"path": "jax/experimental/sparse/_base.py", "type": "blob", "size": 3239}, {"path": "jax/experimental/sparse/_lowerings.py", "type": "blob", "size": 13006}, {"path": "jax/experimental/sparse/ad.py", "type": "blob", "size": 7789}, {"path": "jax/experimental/sparse/api.py", "type": "blob", "size": 6596}, {"path": "jax/experimental/sparse/bcoo.py", "type": "blob", "size": 129501}, {"path": "jax/experimental/sparse/bcsr.py", "type": "blob", "size": 41196}, {"path": "jax/experimental/sparse/coo.py", "type": "blob", "size": 23801}, {"path": "jax/experimental/sparse/csr.py", "type": "blob", "size": 23933}, {"path": "jax/experimental/sparse/linalg.py", "type": "blob", "size": 23425}, {"path": "jax/experimental/sparse/nm.py", "type": "blob", "size": 8461}, {"path": "jax/experimental/sparse/random.py", "type": "blob", "size": 3883}, {"path": "jax/experimental/sparse/test_util.py", "type": "blob", "size": 9433}, {"path": "jax/experimental/sparse/transform.py", "type": "blob", "size": 37575}, {"path": "jax/experimental/sparse/util.py", "type": "blob", "size": 4337}, {"path": "jax/experimental/topologies.py", "type": "blob", "size": 2118}, {"path": "jax/experimental/transfer.py", "type": "blob", "size": 2535}, {"path": "jax/experimental/x64_context.py", "type": "blob", "size": 2394}, {"path": "jax/experimental/xla_metadata.py", "type": "blob", "size": 658}, {"path": "jax/export.py", "type": "blob", "size": 1537}, {"path": "jax/extend", "type": "tree", "size": null}, {"path": "jax/extend/BUILD", "type": "blob", "size": 2390}, {"path": "jax/extend/__init__.py", "type": "blob", "size": 1628}, {"path": "jax/extend/backend.py", "type": "blob", "size": 1311}, {"path": "jax/extend/core", "type": "tree", "size": null}, {"path": "jax/extend/core/__init__.py", "type": "blob", "size": 1036}, {"path": "jax/extend/core/primitives.py", "type": "blob", "size": 6215}, {"path": "jax/extend/ifrt_programs.py", "type": "blob", "size": 787}, {"path": "jax/extend/linear_util.py", "type": "blob", "size": 1601}, {"path": "jax/extend/mlir", "type": "tree", "size": null}, {"path": "jax/extend/mlir/BUILD", "type": "blob", "size": 1196}, {"path": "jax/extend/mlir/__init__.py", "type": "blob", "size": 889}, {"path": "jax/extend/mlir/dialects", "type": "tree", "size": null}, {"path": "jax/extend/mlir/dialects/BUILD", "type": "blob", "size": 2779}, {"path": "jax/extend/mlir/dialects/__init__.py", "type": "blob", "size": 581}, {"path": "jax/extend/mlir/dialects/arith.py", "type": "blob", "size": 643}, {"path": "jax/extend/mlir/dialects/builtin.py", "type": "blob", "size": 645}, {"path": "jax/extend/mlir/dialects/chlo.py", "type": "blob", "size": 642}, {"path": "jax/extend/mlir/dialects/func.py", "type": "blob", "size": 642}, {"path": "jax/extend/mlir/dialects/math.py", "type": "blob", "size": 642}, {"path": "jax/extend/mlir/dialects/memref.py", "type": "blob", "size": 644}, {"path": "jax/extend/mlir/dialects/scf.py", "type": "blob", "size": 641}, {"path": "jax/extend/mlir/dialects/sdy.py", "type": "blob", "size": 641}, {"path": "jax/extend/mlir/dialects/sparse_tensor.py", "type": "blob", "size": 651}, {"path": "jax/extend/mlir/dialects/stablehlo.py", "type": "blob", "size": 647}, {"path": "jax/extend/mlir/dialects/vector.py", "type": "blob", "size": 644}, {"path": "jax/extend/mlir/ir.py", "type": "blob", "size": 631}, {"path": "jax/extend/mlir/passmanager.py", "type": "blob", "size": 640}, {"path": "jax/extend/random.py", "type": "blob", "size": 1083}, {"path": "jax/extend/sharding.py", "type": "blob", "size": 1327}, {"path": "jax/extend/source_info_util.py", "type": "blob", "size": 1215}, {"path": "jax/ffi.py", "type": "blob", "size": 1116}, {"path": "jax/flatten_util.py", "type": "blob", "size": 645}, {"path": "jax/image", "type": "tree", "size": null}, {"path": "jax/image/__init__.py", "type": "blob", "size": 1027}, {"path": "jax/interpreters", "type": "tree", "size": null}, {"path": "jax/interpreters/__init__.py", "type": "blob", "size": 690}, {"path": "jax/interpreters/ad.py", "type": "blob", "size": 6484}, {"path": "jax/interpreters/batching.py", "type": "blob", "size": 9370}, {"path": "jax/interpreters/mlir.py", "type": "blob", "size": 3324}, {"path": "jax/interpreters/partial_eval.py", "type": "blob", "size": 11865}, {"path": "jax/interpreters/pxla.py", "type": "blob", "size": 1805}, {"path": "jax/interpreters/xla.py", "type": "blob", "size": 1591}, {"path": "jax/lax", "type": "tree", "size": null}, {"path": "jax/lax/__init__.py", "type": "blob", "size": 11890}, {"path": "jax/lax/linalg.py", "type": "blob", "size": 1631}, {"path": "jax/lib", "type": "tree", "size": null}, {"path": "jax/lib/__init__.py", "type": "blob", "size": 774}, {"path": "jax/lib/xla_bridge.py", "type": "blob", "size": 1354}, {"path": "jax/lib/xla_client.py", "type": "blob", "size": 1932}, {"path": "jax/lib/xla_extension.py", "type": "blob", "size": 1677}, {"path": "jax/memory.py", "type": "blob", "size": 625}, {"path": "jax/monitoring.py", "type": "blob", "size": 1415}, {"path": "jax/nn", "type": "tree", "size": null}, {"path": "jax/nn/__init__.py", "type": "blob", "size": 1782}, {"path": "jax/nn/__init__.pyi", "type": "blob", "size": 4466}, {"path": "jax/nn/initializers.py", "type": "blob", "size": 1469}, {"path": "jax/numpy", "type": "tree", "size": null}, {"path": "jax/numpy/__init__.py", "type": "blob", "size": 12241}, {"path": "jax/numpy/__init__.pyi", "type": "blob", "size": 44860}, {"path": "jax/numpy/fft.py", "type": "blob", "size": 1084}, {"path": "jax/numpy/linalg.py", "type": "blob", "size": 1419}, {"path": "jax/ops", "type": "tree", "size": null}, {"path": "jax/ops/__init__.py", "type": "blob", "size": 870}, {"path": "jax/profiler.py", "type": "blob", "size": 1276}, {"path": "jax/py.typed", "type": "blob", "size": 0}, {"path": "jax/random.py", "type": "blob", "size": 9433}, {"path": "jax/ref.py", "type": "blob", "size": 1461}, {"path": "jax/scipy", "type": "tree", "size": null}, {"path": "jax/scipy/__init__.py", "type": "blob", "size": 1474}, {"path": "jax/scipy/cluster", "type": "tree", "size": null}, {"path": "jax/scipy/cluster/__init__.py", "type": "blob", "size": 768}, {"path": "jax/scipy/cluster/vq.py", "type": "blob", "size": 776}, {"path": "jax/scipy/fft.py", "type": "blob", "size": 809}, {"path": "jax/scipy/integrate.py", "type": "blob", "size": 777}, {"path": "jax/scipy/interpolate", "type": "tree", "size": null}, {"path": "jax/scipy/interpolate/__init__.py", "type": "blob", "size": 760}, {"path": "jax/scipy/linalg.py", "type": "blob", "size": 1408}, {"path": "jax/scipy/ndimage.py", "type": "blob", "size": 788}, {"path": "jax/scipy/optimize", "type": "tree", "size": null}, {"path": "jax/scipy/optimize/__init__.py", "type": "blob", "size": 822}, {"path": "jax/scipy/signal.py", "type": "blob", "size": 975}, {"path": "jax/scipy/sparse", "type": "tree", "size": null}, {"path": "jax/scipy/sparse/__init__.py", "type": "blob", "size": 757}, {"path": "jax/scipy/sparse/linalg.py", "type": "blob", "size": 810}, {"path": "jax/scipy/spatial", "type": "tree", "size": null}, {"path": "jax/scipy/spatial/__init__.py", "type": "blob", "size": 581}, {"path": "jax/scipy/spatial/transform.py", "type": "blob", "size": 802}, {"path": "jax/scipy/special.py", "type": "blob", "size": 2605}, {"path": "jax/scipy/stats", "type": "tree", "size": null}, {"path": "jax/scipy/stats/__init__.py", "type": "blob", "size": 2093}, {"path": "jax/scipy/stats/bernoulli.py", "type": "blob", "size": 819}, {"path": "jax/scipy/stats/beta.py", "type": "blob", "size": 851}, {"path": "jax/scipy/stats/betabinom.py", "type": "blob", "size": 792}, {"path": "jax/scipy/stats/binom.py", "type": "blob", "size": 659}, {"path": "jax/scipy/stats/cauchy.py", "type": "blob", "size": 881}, {"path": "jax/scipy/stats/chi2.py", "type": "blob", "size": 851}, {"path": "jax/scipy/stats/dirichlet.py", "type": "blob", "size": 792}, {"path": "jax/scipy/stats/expon.py", "type": "blob", "size": 866}, {"path": "jax/scipy/stats/gamma.py", "type": "blob", "size": 852}, {"path": "jax/scipy/stats/gennorm.py", "type": "blob", "size": 804}, {"path": "jax/scipy/stats/geom.py", "type": "blob", "size": 787}, {"path": "jax/scipy/stats/gumbel_l.py", "type": "blob", "size": 868}, {"path": "jax/scipy/stats/gumbel_r.py", "type": "blob", "size": 868}, {"path": "jax/scipy/stats/laplace.py", "type": "blob", "size": 804}, {"path": "jax/scipy/stats/logistic.py", "type": "blob", "size": 845}, {"path": "jax/scipy/stats/multinomial.py", "type": "blob", "size": 794}, {"path": "jax/scipy/stats/multivariate_normal.py", "type": "blob", "size": 802}, {"path": "jax/scipy/stats/nbinom.py", "type": "blob", "size": 660}, {"path": "jax/scipy/stats/norm.py", "type": "blob", "size": 879}, {"path": "jax/scipy/stats/pareto.py", "type": "blob", "size": 789}, {"path": "jax/scipy/stats/poisson.py", "type": "blob", "size": 804}, {"path": "jax/scipy/stats/t.py", "type": "blob", "size": 784}, {"path": "jax/scipy/stats/truncnorm.py", "type": "blob", "size": 855}, {"path": "jax/scipy/stats/uniform.py", "type": "blob", "size": 818}, {"path": "jax/scipy/stats/vonmises.py", "type": "blob", "size": 791}, {"path": "jax/scipy/stats/wrapcauchy.py", "type": "blob", "size": 793}, {"path": "jax/sharding.py", "type": "blob", "size": 1377}, {"path": "jax/stages.py", "type": "blob", "size": 1286}, {"path": "jax/test_util.py", "type": "blob", "size": 835}, {"path": "jax/tools", "type": "tree", "size": null}, {"path": "jax/tools/BUILD", "type": "blob", "size": 1455}, {"path": "jax/tools/__init__.py", "type": "blob", "size": 581}, {"path": "jax/tools/build_defs.bzl", "type": "blob", "size": 5991}, {"path": "jax/tools/colab_tpu.py", "type": "blob", "size": 890}, {"path": "jax/tools/jax_to_ir.py", "type": "blob", "size": 8711}, {"path": "jax/tools/pgo_nsys_converter.py", "type": "blob", "size": 3411}, {"path": "jax/tools/toolchains", "type": "tree", "size": null}, {"path": "jax/tools/toolchains/BUILD", "type": "blob", "size": 974}, {"path": "jax/tree.py", "type": "blob", "size": 1152}, {"path": "jax/tree_util.py", "type": "blob", "size": 3189}, {"path": "jax/typing.py", "type": "blob", "size": 3319}, {"path": "jax/version.py", "type": "blob", "size": 6733}, {"path": "jax_plugins", "type": "tree", "size": null}, {"path": "jax_plugins/BUILD.bazel", "type": "blob", "size": 1097}, {"path": "jax_plugins/cuda", "type": "tree", "size": null}, {"path": "jax_plugins/cuda/BUILD.bazel", "type": "blob", "size": 1537}, {"path": "jax_plugins/cuda/__init__.py", "type": "blob", "size": 13195}, {"path": "jax_plugins/cuda/gpu_version_script.lds", "type": "blob", "size": 130}, {"path": "jax_plugins/cuda/plugin_pyproject.toml", "type": "blob", "size": 94}, {"path": "jax_plugins/cuda/plugin_setup.py", "type": "blob", "size": 4599}, {"path": "jax_plugins/cuda/pyproject.toml", "type": "blob", "size": 94}, {"path": "jax_plugins/cuda/setup.py", "type": "blob", "size": 2071}, {"path": "jax_plugins/rocm", "type": "tree", "size": null}, {"path": "jax_plugins/rocm/BUILD.bazel", "type": "blob", "size": 1488}, {"path": "jax_plugins/rocm/__init__.py", "type": "blob", "size": 3123}, {"path": "jax_plugins/rocm/gpu_version_script.lds", "type": "blob", "size": 83}, {"path": "jax_plugins/rocm/plugin_pyproject.toml", "type": "blob", "size": 94}, {"path": "jax_plugins/rocm/plugin_setup.py", "type": "blob", "size": 2519}, {"path": "jax_plugins/rocm/pyproject.toml", "type": "blob", "size": 94}, {"path": "jax_plugins/rocm/setup.py", "type": "blob", "size": 2255}, {"path": "jaxlib", "type": "tree", "size": null}, {"path": "jaxlib/BUILD", "type": "blob", "size": 42487}, {"path": "jaxlib/README.md", "type": "blob", "size": 402}, {"path": "jaxlib/_ifrt_proxy.pyi", "type": "blob", "size": 1080}, {"path": "jaxlib/_jax", "type": "tree", "size": null}, {"path": "jaxlib/_jax/__init__.pyi", "type": "blob", "size": 32376}, {"path": "jaxlib/_jax/config.pyi", "type": "blob", "size": 1255}, {"path": "jaxlib/_jax/ffi.pyi", "type": "blob", "size": 1438}, {"path": "jaxlib/_jax/guard_lib.pyi", "type": "blob", "size": 1391}, {"path": "jaxlib/_jax/ifrt_programs.pyi", "type": "blob", "size": 1435}, {"path": "jaxlib/_jax/jax_jit.pyi", "type": "blob", "size": 1913}, {"path": "jaxlib/_jax/mlir.pyi", "type": "blob", "size": 1477}, {"path": "jaxlib/_jax/pmap_lib.pyi", "type": "blob", "size": 2680}, {"path": "jaxlib/_jax/profiler.pyi", "type": "blob", "size": 1994}, {"path": "jaxlib/_jax/pytree.pyi", "type": "blob", "size": 4761}, {"path": "jaxlib/_jax/transfer_guard_lib.pyi", "type": "blob", "size": 1281}, {"path": "jaxlib/_pathways.pyi", "type": "blob", "size": 1223}, {"path": "jaxlib/_pretty_printer.cc", "type": "blob", "size": 24312}, {"path": "jaxlib/_pretty_printer.pyi", "type": "blob", "size": 2139}, {"path": "jaxlib/absl_status_casters.h", "type": "blob", "size": 7664}, {"path": "jaxlib/cached_py_object.h", "type": "blob", "size": 1950}, {"path": "jaxlib/callback.cc", "type": "blob", "size": 6295}, {"path": "jaxlib/callback.h", "type": "blob", "size": 2953}, {"path": "jaxlib/config.cc", "type": "blob", "size": 11889}, {"path": "jaxlib/config.h", "type": "blob", "size": 2531}, {"path": "jaxlib/config_test.py", "type": "blob", "size": 2189}, {"path": "jaxlib/cpu", "type": "tree", "size": null}, {"path": "jaxlib/cpu/BUILD", "type": "blob", "size": 3133}, {"path": "jaxlib/cpu/_lapack", "type": "tree", "size": null}, {"path": "jaxlib/cpu/_lapack/__init__.pyi", "type": "blob", "size": 672}, {"path": "jaxlib/cpu/_lapack/eig.pyi", "type": "blob", "size": 682}, {"path": "jaxlib/cpu/_lapack/schur.pyi", "type": "blob", "size": 767}, {"path": "jaxlib/cpu/_lapack/svd.pyi", "type": "blob", "size": 790}, {"path": "jaxlib/cpu/_sparse", "type": "tree", "size": null}, {"path": "jaxlib/cpu/_sparse/__init__.pyi", "type": "blob", "size": 615}, {"path": "jaxlib/cpu/cpu_kernels.cc", "type": "blob", "size": 3668}, {"path": "jaxlib/cpu/lapack.cc", "type": "blob", "size": 11850}, {"path": "jaxlib/cpu/lapack_kernels.cc", "type": "blob", "size": 73910}, {"path": "jaxlib/cpu/lapack_kernels.h", "type": "blob", "size": 27246}, {"path": "jaxlib/cpu/lapack_kernels_using_lapack.cc", "type": "blob", "size": 8706}, {"path": "jaxlib/cpu/sparse.cc", "type": "blob", "size": 1069}, {"path": "jaxlib/cpu/sparse_kernels.cc", "type": "blob", "size": 8754}, {"path": "jaxlib/cpu/sparse_kernels.h", "type": "blob", "size": 941}, {"path": "jaxlib/cpu_feature_guard.c", "type": "blob", "size": 5917}, {"path": "jaxlib/cpu_sparse.py", "type": "blob", "size": 849}, {"path": "jaxlib/cuda", "type": "tree", "size": null}, {"path": "jaxlib/cuda/BUILD", "type": "blob", "size": 18641}, {"path": "jaxlib/cuda/cuda_plugin_extension.cc", "type": "blob", "size": 2952}, {"path": "jaxlib/cuda/versions.cc", "type": "blob", "size": 2154}, {"path": "jaxlib/cuda/versions_helpers.cc", "type": "blob", "size": 4108}, {"path": "jaxlib/cuda/versions_helpers.h", "type": "blob", "size": 1120}, {"path": "jaxlib/custom_call_sharding.cc", "type": "blob", "size": 14583}, {"path": "jaxlib/custom_call_sharding.h", "type": "blob", "size": 938}, {"path": "jaxlib/dlpack.cc", "type": "blob", "size": 14857}, {"path": "jaxlib/dlpack.h", "type": "blob", "size": 1965}, {"path": "jaxlib/dlpack_support.cc", "type": "blob", "size": 6483}, {"path": "jaxlib/dlpack_support.h", "type": "blob", "size": 1042}, {"path": "jaxlib/ffi.cc", "type": "blob", "size": 13278}, {"path": "jaxlib/ffi.h", "type": "blob", "size": 4943}, {"path": "jaxlib/ffi_helpers.h", "type": "blob", "size": 8703}, {"path": "jaxlib/gpu", "type": "tree", "size": null}, {"path": "jaxlib/gpu/BUILD", "type": "blob", "size": 3512}, {"path": "jaxlib/gpu/blas_handle_pool.cc", "type": "blob", "size": 1454}, {"path": "jaxlib/gpu/blas_handle_pool.h", "type": "blob", "size": 1070}, {"path": "jaxlib/gpu/ffi_wrapper.h", "type": "blob", "size": 2888}, {"path": "jaxlib/gpu/gpu_kernel_helpers.cc", "type": "blob", "size": 10847}, {"path": "jaxlib/gpu/gpu_kernel_helpers.h", "type": "blob", "size": 2788}, {"path": "jaxlib/gpu/gpu_kernels.cc", "type": "blob", "size": 4020}, {"path": "jaxlib/gpu/gpu_plugin_extension.cc", "type": "blob", "size": 9957}, {"path": "jaxlib/gpu/gpu_plugin_extension.h", "type": "blob", "size": 898}, {"path": "jaxlib/gpu/handle_pool.h", "type": "blob", "size": 3369}, {"path": "jaxlib/gpu/hybrid.cc", "type": "blob", "size": 2860}, {"path": "jaxlib/gpu/hybrid_kernels.cc", "type": "blob", "size": 32984}, {"path": "jaxlib/gpu/hybrid_kernels.h", "type": "blob", "size": 1719}, {"path": "jaxlib/gpu/linalg.cc", "type": "blob", "size": 1251}, {"path": "jaxlib/gpu/linalg_kernels.cc", "type": "blob", "size": 5441}, {"path": "jaxlib/gpu/linalg_kernels.cu.cc", "type": "blob", "size": 5459}, {"path": "jaxlib/gpu/linalg_kernels.h", "type": "blob", "size": 1647}, {"path": "jaxlib/gpu/make_batch_pointers.cu.cc", "type": "blob", "size": 1791}, {"path": "jaxlib/gpu/make_batch_pointers.h", "type": "blob", "size": 1107}, {"path": "jaxlib/gpu/prng.cc", "type": "blob", "size": 1161}, {"path": "jaxlib/gpu/prng_kernels.cc", "type": "blob", "size": 2580}, {"path": "jaxlib/gpu/prng_kernels.cu.cc", "type": "blob", "size": 3892}, {"path": "jaxlib/gpu/prng_kernels.h", "type": "blob", "size": 1317}, {"path": "jaxlib/gpu/py_client_gpu.cc", "type": "blob", "size": 13119}, {"path": "jaxlib/gpu/py_client_gpu.h", "type": "blob", "size": 1183}, {"path": "jaxlib/gpu/rnn.cc", "type": "blob", "size": 2031}, {"path": "jaxlib/gpu/rnn_kernels.cc", "type": "blob", "size": 22557}, {"path": "jaxlib/gpu/rnn_kernels.h", "type": "blob", "size": 1661}, {"path": "jaxlib/gpu/solver.cc", "type": "blob", "size": 1830}, {"path": "jaxlib/gpu/solver_handle_pool.cc", "type": "blob", "size": 2183}, {"path": "jaxlib/gpu/solver_handle_pool.h", "type": "blob", "size": 1411}, {"path": "jaxlib/gpu/solver_interface.cc", "type": "blob", "size": 21675}, {"path": "jaxlib/gpu/solver_interface.h", "type": "blob", "size": 10555}, {"path": "jaxlib/gpu/solver_kernels_ffi.cc", "type": "blob", "size": 53340}, {"path": "jaxlib/gpu/solver_kernels_ffi.h", "type": "blob", "size": 1469}, {"path": "jaxlib/gpu/sparse.cc", "type": "blob", "size": 21916}, {"path": "jaxlib/gpu/sparse_kernels.cc", "type": "blob", "size": 25836}, {"path": "jaxlib/gpu/sparse_kernels.h", "type": "blob", "size": 2710}, {"path": "jaxlib/gpu/triton.cc", "type": "blob", "size": 7109}, {"path": "jaxlib/gpu/triton.proto", "type": "blob", "size": 1441}, {"path": "jaxlib/gpu/triton_kernels.cc", "type": "blob", "size": 27698}, {"path": "jaxlib/gpu/triton_kernels.h", "type": "blob", "size": 3873}, {"path": "jaxlib/gpu/triton_utils.cc", "type": "blob", "size": 2430}, {"path": "jaxlib/gpu/triton_utils.h", "type": "blob", "size": 1184}, {"path": "jaxlib/gpu/vendor.h", "type": "blob", "size": 35915}, {"path": "jaxlib/gpu_common_utils.py", "type": "blob", "size": 905}, {"path": "jaxlib/gpu_linalg.py", "type": "blob", "size": 1394}, {"path": "jaxlib/gpu_prng.py", "type": "blob", "size": 1181}, {"path": "jaxlib/gpu_rnn.py", "type": "blob", "size": 6410}, {"path": "jaxlib/gpu_solver.py", "type": "blob", "size": 2141}, {"path": "jaxlib/gpu_sparse.py", "type": "blob", "size": 1477}, {"path": "jaxlib/gpu_triton.py", "type": "blob", "size": 2079}, {"path": "jaxlib/guard_lib.cc", "type": "blob", "size": 7123}, {"path": "jaxlib/guard_lib.h", "type": "blob", "size": 3774}, {"path": "jaxlib/ifrt_proxy.cc", "type": "blob", "size": 5962}, {"path": "jaxlib/init.py", "type": "blob", "size": 629}, {"path": "jaxlib/jax.bzl", "type": "blob", "size": 25481}, {"path": "jaxlib/jax.cc", "type": "blob", "size": 42323}, {"path": "jaxlib/jax_common.json", "type": "blob", "size": 71}, {"path": "jaxlib/jax_jit.cc", "type": "blob", "size": 19432}, {"path": "jaxlib/jax_jit.h", "type": "blob", "size": 8906}, {"path": "jaxlib/jax_python_wheel.bzl", "type": "blob", "size": 1593}, {"path": "jaxlib/kernel_helpers.h", "type": "blob", "size": 1718}, {"path": "jaxlib/kernel_nanobind_helpers.h", "type": "blob", "size": 2676}, {"path": "jaxlib/lapack.py", "type": "blob", "size": 1820}, {"path": "jaxlib/libjax_common.lds", "type": "blob", "size": 54}, {"path": "jaxlib/libjax_common_darwin.lds", "type": "blob", "size": 18}, {"path": "jaxlib/mlir.cc", "type": "blob", "size": 10431}, {"path": "jaxlib/mlir.h", "type": "blob", "size": 876}, {"path": "jaxlib/mlir", "type": "tree", "size": null}, {"path": "jaxlib/mlir/BUILD.bazel", "type": "blob", "size": 7257}, {"path": "jaxlib/mlir/_mlir_libs", "type": "tree", "size": null}, {"path": "jaxlib/mlir/_mlir_libs/BUILD.bazel", "type": "blob", "size": 9967}, {"path": "jaxlib/mlir/_mlir_libs/_triton_ext.pyi", "type": "blob", "size": 1074}, {"path": "jaxlib/mlir/_mlir_libs/jax_mlir_ext.cc", "type": "blob", "size": 8989}, {"path": "jaxlib/mlir/_mlir_libs/mosaic_gpu_ext.cc", "type": "blob", "size": 5888}, {"path": "jaxlib/mlir/_mlir_libs/tpu_ext.cc", "type": "blob", "size": 36720}, {"path": "jaxlib/mlir/_mlir_libs/traceback_to_location.cc", "type": "blob", "size": 4152}, {"path": "jaxlib/mlir/_mlir_libs/traceback_to_location.h", "type": "blob", "size": 2682}, {"path": "jaxlib/mlir/_mlir_libs/triton_ext.cc", "type": "blob", "size": 2535}, {"path": "jaxlib/mosaic", "type": "tree", "size": null}, {"path": "jaxlib/mosaic/BUILD", "type": "blob", "size": 8608}, {"path": "jaxlib/mosaic/dialect", "type": "tree", "size": null}, {"path": "jaxlib/mosaic/dialect/gpu", "type": "tree", "size": null}, {"path": "jaxlib/mosaic/dialect/gpu/BUILD", "type": "blob", "size": 6713}, {"path": "jaxlib/mosaic/dialect/gpu/integrations", "type": "tree", "size": null}, {"path": "jaxlib/mosaic/dialect/gpu/integrations/c", "type": "tree", "size": null}, {"path": "jaxlib/mosaic/dialect/gpu/integrations/c/attributes.cc", "type": "blob", "size": 3945}, {"path": "jaxlib/mosaic/dialect/gpu/integrations/c/attributes.h", "type": "blob", "size": 2685}, {"path": "jaxlib/mosaic/dialect/gpu/integrations/c/gpu_dialect.cc", "type": "blob", "size": 942}, {"path": "jaxlib/mosaic/dialect/gpu/integrations/c/gpu_dialect.h", "type": "blob", "size": 1016}, {"path": "jaxlib/mosaic/dialect/gpu/mosaic_gpu.cc", "type": "blob", "size": 24820}, {"path": "jaxlib/mosaic/dialect/gpu/mosaic_gpu.h", "type": "blob", "size": 2877}, {"path": "jaxlib/mosaic/dialect/gpu/mosaic_gpu.td", "type": "blob", "size": 29198}, {"path": "jaxlib/mosaic/dialect/gpu/mosaic_gpu_test.cc", "type": "blob", "size": 7077}, {"path": "jaxlib/mosaic/dialect/tpu", "type": "tree", "size": null}, {"path": "jaxlib/mosaic/dialect/tpu/array_util.cc", "type": "blob", "size": 1968}, {"path": "jaxlib/mosaic/dialect/tpu/array_util.h", "type": "blob", "size": 4330}, {"path": "jaxlib/mosaic/dialect/tpu/array_util_test.cc", "type": "blob", "size": 2275}, {"path": "jaxlib/mosaic/dialect/tpu/integrations", "type": "tree", "size": null}, {"path": "jaxlib/mosaic/dialect/tpu/integrations/c", "type": "tree", "size": null}, {"path": "jaxlib/mosaic/dialect/tpu/integrations/c/tpu_dialect.cc", "type": "blob", "size": 17114}, {"path": "jaxlib/mosaic/dialect/tpu/integrations/c/tpu_dialect.h", "type": "blob", "size": 8968}, {"path": "jaxlib/mosaic/dialect/tpu/layout.cc", "type": "blob", "size": 25095}, {"path": "jaxlib/mosaic/dialect/tpu/layout.h", "type": "blob", "size": 25249}, {"path": "jaxlib/mosaic/dialect/tpu/tpu.td", "type": "blob", "size": 53713}, {"path": "jaxlib/mosaic/dialect/tpu/tpu_dialect.cc", "type": "blob", "size": 9678}, {"path": "jaxlib/mosaic/dialect/tpu/tpu_dialect.h", "type": "blob", "size": 4764}, {"path": "jaxlib/mosaic/dialect/tpu/tpu_ops.cc", "type": "blob", "size": 81411}, {"path": "jaxlib/mosaic/dialect/tpu/tpu_ops_verification_test.cc", "type": "blob", "size": 46507}, {"path": "jaxlib/mosaic/dialect/tpu/transforms", "type": "tree", "size": null}, {"path": "jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc", "type": "blob", "size": 405821}, {"path": "jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.h", "type": "blob", "size": 3155}, {"path": "jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout_extensions.h", "type": "blob", "size": 1404}, {"path": "jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc", "type": "blob", "size": 76783}, {"path": "jaxlib/mosaic/dialect/tpu/transforms/communication.cc", "type": "blob", "size": 5203}, {"path": "jaxlib/mosaic/dialect/tpu/transforms/debug_assert_insertion.cc", "type": "blob", "size": 6389}, {"path": "jaxlib/mosaic/dialect/tpu/transforms/extensions", "type": "tree", "size": null}, {"path": "jaxlib/mosaic/dialect/tpu/transforms/extensions/apply_vector_layout_extensions.cc", "type": "blob", "size": 1186}, {"path": "jaxlib/mosaic/dialect/tpu/transforms/extensions/infer_vector_layout_extensions.cc", "type": "blob", "size": 1150}, {"path": "jaxlib/mosaic/dialect/tpu/transforms/infer_memref_layout.cc", "type": "blob", "size": 19528}, {"path": "jaxlib/mosaic/dialect/tpu/transforms/infer_memref_layout.h", "type": "blob", "size": 1556}, {"path": "jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc", "type": "blob", "size": 99618}, {"path": "jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout_extensions.h", "type": "blob", "size": 1287}, {"path": "jaxlib/mosaic/dialect/tpu/transforms/linalg_vectorization.cc", "type": "blob", "size": 23544}, {"path": "jaxlib/mosaic/dialect/tpu/transforms/memory_space_specialization.cc", "type": "blob", "size": 4095}, {"path": "jaxlib/mosaic/dialect/tpu/transforms/relayout_insertion.cc", "type": "blob", "size": 9493}, {"path": "jaxlib/mosaic/dialect/tpu/transforms/serde.cc", "type": "blob", "size": 11790}, {"path": "jaxlib/mosaic/dialect/tpu/transforms/serde.h", "type": "blob", "size": 2722}, {"path": "jaxlib/mosaic/dialect/tpu/util.cc", "type": "blob", "size": 13143}, {"path": "jaxlib/mosaic/dialect/tpu/util.h", "type": "blob", "size": 12144}, {"path": "jaxlib/mosaic/dialect/tpu/vreg_util.cc", "type": "blob", "size": 10687}, {"path": "jaxlib/mosaic/dialect/tpu/vreg_util.h", "type": "blob", "size": 4322}, {"path": "jaxlib/mosaic/dialect/tpu/vreg_util_test.cc", "type": "blob", "size": 9500}, {"path": "jaxlib/mosaic/gpu", "type": "tree", "size": null}, {"path": "jaxlib/mosaic/gpu/BUILD", "type": "blob", "size": 12089}, {"path": "jaxlib/mosaic/gpu/assembly_to_binary.cc", "type": "blob", "size": 5508}, {"path": "jaxlib/mosaic/gpu/assembly_to_binary.h", "type": "blob", "size": 1282}, {"path": "jaxlib/mosaic/gpu/custom_call.cc", "type": "blob", "size": 31423}, {"path": "jaxlib/mosaic/gpu/dump.cc", "type": "blob", "size": 13421}, {"path": "jaxlib/mosaic/gpu/dump.h", "type": "blob", "size": 2983}, {"path": "jaxlib/mosaic/gpu/dump_test.cc", "type": "blob", "size": 2220}, {"path": "jaxlib/mosaic/gpu/gpu_module_to_assembly.cc", "type": "blob", "size": 7984}, {"path": "jaxlib/mosaic/gpu/gpu_module_to_assembly.h", "type": "blob", "size": 1489}, {"path": "jaxlib/mosaic/gpu/gpu_module_to_assembly_test.cc", "type": "blob", "size": 9955}, {"path": "jaxlib/mosaic/gpu/integrations", "type": "tree", "size": null}, {"path": "jaxlib/mosaic/gpu/integrations/c", "type": "tree", "size": null}, {"path": "jaxlib/mosaic/gpu/integrations/c/passes.cc", "type": "blob", "size": 829}, {"path": "jaxlib/mosaic/gpu/integrations/c/passes.h", "type": "blob", "size": 956}, {"path": "jaxlib/mosaic/gpu/launch_lowering.cc", "type": "blob", "size": 16093}, {"path": "jaxlib/mosaic/gpu/launch_lowering.h", "type": "blob", "size": 900}, {"path": "jaxlib/mosaic/gpu/library_paths.h", "type": "blob", "size": 943}, {"path": "jaxlib/mosaic/gpu/mosaic_gpu_ext.cc", "type": "blob", "size": 6622}, {"path": "jaxlib/mosaic/gpu/nvshmem.h", "type": "blob", "size": 2844}, {"path": "jaxlib/mosaic/gpu/passes.cc", "type": "blob", "size": 11134}, {"path": "jaxlib/mosaic/gpu/passes.h", "type": "blob", "size": 1238}, {"path": "jaxlib/mosaic/gpu/runtime.cc", "type": "blob", "size": 9706}, {"path": "jaxlib/mosaic/gpu/serde.cc", "type": "blob", "size": 6649}, {"path": "jaxlib/mosaic/gpu/serde.h", "type": "blob", "size": 2580}, {"path": "jaxlib/mosaic/gpu/target.cc", "type": "blob", "size": 3812}, {"path": "jaxlib/mosaic/gpu/target.h", "type": "blob", "size": 1052}, {"path": "jaxlib/mosaic/gpu/wheel", "type": "tree", "size": null}, {"path": "jaxlib/mosaic/gpu/wheel/BUILD.bazel", "type": "blob", "size": 944}, {"path": "jaxlib/mosaic/gpu/wheel/__init__.py", "type": "blob", "size": 704}, {"path": "jaxlib/mosaic/gpu/wheel/mosaic_symbols.lds", "type": "blob", "size": 112}, {"path": "jaxlib/mosaic/gpu/wheel/setup.py", "type": "blob", "size": 2067}, {"path": "jaxlib/mosaic/pass_boilerplate.h", "type": "blob", "size": 2333}, {"path": "jaxlib/mosaic/python", "type": "tree", "size": null}, {"path": "jaxlib/mosaic/python/BUILD", "type": "blob", "size": 2086}, {"path": "jaxlib/mosaic/python/layout_defs.py", "type": "blob", "size": 1574}, {"path": "jaxlib/mosaic/python/mosaic_gpu.py", "type": "blob", "size": 1534}, {"path": "jaxlib/mosaic/python/tpu.py", "type": "blob", "size": 1925}, {"path": "jaxlib/mosaic/python/tpu_python.td", "type": "blob", "size": 685}, {"path": "jaxlib/mosaic/serde.cc", "type": "blob", "size": 6149}, {"path": "jaxlib/mosaic/serde.h", "type": "blob", "size": 2125}, {"path": "jaxlib/nb_class_ptr.h", "type": "blob", "size": 2677}, {"path": "jaxlib/partition_spec.cc", "type": "blob", "size": 9138}, {"path": "jaxlib/partition_spec.h", "type": "blob", "size": 2061}, {"path": "jaxlib/pathways.cc", "type": "blob", "size": 14827}, {"path": "jaxlib/pjit.cc", "type": "blob", "size": 54788}, {"path": "jaxlib/pjit.h", "type": "blob", "size": 857}, {"path": "jaxlib/plugin_support.py", "type": "blob", "size": 3627}, {"path": "jaxlib/pmap_lib.cc", "type": "blob", "size": 43300}, {"path": "jaxlib/pmap_lib.h", "type": "blob", "size": 1161}, {"path": "jaxlib/pprof_profile_builder.cc", "type": "blob", "size": 3815}, {"path": "jaxlib/pprof_profile_builder.h", "type": "blob", "size": 2431}, {"path": "jaxlib/py_array.cc", "type": "blob", "size": 87747}, {"path": "jaxlib/py_array.h", "type": "blob", "size": 12407}, {"path": "jaxlib/py_client.cc", "type": "blob", "size": 39412}, {"path": "jaxlib/py_client.h", "type": "blob", "size": 10286}, {"path": "jaxlib/py_client_cpu.cc", "type": "blob", "size": 10081}, {"path": "jaxlib/py_client_cpu.h", "type": "blob", "size": 935}, {"path": "jaxlib/py_compile_only_client.cc", "type": "blob", "size": 4911}, {"path": "jaxlib/py_compile_only_client.h", "type": "blob", "size": 1807}, {"path": "jaxlib/py_device.cc", "type": "blob", "size": 11822}, {"path": "jaxlib/py_device.h", "type": "blob", "size": 2453}, {"path": "jaxlib/py_device_list.cc", "type": "blob", "size": 17000}, {"path": "jaxlib/py_device_list.h", "type": "blob", "size": 5115}, {"path": "jaxlib/py_executable.cc", "type": "blob", "size": 17730}, {"path": "jaxlib/py_executable.h", "type": "blob", "size": 9851}, {"path": "jaxlib/py_host_callback.cc", "type": "blob", "size": 10579}, {"path": "jaxlib/py_host_callback.h", "type": "blob", "size": 4288}, {"path": "jaxlib/py_host_callback.proto", "type": "blob", "size": 900}, {"path": "jaxlib/py_memory_space.cc", "type": "blob", "size": 3464}, {"path": "jaxlib/py_memory_space.h", "type": "blob", "size": 2012}, {"path": "jaxlib/py_program.cc", "type": "blob", "size": 12144}, {"path": "jaxlib/py_program.h", "type": "blob", "size": 858}, {"path": "jaxlib/py_socket_transfer.cc", "type": "blob", "size": 23617}, {"path": "jaxlib/py_socket_transfer.h", "type": "blob", "size": 909}, {"path": "jaxlib/py_user_context.cc", "type": "blob", "size": 3125}, {"path": "jaxlib/py_user_context.h", "type": "blob", "size": 3160}, {"path": "jaxlib/py_values.cc", "type": "blob", "size": 51618}, {"path": "jaxlib/py_values.h", "type": "blob", "size": 6125}, {"path": "jaxlib/pyinit_stub.c", "type": "blob", "size": 998}, {"path": "jaxlib/python_ref_manager.cc", "type": "blob", "size": 3627}, {"path": "jaxlib/python_ref_manager.h", "type": "blob", "size": 3957}, {"path": "jaxlib/pytree.cc", "type": "blob", "size": 66508}, {"path": "jaxlib/pytree.h", "type": "blob", "size": 14712}, {"path": "jaxlib/pytree.proto", "type": "blob", "size": 604}, {"path": "jaxlib/pytree_test.py", "type": "blob", "size": 2898}, {"path": "jaxlib/pywrap.bzl", "type": "blob", "size": 2743}, {"path": "jaxlib/rocm", "type": "tree", "size": null}, {"path": "jaxlib/rocm/BUILD", "type": "blob", "size": 15854}, {"path": "jaxlib/rocm/rocm_plugin_extension.cc", "type": "blob", "size": 3786}, {"path": "jaxlib/setup.py", "type": "blob", "size": 3625}, {"path": "jaxlib/sharded_device_array.h", "type": "blob", "size": 7904}, {"path": "jaxlib/sharding.cc", "type": "blob", "size": 14259}, {"path": "jaxlib/sharding.h", "type": "blob", "size": 8177}, {"path": "jaxlib/symlink_files.bzl", "type": "blob", "size": 6145}, {"path": "jaxlib/to_ifrt_sharding.cc", "type": "blob", "size": 5582}, {"path": "jaxlib/to_ifrt_sharding.h", "type": "blob", "size": 2217}, {"path": "jaxlib/tools", "type": "tree", "size": null}, {"path": "jaxlib/tools/BUILD.bazel", "type": "blob", "size": 16970}, {"path": "jaxlib/tools/LICENSE.txt", "type": "blob", "size": 229029}, {"path": "jaxlib/tools/build_gpu_kernels_wheel.py", "type": "blob", "size": 7361}, {"path": "jaxlib/tools/build_gpu_plugin_wheel.py", "type": "blob", "size": 6405}, {"path": "jaxlib/tools/build_mosaic_wheel.py", "type": "blob", "size": 4113}, {"path": "jaxlib/tools/build_utils.py", "type": "blob", "size": 5954}, {"path": "jaxlib/tools/build_wheel.py", "type": "blob", "size": 15451}, {"path": "jaxlib/tools/wheel_size_test.py", "type": "blob", "size": 1712}, {"path": "jaxlib/traceback.cc", "type": "blob", "size": 15134}, {"path": "jaxlib/traceback.h", "type": "blob", "size": 2446}, {"path": "jaxlib/triton", "type": "tree", "size": null}, {"path": "jaxlib/triton/BUILD", "type": "blob", "size": 3392}, {"path": "jaxlib/triton/__init__.py", "type": "blob", "size": 646}, {"path": "jaxlib/triton/dialect.py", "type": "blob", "size": 2970}, {"path": "jaxlib/triton/triton.td", "type": "blob", "size": 48}, {"path": "jaxlib/triton/triton_dialect_capi.cc", "type": "blob", "size": 2305}, {"path": "jaxlib/triton/triton_dialect_capi.h", "type": "blob", "size": 1461}, {"path": "jaxlib/util.cc", "type": "blob", "size": 2667}, {"path": "jaxlib/util.h", "type": "blob", "size": 1230}, {"path": "jaxlib/utils.cc", "type": "blob", "size": 14211}, {"path": "jaxlib/weakref_lru_cache.cc", "type": "blob", "size": 14047}, {"path": "jaxlib/weakref_lru_cache.pyi", "type": "blob", "size": 1324}, {"path": "jaxlib/weakref_lru_cache_test.py", "type": "blob", "size": 6316}, {"path": "jaxlib/xla_client.py", "type": "blob", "size": 18027}, {"path": "jaxlib/xla_compiler.cc", "type": "blob", "size": 67603}, {"path": "jaxlib/xla_compiler.h", "type": "blob", "size": 907}, {"path": "platform_mappings", "type": "blob", "size": 371}, {"path": "pyproject.toml", "type": "blob", "size": 4597}, {"path": "setup.py", "type": "blob", "size": 4963}, {"path": "test_shard_count.bzl", "type": "blob", "size": 1071}, {"path": "tests", "type": "tree", "size": null}, {"path": "tests/BUILD", "type": "blob", "size": 46253}, {"path": "tests/absl_cpp_logging_test.py", "type": "blob", "size": 1544}, {"path": "tests/ann_test.py", "type": "blob", "size": 7305}, {"path": "tests/aot_test.py", "type": "blob", "size": 9416}, {"path": "tests/api_test.py", "type": "blob", "size": 262310}, {"path": "tests/api_util_test.py", "type": "blob", "size": 3101}, {"path": "tests/array_api_skips.txt", "type": "blob", "size": 6515}, {"path": "tests/array_api_test.py", "type": "blob", "size": 7871}, {"path": "tests/array_extensibility_test.py", "type": "blob", "size": 20085}, {"path": "tests/array_interoperability_test.py", "type": "blob", "size": 13006}, {"path": "tests/array_test.py", "type": "blob", "size": 64667}, {"path": "tests/batching_test.py", "type": "blob", "size": 54131}, {"path": "tests/blocked_sampler_test.py", "type": "blob", "size": 6301}, {"path": "tests/buffer_callback_test.py", "type": "blob", "size": 5840}, {"path": "tests/cache_key_test.py", "type": "blob", "size": 15285}, {"path": "tests/checkify_test.py", "type": "blob", "size": 44154}, {"path": "tests/cholesky_update_test.py", "type": "blob", "size": 2446}, {"path": "tests/clear_backends_test.py", "type": "blob", "size": 1165}, {"path": "tests/colocated_python_test.py", "type": "blob", "size": 23610}, {"path": "tests/compilation_cache_test.py", "type": "blob", "size": 28845}, {"path": "tests/config_test.py", "type": "blob", "size": 3091}, {"path": "tests/core_test.py", "type": "blob", "size": 25667}, {"path": "tests/cudnn_fusion_test.py", "type": "blob", "size": 2584}, {"path": "tests/custom_api_test.py", "type": "blob", "size": 139075}, {"path": "tests/custom_linear_solve_test.py", "type": "blob", "size": 17420}, {"path": "tests/custom_partitioning_sharding_rule_test.py", "type": "blob", "size": 22413}, {"path": "tests/custom_partitioning_test.py", "type": "blob", "size": 15849}, {"path": "tests/custom_root_test.py", "type": "blob", "size": 8356}, {"path": "tests/debug_info_test.py", "type": "blob", "size": 84392}, {"path": "tests/debug_nans_test.py", "type": "blob", "size": 8989}, {"path": "tests/debugger_test.py", "type": "blob", "size": 11388}, {"path": "tests/debugging_primitives_test.py", "type": "blob", "size": 45819}, {"path": "tests/deprecation_test.py", "type": "blob", "size": 2886}, {"path": "tests/device_test.py", "type": "blob", "size": 1836}, {"path": "tests/distributed_initialize_test.py", "type": "blob", "size": 1432}, {"path": "tests/distributed_test.py", "type": "blob", "size": 2381}, {"path": "tests/dtypes_test.py", "type": "blob", "size": 51079}, {"path": "tests/dynamic_api_test.py", "type": "blob", "size": 62329}, {"path": "tests/error_check_test.py", "type": "blob", "size": 11591}, {"path": "tests/errors_test.py", "type": "blob", "size": 12711}, {"path": "tests/experimental_rnn_test.py", "type": "blob", "size": 9182}, {"path": "tests/export_back_compat_test.py", "type": "blob", "size": 46297}, {"path": "tests/export_harnesses_multi_platform_test.py", "type": "blob", "size": 6866}, {"path": "tests/export_test.py", "type": "blob", "size": 88609}, {"path": "tests/extend_test.py", "type": "blob", "size": 5093}, {"path": "tests/ffi_test.py", "type": "blob", "size": 15033}, {"path": "tests/fft_test.py", "type": "blob", "size": 17147}, {"path": "tests/filecheck", "type": "tree", "size": null}, {"path": "tests/filecheck/README.md", "type": "blob", "size": 391}, {"path": "tests/filecheck/array.filecheck.py", "type": "blob", "size": 3287}, {"path": "tests/filecheck/custom_call.filecheck.py", "type": "blob", "size": 3913}, {"path": "tests/filecheck/jax_filecheck_helpers.py", "type": "blob", "size": 1229}, {"path": "tests/filecheck/jax_mlir_ext.filecheck.py", "type": "blob", "size": 5741}, {"path": "tests/filecheck/math.filecheck.py", "type": "blob", "size": 12345}, {"path": "tests/filecheck/names.filecheck.py", "type": "blob", "size": 1220}, {"path": "tests/filecheck/shapes.filecheck.py", "type": "blob", "size": 3165}, {"path": "tests/filecheck/subcomputations.filecheck.py", "type": "blob", "size": 2720}, {"path": "tests/fused_attention_stablehlo_test.py", "type": "blob", "size": 42321}, {"path": "tests/fused_test.py", "type": "blob", "size": 2218}, {"path": "tests/garbage_collection_guard_test.py", "type": "blob", "size": 2241}, {"path": "tests/generated_fun_test.py", "type": "blob", "size": 8172}, {"path": "tests/gpu_memory_flags_test.py", "type": "blob", "size": 1757}, {"path": "tests/heap_profiler_test.py", "type": "blob", "size": 1243}, {"path": "tests/hijax_test.py", "type": "blob", "size": 26548}, {"path": "tests/image_test.py", "type": "blob", "size": 14806}, {"path": "tests/jax_jit_test.py", "type": "blob", "size": 10682}, {"path": "tests/jax_numpy_error_test.py", "type": "blob", "size": 9366}, {"path": "tests/jax_to_ir_test.py", "type": "blob", "size": 5615}, {"path": "tests/jaxpr_effects_test.py", "type": "blob", "size": 33848}, {"path": "tests/jaxpr_util_test.py", "type": "blob", "size": 3725}, {"path": "tests/jet_test.py", "type": "blob", "size": 17053}, {"path": "tests/key_reuse_test.py", "type": "blob", "size": 24489}, {"path": "tests/lax_autodiff_test.py", "type": "blob", "size": 49024}, {"path": "tests/lax_control_flow_test.py", "type": "blob", "size": 119195}, {"path": "tests/lax_metal_test.py", "type": "blob", "size": 228683}, {"path": "tests/lax_numpy_einsum_test.py", "type": "blob", "size": 12754}, {"path": "tests/lax_numpy_indexing_test.py", "type": "blob", "size": 70859}, {"path": "tests/lax_numpy_operators_test.py", "type": "blob", "size": 36008}, {"path": "tests/lax_numpy_reducers_test.py", "type": "blob", "size": 42290}, {"path": "tests/lax_numpy_test.py", "type": "blob", "size": 267055}, {"path": "tests/lax_numpy_ufuncs_test.py", "type": "blob", "size": 21698}, {"path": "tests/lax_numpy_vectorize_test.py", "type": "blob", "size": 9775}, {"path": "tests/lax_scipy_sparse_test.py", "type": "blob", "size": 16231}, {"path": "tests/lax_scipy_special_functions_test.py", "type": "blob", "size": 15295}, {"path": "tests/lax_scipy_spectral_dac_test.py", "type": "blob", "size": 2091}, {"path": "tests/lax_scipy_test.py", "type": "blob", "size": 26367}, {"path": "tests/lax_test.py", "type": "blob", "size": 214081}, {"path": "tests/lax_vmap_op_test.py", "type": "blob", "size": 2942}, {"path": "tests/lax_vmap_test.py", "type": "blob", "size": 33373}, {"path": "tests/layout_test.py", "type": "blob", "size": 27641}, {"path": "tests/lazy_loader_test.py", "type": "blob", "size": 1351}, {"path": "tests/linalg_sharding_test.py", "type": "blob", "size": 7539}, {"path": "tests/linalg_test.py", "type": "blob", "size": 90721}, {"path": "tests/lobpcg_test.py", "type": "blob", "size": 14994}, {"path": "tests/logging_test.py", "type": "blob", "size": 10239}, {"path": "tests/lru_cache_test.py", "type": "blob", "size": 4900}, {"path": "tests/magma_linalg_test.py", "type": "blob", "size": 5315}, {"path": "tests/memories_test.py", "type": "blob", "size": 67906}, {"path": "tests/mesh_utils_test.py", "type": "blob", "size": 32737}, {"path": "tests/metadata_test.py", "type": "blob", "size": 4169}, {"path": "tests/mock_gpu_test.py", "type": "blob", "size": 2314}, {"path": "tests/mock_gpu_topology_test.py", "type": "blob", "size": 2265}, {"path": "tests/monitoring_test.py", "type": "blob", "size": 7068}, {"path": "tests/mosaic", "type": "tree", "size": null}, {"path": "tests/mosaic/BUILD", "type": "blob", "size": 5408}, {"path": "tests/mosaic/flash_attention_test.py", "type": "blob", "size": 2786}, {"path": "tests/mosaic/gpu_dialect_test.py", "type": "blob", "size": 49461}, {"path": "tests/mosaic/gpu_equations_test.py", "type": "blob", "size": 16933}, {"path": "tests/mosaic/gpu_layout_inference_test.py", "type": "blob", "size": 44210}, {"path": "tests/mosaic/gpu_test.py", "type": "blob", "size": 198431}, {"path": "tests/mosaic/gpu_test_distributed.py", "type": "blob", "size": 6744}, {"path": "tests/mosaic/gpu_test_multidevice.py", "type": "blob", "size": 2594}, {"path": "tests/mosaic/gpu_torch_test.py", "type": "blob", "size": 3406}, {"path": "tests/mosaic/gpu_transform_inference_test.py", "type": "blob", "size": 24199}, {"path": "tests/mosaic/matmul_test.py", "type": "blob", "size": 6955}, {"path": "tests/mosaic/profiler_cupti_test.py", "type": "blob", "size": 3755}, {"path": "tests/mosaic_test.py", "type": "blob", "size": 1137}, {"path": "tests/multi_device_test.py", "type": "blob", "size": 12442}, {"path": "tests/multibackend_test.py", "type": "blob", "size": 7517}, {"path": "tests/multiprocess_gpu_test.py", "type": "blob", "size": 19296}, {"path": "tests/mutable_array_test.py", "type": "blob", "size": 32239}, {"path": "tests/name_stack_test.py", "type": "blob", "size": 22165}, {"path": "tests/nn_test.py", "type": "blob", "size": 32405}, {"path": "tests/notebooks", "type": "tree", "size": null}, {"path": "tests/notebooks/colab_cpu.ipynb", "type": "blob", "size": 6308}, {"path": "tests/notebooks/colab_gpu.ipynb", "type": "blob", "size": 6022}, {"path": "tests/ode_test.py", "type": "blob", "size": 8746}, {"path": "tests/optimizers_test.py", "type": "blob", "size": 10229}, {"path": "tests/package_structure_test.py", "type": "blob", "size": 3159}, {"path": "tests/pallas", "type": "tree", "size": null}, {"path": "tests/pallas/BUILD", "type": "blob", "size": 28395}, {"path": "tests/pallas/export_back_compat_pallas_test.py", "type": "blob", "size": 5521}, {"path": "tests/pallas/export_pallas_test.py", "type": "blob", "size": 3611}, {"path": "tests/pallas/fuser_block_spec_test.py", "type": "blob", "size": 47317}, {"path": "tests/pallas/fusion_test.py", "type": "blob", "size": 8147}, {"path": "tests/pallas/gpu_attention_test.py", "type": "blob", "size": 6269}, {"path": "tests/pallas/gpu_ops_test.py", "type": "blob", "size": 15095}, {"path": "tests/pallas/gpu_paged_attention_test.py", "type": "blob", "size": 7577}, {"path": "tests/pallas/gpu_pallas_distributed_test.py", "type": "blob", "size": 8311}, {"path": "tests/pallas/indexing_test.py", "type": "blob", "size": 26510}, {"path": "tests/pallas/mgpu_attention_test.py", "type": "blob", "size": 6236}, {"path": "tests/pallas/mgpu_collective_matmul_test.py", "type": "blob", "size": 5407}, {"path": "tests/pallas/mgpu_matmul_test.py", "type": "blob", "size": 8207}, {"path": "tests/pallas/mgpu_ragged_dot_test.py", "type": "blob", "size": 5684}, {"path": "tests/pallas/mosaic_gpu_test.py", "type": "blob", "size": 206684}, {"path": "tests/pallas/ops_test.py", "type": "blob", "size": 95374}, {"path": "tests/pallas/pallas_cost_estimate_test.py", "type": "blob", "size": 4153}, {"path": "tests/pallas/pallas_error_handling_test.py", "type": "blob", "size": 6031}, {"path": "tests/pallas/pallas_jumble_test.py", "type": "blob", "size": 10935}, {"path": "tests/pallas/pallas_shape_poly_test.py", "type": "blob", "size": 7295}, {"path": "tests/pallas/pallas_test.py", "type": "blob", "size": 92910}, {"path": "tests/pallas/pallas_vmap_test.py", "type": "blob", "size": 8524}, {"path": "tests/pallas/tpu_all_gather_test.py", "type": "blob", "size": 4469}, {"path": "tests/pallas/tpu_fusible_matmul_test.py", "type": "blob", "size": 34066}, {"path": "tests/pallas/tpu_gmm_test.py", "type": "blob", "size": 11024}, {"path": "tests/pallas/tpu_ops_test.py", "type": "blob", "size": 20310}, {"path": "tests/pallas/tpu_paged_attention_kernel_test.py", "type": "blob", "size": 11557}, {"path": "tests/pallas/tpu_pallas_async_test.py", "type": "blob", "size": 32979}, {"path": "tests/pallas/tpu_pallas_call_print_test.py", "type": "blob", "size": 4830}, {"path": "tests/pallas/tpu_pallas_distributed_test.py", "type": "blob", "size": 27370}, {"path": "tests/pallas/tpu_pallas_interpret_distributed_test.py", "type": "blob", "size": 38476}, {"path": "tests/pallas/tpu_pallas_interpret_test.py", "type": "blob", "size": 44582}, {"path": "tests/pallas/tpu_pallas_interpret_thread_map_test.py", "type": "blob", "size": 2098}, {"path": "tests/pallas/tpu_pallas_memory_space_test.py", "type": "blob", "size": 8240}, {"path": "tests/pallas/tpu_pallas_pipeline_test.py", "type": "blob", "size": 76014}, {"path": "tests/pallas/tpu_pallas_random_test.py", "type": "blob", "size": 14534}, {"path": "tests/pallas/tpu_pallas_sparsecore_debug_check_test.py", "type": "blob", "size": 5142}, {"path": "tests/pallas/tpu_pallas_state_test.py", "type": "blob", "size": 9089}, {"path": "tests/pallas/tpu_pallas_test.py", "type": "blob", "size": 126711}, {"path": "tests/pallas/tpu_ragged_paged_attention_test.py", "type": "blob", "size": 13127}, {"path": "tests/pallas/tpu_sparsecore_pallas_distributed_test.py", "type": "blob", "size": 4687}, {"path": "tests/pallas/tpu_sparsecore_pallas_test.py", "type": "blob", "size": 36101}, {"path": "tests/pallas/tpu_splash_attention_kernel_sharded_test.py", "type": "blob", "size": 6856}, {"path": "tests/pallas/tpu_splash_attention_kernel_test.py", "type": "blob", "size": 26638}, {"path": "tests/pallas/tpu_splash_attention_mask_test.py", "type": "blob", "size": 68212}, {"path": "tests/pallas/triton_pallas_test.py", "type": "blob", "size": 10676}, {"path": "tests/pgle_test.py", "type": "blob", "size": 19428}, {"path": "tests/pickle_test.py", "type": "blob", "size": 8417}, {"path": "tests/pjit_test.py", "type": "blob", "size": 351495}, {"path": "tests/pmap_test.py", "type": "blob", "size": 128970}, {"path": "tests/polynomial_test.py", "type": "blob", "size": 4364}, {"path": "tests/pretty_printer_test.py", "type": "blob", "size": 3883}, {"path": "tests/profiler_test.py", "type": "blob", "size": 15375}, {"path": "tests/python_callback_test.py", "type": "blob", "size": 43324}, {"path": "tests/pytorch_interoperability_test.py", "type": "blob", "size": 5632}, {"path": "tests/qdwh_test.py", "type": "blob", "size": 7389}, {"path": "tests/ragged_collective_test.py", "type": "blob", "size": 32750}, {"path": "tests/random_lax_test.py", "type": "blob", "size": 63755}, {"path": "tests/random_test.py", "type": "blob", "size": 61737}, {"path": "tests/roofline_test.py", "type": "blob", "size": 36615}, {"path": "tests/scaled_matmul_stablehlo_test.py", "type": "blob", "size": 31340}, {"path": "tests/scheduling_groups_test.py", "type": "blob", "size": 1689}, {"path": "tests/scipy_fft_test.py", "type": "blob", "size": 5378}, {"path": "tests/scipy_interpolate_test.py", "type": "blob", "size": 2349}, {"path": "tests/scipy_ndimage_test.py", "type": "blob", "size": 5440}, {"path": "tests/scipy_optimize_test.py", "type": "blob", "size": 7040}, {"path": "tests/scipy_signal_test.py", "type": "blob", "size": 16712}, {"path": "tests/scipy_spatial_test.py", "type": "blob", "size": 14623}, {"path": "tests/scipy_stats_test.py", "type": "blob", "size": 72920}, {"path": "tests/shape_poly_test.py", "type": "blob", "size": 169039}, {"path": "tests/shard_alike_test.py", "type": "blob", "size": 7944}, {"path": "tests/shard_map_test.py", "type": "blob", "size": 159509}, {"path": "tests/source_info_test.py", "type": "blob", "size": 1764}, {"path": "tests/source_mapper_test.py", "type": "blob", "size": 3845}, {"path": "tests/sourcemap_test.py", "type": "blob", "size": 2366}, {"path": "tests/sparse_bcoo_bcsr_test.py", "type": "blob", "size": 79056}, {"path": "tests/sparse_test.py", "type": "blob", "size": 48868}, {"path": "tests/sparsify_test.py", "type": "blob", "size": 23623}, {"path": "tests/stack_test.py", "type": "blob", "size": 1539}, {"path": "tests/state_test.py", "type": "blob", "size": 67100}, {"path": "tests/stax_test.py", "type": "blob", "size": 8198}, {"path": "tests/string_array_test.py", "type": "blob", "size": 6894}, {"path": "tests/svd_test.py", "type": "blob", "size": 10914}, {"path": "tests/testdata", "type": "tree", "size": null}, {"path": "tests/testdata/example_pjrt_plugin_config.json", "type": "blob", "size": 188}, {"path": "tests/third_party", "type": "tree", "size": null}, {"path": "tests/third_party/scipy", "type": "tree", "size": null}, {"path": "tests/third_party/scipy/LICENSE", "type": "blob", "size": 1536}, {"path": "tests/third_party/scipy/line_search_test.py", "type": "blob", "size": 4869}, {"path": "tests/traceback_test.py", "type": "blob", "size": 4920}, {"path": "tests/transfer_guard_test.py", "type": "blob", "size": 8642}, {"path": "tests/tree_util_test.py", "type": "blob", "size": 59320}, {"path": "tests/typing_test.py", "type": "blob", "size": 5959}, {"path": "tests/unary_ops_accuracy_test.py", "type": "blob", "size": 12107}, {"path": "tests/util_test.py", "type": "blob", "size": 13296}, {"path": "tests/version_test.py", "type": "blob", "size": 8844}, {"path": "tests/warnings_util_test.py", "type": "blob", "size": 3127}, {"path": "tests/x64_context_test.py", "type": "blob", "size": 8157}, {"path": "tests/xla_bridge_test.py", "type": "blob", "size": 13560}, {"path": "tests/xla_interpreter_test.py", "type": "blob", "size": 1160}, {"path": "tests/xla_metadata_test.py", "type": "blob", "size": 13001}, {"path": "third_party", "type": "tree", "size": null}, {"path": "third_party/BUILD.bazel", "type": "blob", "size": 617}, {"path": "third_party/flatbuffers", "type": "tree", "size": null}, {"path": "third_party/flatbuffers/BUILD.bazel", "type": "blob", "size": 666}, {"path": "third_party/flatbuffers/flatbuffers.patch", "type": "blob", "size": 1703}, {"path": "third_party/flatbuffers/workspace.bzl", "type": "blob", "size": 1096}, {"path": "third_party/repo.bzl", "type": "blob", "size": 6124}, {"path": "third_party/xla", "type": "tree", "size": null}, {"path": "third_party/xla/BUILD.bazel", "type": "blob", "size": 0}, {"path": "third_party/xla/revision.bzl", "type": "blob", "size": 1063}, {"path": "third_party/xla/workspace.bzl", "type": "blob", "size": 1830}], "contributors": {"hawkinsp": 3208, "mattjj": 2690, "Google-ML-Automation": 2541, "jakevdp": 2442, "yashk2810": 1769, "gnecula": 1191, "apaszke": 834, "froystig": 687, "superbobry": 612, "skye": 502, "sharadmv": 389, "bchetioui": 322, "dfm": 283, "anon:Jake VanderPlas": 258, "pschuh": 192, "tlongeri": 192, "justinjfu": 186, "shoyer": 152, "bythew3i": 137, "jekbradbury": 127, "chr1sj0nes": 123, "dougalm": 121, "nitins17": 112, "rajasekharporeddy": 111, "LenaMartens": 107, "dimitar-asenov": 100, "zhangqiaorjc": 98, "jblespiau": 87, "ayaka14732": 87, "allanrenucci": 85, "cperivol": 85, "anon:Jieying Luo": 84, "fehiepsi": 77, "danielsuo": 72, "levskaya": 67, "j-towns": 66, "tlu7": 66, "hyeontaek": 61, "marcvanzee": 56, "tomhennigan": 55, "dependabot[bot]": 55, "Cjkkkk": 55, "WindQAQ": 54, "carlosgmartin": 54, "emilyfertig": 53, "nouiz": 53, "vfdev-5": 49, "bartchr808": 48, "Rifur13": 45, "MichaelHudgins": 45, "Ruturaj4": 44, "jburnim": 43, "axch": 42, "8bitmp3": 41, "maxwillzq": 41, "junwhanahn": 37, "kanglant": 36, "lgeiger": 34, "majnemer": 34, "andportnoy": 33, "naummo": 32, "zacmustin": 32, "NeilGirdhar": 30, "mwhittaker": 29, "anon:Pawe\u0142 Paruzel": 28, "juliuskunze": 28, "anon:Rahul Batra": 27, "rdyro": 27, "alexbw": 26, "anon:Erich Elsen": 26, "jacobjinkelly": 26, "belitskiy": 25, "kaixih": 23, "IvyZX": 23, "wenscarl": 23, "minoring": 23, "pearu": 23, "olupton": 23, "Micky774": 23, "atondwal": 23, "yueshengys": 21, "tomnatan30": 20, "cky9301": 20, "ezhulenev": 20, "dpfau": 20, "bixia1": 20, "inailuig": 19, "anon:Eugene Burmako": 19, "rsanthanam-amd": 19, "oliverdutton": 19, "cloudhan": 18, "patrick-kidger": 18, "brianwa84": 18, "selamw1": 18, "romanngg": 17, "aslanides": 17, "ghpvnist": 17, "gspschmid": 17, "epiqueras": 17, "vam-google": 16}, "_source": {"fetched_at": 1758915549.0969427, "api_base": "https://api.github.com/repos/google/jax", "limits": {"max_files": 25, "max_total_bytes": 1000000}}}, "fetched_at": 1758915549.0969427}, "pallets/flask": {"payload": {"url": "https://github.com/pallets/flask", "repo_id": "pallets/flask", "repo_type": "code", "name": "flask", "full_name": "pallets/flask", "description": "The Python micro framework for building web applications.", "homepage": "https://flask.palletsprojects.com", "default_branch": "main", "topics": ["flask", "jinja", "pallets", "python", "web-framework", "werkzeug", "wsgi"], "language": "Python", "archived": false, "disabled": false, "fork": false, "created_at": "2010-04-06T11:11:59Z", "updated_at": "2025-09-26T16:19:01Z", "pushed_at": "2025-09-20T00:33:34Z", "stars": 70443, "forks": 16541, "open_issues": 9, "watchers": 2108, "license_spdx": "BSD-3-Clause", "readme_text": "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/pallets/flask/refs/heads/stable/docs/_static/flask-name.svg\" alt=\"\" height=\"150\"></div>\n\n# Flask\n\nFlask is a lightweight [WSGI] web application framework. It is designed\nto make getting started quick and easy, with the ability to scale up to\ncomplex applications. It began as a simple wrapper around [Werkzeug]\nand [Jinja], and has become one of the most popular Python web\napplication frameworks.\n\nFlask offers suggestions, but doesn't enforce any dependencies or\nproject layout. It is up to the developer to choose the tools and\nlibraries they want to use. There are many extensions provided by the\ncommunity that make adding new functionality easy.\n\n[WSGI]: https://wsgi.readthedocs.io/\n[Werkzeug]: https://werkzeug.palletsprojects.com/\n[Jinja]: https://jinja.palletsprojects.com/\n\n## A Simple Example\n\n```python\n# save this as app.py\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello():\n    return \"Hello, World!\"\n```\n\n```\n$ flask run\n  * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n```\n\n## Donate\n\nThe Pallets organization develops and supports Flask and the libraries\nit uses. In order to grow the community of contributors and users, and\nallow the maintainers to devote more time to the projects, [please\ndonate today].\n\n[please donate today]: https://palletsprojects.com/donate\n\n## Contributing\n\nSee our [detailed contributing documentation][contrib] for many ways to\ncontribute, including reporting issues, requesting features, asking or answering\nquestions, and making PRs.\n\n[contrib]: https://palletsprojects.com/contributing/\n", "doc_texts": {".github/ISSUE_TEMPLATE/bug-report.md": "---\nname: Bug report\nabout: Report a bug in Flask (not other projects which depend on Flask)\n---\n\n<!--\nThis issue tracker is a tool to address bugs in Flask itself. Please use\nGitHub Discussions or the Pallets Discord for questions about your own code.\n\nReplace this comment with a clear outline of what the bug is.\n-->\n\n<!--\nDescribe how to replicate the bug.\n\nInclude a minimal reproducible example that demonstrates the bug.\nInclude the full traceback if there was an exception.\n-->\n\n<!--\nDescribe the expected behavior that should have happened but didn't.\n-->\n\nEnvironment:\n\n- Python version:\n- Flask version:\n", ".github/ISSUE_TEMPLATE/feature-request.md": "---\nname: Feature request\nabout: Suggest a new feature for Flask\n---\n\n<!--\nReplace this comment with a description of what the feature should do.\nInclude details such as links to relevant specs or previous discussions.\n-->\n\n<!--\nReplace this comment with an example of the problem which this feature\nwould resolve. Is this problem solvable without changes to Flask, such\nas by subclassing or using an extension?\n-->\n", ".github/pull_request_template.md": "<!--\nBefore opening a PR, open a ticket describing the issue or feature the\nPR will address. An issue is not required for fixing typos in\ndocumentation, or other simple non-code changes.\n\nReplace this comment with a description of the change. Describe how it\naddresses the linked ticket.\n-->\n\n<!--\nLink to relevant issues or previous PRs, one per line. Use \"fixes\" to\nautomatically close an issue.\n\nfixes #<issue number>\n-->\n\n<!--\nEnsure each step in CONTRIBUTING.rst is complete, especially the following:\n\n- Add tests that demonstrate the correct behavior of the change. Tests\n  should fail without the change.\n- Add or update relevant docs, in the docs folder and in code.\n- Add an entry in CHANGES.rst summarizing the change and linking to the issue.\n- Add `.. versionchanged::` entries in any relevant code docs.\n-->\n", "CHANGES.rst": "Version 3.2.0\n-------------\n\nUnreleased\n\n-   Drop support for Python 3.9. :pr:`5730`\n-   Remove previously deprecated code: ``__version__``. :pr:`5648`\n-   ``RequestContext`` has merged with ``AppContext``. ``RequestContext`` is now\n    a deprecated alias. If an app context is already pushed, it is not reused\n    when dispatching a request. This greatly simplifies the internal code for tracking\n    the active context. :issue:`5639`\n-   ``template_filter``, ``template_test``, and ``template_global`` decorators\n    can be used without parentheses. :issue:`5729`\n\n\nVersion 3.1.2\n-------------\n\nReleased 2025-08-19\n\n-   ``stream_with_context`` does not fail inside async views. :issue:`5774`\n-   When using ``follow_redirects`` in the test client, the final state\n    of ``session`` is correct. :issue:`5786`\n-   Relax type hint for passing bytes IO to ``send_file``. :issue:`5776`\n\n\nVersion 3.1.1\n-------------\n\nReleased 2025-05-13\n\n-   Fix signing key selection order when key rotation is enabled via\n    ``SECRET_KEY_FALLBACKS``. :ghsa:`4grg-w6v8-c28g`\n-   Fix type hint for ``cli_runner.invoke``. :issue:`5645`\n-   ``flask --help`` loads the app and plugins first to make sure all commands\n    are shown. :issue:`5673`\n-   Mark sans-io base class as being able to handle views that return\n    ``AsyncIterable``. This is not accurate for Flask, but makes typing easier\n    for Quart. :pr:`5659`\n\n\nVersion 3.1.0\n-------------\n\nReleased 2024-11-13\n\n-   Drop support for Python 3.8. :pr:`5623`\n-   Update minimum dependency versions to latest feature releases.\n    Werkzeug >= 3.1, ItsDangerous >= 2.2, Blinker >= 1.9. :pr:`5624,5633`\n-   Provide a configuration option to control automatic option\n    responses. :pr:`5496`\n-   ``Flask.open_resource``/``open_instance_resource`` and\n    ``Blueprint.open_resource`` take an ``encoding`` parameter to use when\n    opening in text mode. It defaults to ``utf-8``. :issue:`5504`\n-   ``Request.max_content_length`` can be customized per-request instead of only\n    through the ``MAX_CONTENT_LENGTH`` config. Added\n    ``MAX_FORM_MEMORY_SIZE`` and ``MAX_FORM_PARTS`` config. Added documentation\n    about resource limits to the security page. :issue:`5625`\n-   Add support for the ``Partitioned`` cookie attribute (CHIPS), with the\n    ``SESSION_COOKIE_PARTITIONED`` config. :issue:`5472`\n-   ``-e path`` takes precedence over default ``.env`` and ``.flaskenv`` files.\n    ``load_dotenv`` loads default files in addition to a path unless\n    ``load_defaults=False`` is passed. :issue:`5628`\n-   Support key rotation with the ``SECRET_KEY_FALLBACKS`` config, a list of old\n    secret keys that can still be used for unsigning. Extensions will need to\n    add support. :issue:`5621`\n-   Fix how setting ``host_matching=True`` or ``subdomain_matching=False``\n    interacts with ``SERVER_NAME``. Setting ``SERVER_NAME`` no longer restricts\n    requests to only that domain. :issue:`5553`\n-   ``Request.trusted_hosts`` is checked during routing, and can be set through\n    the ``TRUSTED_HOSTS`` config. :issue:`5636`\n\n\nVersion 3.0.3\n-------------\n\nReleased 2024-04-07\n\n-   The default ``hashlib.sha1`` may not be available in FIPS builds. Don't\n    access it at import time so the developer has time to change the default.\n    :issue:`5448`\n-   Don't initialize the ``cli`` attribute in the sansio scaffold, but rather in\n    the ``Flask`` concrete class. :pr:`5270`\n\n\nVersion 3.0.2\n-------------\n\nReleased 2024-02-03\n\n-   Correct type for ``jinja_loader`` property. :issue:`5388`\n-   Fix error with ``--extra-files`` and ``--exclude-patterns`` CLI options.\n    :issue:`5391`\n\n\nVersion 3.0.1\n-------------\n\nReleased 2024-01-18\n\n-   Correct type for ``path`` argument to ``send_file``. :issue:`5336`\n-   Fix a typo in an error message for the ``flask run --key`` option. :pr:`5344`\n-   Session data is untagged without relying on the built-in ``json.loads``\n    ``object_hook``. This allows other JSON providers that don't implement that.\n    :issue:`5381`\n-   Address more type findings when using mypy strict mode. :pr:`5383`\n\n\nVersion 3.0.0\n-------------\n\nReleased 2023-09-30\n\n-   Remove previously deprecated code. :pr:`5223`\n-   Deprecate the ``__version__`` attribute. Use feature detection, or\n    ``importlib.metadata.version(\"flask\")``, instead. :issue:`5230`\n-   Restructure the code such that the Flask (app) and Blueprint\n    classes have Sans-IO bases. :pr:`5127`\n-   Allow self as an argument to url_for. :pr:`5264`\n-   Require Werkzeug >= 3.0.0.\n\n\nVersion 2.3.3\n-------------\n\nReleased 2023-08-21\n\n-   Python 3.12 compatibility.\n-   Require Werkzeug >= 2.3.7.\n-   Use ``flit_core`` instead of ``setuptools`` as build backend.\n-   Refactor how an app's root and instance paths are determined. :issue:`5160`\n\n\nVersion 2.3.2\n-------------\n\nReleased 2023-05-01\n\n-   Set ``Vary: Cookie`` header when the session is accessed, modified, or refreshed.\n-   Update Werkzeug requirement to >=2.3.3 to apply recent bug fixes.\n    :ghsa:`m2qf-hxjv-5gpq`\n\n\nVersion 2.3.1\n-------------\n\nReleased 2023-04-25\n\n-   Restore deprecated ``from flask import Markup``. :issue:`5084`\n\n\nVersion 2.3.0\n-------------\n\nReleased 2023-04-25\n\n-   Drop support for Python 3.7. :pr:`5072`\n-   Update minimum requirements to the latest versions: Werkzeug>=2.3.0, Jinja2>3.1.2,\n    itsdangerous>=2.1.2, click>=8.1.3.\n-   Remove previously deprecated code. :pr:`4995`\n\n    -   The ``push`` and ``pop`` methods of the deprecated ``_app_ctx_stack`` and\n        ``_request_ctx_stack`` objects are removed. ``top`` still exists to give\n        extensions more time to update, but it will be removed.\n    -   The ``FLASK_ENV`` environment variable, ``ENV`` config key, and ``app.env``\n        property are removed.\n    -   The ``session_cookie_name``, ``send_file_max_age_default``, ``use_x_sendfile``,\n        ``propagate_exceptions``, and ``templates_auto_reload`` properties on ``app``\n        are removed.\n    -   The ``JSON_AS_ASCII``, ``JSON_SORT_KEYS``, ``JSONIFY_MIMETYPE``, and\n        ``JSONIFY_PRETTYPRINT_REGULAR`` config keys are removed.\n    -   The ``app.before_first_request`` and ``bp.before_app_first_request`` decorators\n        are removed.\n    -   ``json_encoder`` and ``json_decoder`` attributes on app and blueprint, and the\n        corresponding ``json.JSONEncoder`` and ``JSONDecoder`` classes, are removed.\n    -   The ``json.htmlsafe_dumps`` and ``htmlsafe_dump`` functions are removed.\n    -   Calling setup methods on blueprints after registration is an error instead of a\n        warning. :pr:`4997`\n\n-   Importing ``escape`` and ``Markup`` from ``flask`` is deprecated. Import them\n    directly from ``markupsafe`` instead. :pr:`4996`\n-   The ``app.got_first_request`` property is deprecated. :pr:`4997`\n-   The ``locked_cached_property`` decorator is deprecated. Use a lock inside the\n    decorated function if locking is needed. :issue:`4993`\n-   Signals are always available. ``blinker>=1.6.2`` is a required dependency. The\n    ``signals_available`` attribute is deprecated. :issue:`5056`\n-   Signals support ``async`` subscriber functions. :pr:`5049`\n-   Remove uses of locks that could cause requests to block each other very briefly.\n    :issue:`4993`\n-   Use modern packaging metadata with ``pyproject.toml`` instead of ``setup.cfg``.\n    :pr:`4947`\n-   Ensure subdomains are applied with nested blueprints. :issue:`4834`\n-   ``config.from_file`` can use ``text=False`` to indicate that the parser wants a\n    binary file instead. :issue:`4989`\n-   If a blueprint is created with an empty name it raises a ``ValueError``.\n    :issue:`5010`\n-   ``SESSION_COOKIE_DOMAIN`` does not fall back to ``SERVER_NAME``. The default is not\n    to set the domain, which modern browsers interpret as an exact match rather than\n    a subdomain match. Warnings about ``localhost`` and IP addresses are also removed.\n    :issue:`5051`\n-   The ``routes`` command shows each rule's ``subdomain`` or ``host`` when domain\n    matching is in use. :issue:`5004`\n-   Use postponed evaluation of annotations. :pr:`5071`\n\n\nVersion 2.2.5\n-------------\n\nReleased 2023-05-02\n\n-   Update for compatibility with Werkzeug 2.3.3.\n-   Set ``Vary: Cookie`` header when the session is accessed, modified, or refreshed.\n\n\nVersion 2.2.4\n-------------\n\nReleased 2023-04-25\n\n-   Update for compatibility with Werkzeug 2.3.\n\n\nVersion 2.2.3\n-------------\n\nReleased 2023-02-15\n\n-   Autoescape is enabled by default for ``.svg`` template files. :issue:`4831`\n-   Fix the type of ``template_folder`` to accept ``pathlib.Path``. :issue:`4892`\n-   Add ``--debug`` option to the ``flask run`` command. :issue:`4777`\n\n\nVersion 2.2.2\n-------------\n\nReleased 2022-08-08\n\n-   Update Werkzeug dependency to >= 2.2.2. This includes fixes related\n    to the new faster router, header parsing, and the development\n    server. :pr:`4754`\n-   Fix the default value for ``app.env`` to be ``\"production\"``. This\n    attribute remains deprecated. :issue:`4740`\n\n\nVersion 2.2.1\n-------------\n\nReleased 2022-08-03\n\n-   Setting or accessing ``json_encoder`` or ``json_decoder`` raises a\n    deprecation warning. :issue:`4732`\n\n\nVersion 2.2.0\n-------------\n\nReleased 2022-08-01\n\n-   Remove previously deprecated code. :pr:`4667`\n\n    -   Old names for some ``send_file`` parameters have been removed.\n        ``download_name`` replaces ``attachment_filename``, ``max_age``\n        replaces ``cache_timeout``, and ``etag`` replaces ``add_etags``.\n        Additionally, ``path`` replaces ``filename`` in\n        ``send_from_directory``.\n    -   The ``RequestContext.g`` property returning ``AppContext.g`` is\n        removed.\n\n-   Update Werkzeug dependency to >= 2.2.\n-   The app and request contexts are managed using Python context vars\n    directly rather than Werkzeug's ``LocalStack``. This should result\n    in better performance and memory use. :pr:`4682`\n\n    -   Extension maintainers, be aware that ``_app_ctx_stack.top``\n        and ``_request_ctx_stack.top`` are deprecated. Store data on\n        ``g`` instead using a unique prefix, like\n        ``g._extension_name_attr``.\n\n-   The ``FLASK_ENV`` environment variable and ``app.env`` attribute are\n    deprecated, removing the distinction between development and debug\n    mode. Debug mode should be controlled directly using the ``--debug``\n    option or ``app.run(debug=True)``. :issue:`4714`\n-   Some attributes that proxied config keys on ``app`` are deprecated:\n    ``session_cookie_name``, ``send_file_max_age_default``,\n    ``use_x_sendfile``, ``propagate_exceptions``, and\n    ``templates_auto_reload``. Use the relevant config keys instead.\n    :issue:`4716`\n-   Add new customization points to the ``Flask`` app object for many\n    previously global behaviors.\n\n    -   ``flask.url_for`` will call ``app.url_for``. :issue:`4568`\n    -   ``flask.abort`` will call ``app.aborter``.\n        ``Flask.aborter_class`` and ``Flask.make_aborter`` can be used\n        to customize this aborter. :issue:`4567`\n    -   ``flask.redirect`` will call ``app.redirect``. :issue:`4569`\n    -   ``flask.json`` is an instance of ``JSONProvider``. A different\n        provider can be set to use a different JSON library.\n        ``flask.jsonify`` will call ``app.json.response``, other\n        functions in ``flask.json`` will call corresponding functions in\n        ``app.json``. :pr:`4692`\n\n-   JSON configuration is moved to attributes on the default\n    ``app.json`` provider. ``JSON_AS_ASCII``, ``JSON_SORT_KEYS``,\n    ``JSONIFY_MIMETYPE``, and ``JSONIFY_PRETTYPRINT_REGULAR`` are\n    deprecated. :pr:`4692`\n-   Setting custom ``json_encoder`` and ``json_decoder`` classes on the\n    app or a blueprint, and the corresponding ``json.JSONEncoder`` and\n    ``JSONDecoder`` classes, are deprecated. JSON behavior can now be\n    overridden using the ``app.json`` provider interface. :pr:`4692`\n-   ``json.htmlsafe_dumps`` and ``json.htmlsafe_dump`` are deprecated,\n    the function is built-in to Jinja now. :pr:`4692`\n-   Refactor ``register_error_handler`` to consolidate error checking.\n    Rewrite some error messages to be more consistent. :issue:`4559`\n-   Use Blueprint decorators and functions intended for setup after\n    registering the blueprint will show a warning. In the next version,\n    this will become an error just like the application setup methods.\n    :issue:`4571`\n-   ``before_first_request`` is deprecated. Run setup code when creating\n    the application instead. :issue:`4605`\n-   Added the ``View.init_every_request`` class attribute. If a view\n    subclass sets this to ``False``, the view will not create a new\n    instance on every request. :issue:`2520`.\n-   A ``flask.cli.FlaskGroup`` Click group can be nested as a\n    sub-command in a custom CLI. :issue:`3263`\n-   Add ``--app`` and ``--debug`` options to the ``flask`` CLI, instead\n    of requiring that they are set through environment variables.\n    :issue:`2836`\n-   Add ``--env-file`` option to the ``flask`` CLI. This allows\n    specifying a dotenv file to load in addition to ``.env`` and\n    ``.flaskenv``. :issue:`3108`\n-   It is no longer required to decorate custom CLI commands on\n    ``app.cli`` or ``blueprint.cli`` with ``@with_appcontext``, an app\n    context will already be active at that point. :issue:`2410`\n-   ``SessionInterface.get_expiration_time`` uses a timezone-aware\n    value. :pr:`4645`\n-   View functions can return generators directly instead of wrapping\n    them in a ``Response``. :pr:`4629`\n-   Add ``stream_template`` and ``stream_template_string`` functions to\n    render a template as a stream of pieces. :pr:`4629`\n-   A new implementation of context preservation during debugging and\n    testing. :pr:`4666`\n\n    -   ``request``, ``g``, and other context-locals point to the\n        correct data when running code in the interactive debugger\n        console. :issue:`2836`\n    -   Teardown functions are always run at the end of the request,\n        even if the context is preserved. They are also run after the\n        preserved context is popped.\n    -   ``stream_with_context`` preserves context separately from a\n        ``with client`` block. It will be cleaned up when\n        ``response.get_data()`` or ``response.close()`` is called.\n\n-   Allow returning a list from a view function, to convert it to a\n    JSON response like a dict is. :issue:`4672`\n-   When type checking, allow ``TypedDict`` to be returned from view\n    functions. :pr:`4695`\n-   Remove the ``--eager-loading/--lazy-loading`` options from the\n    ``flask run`` command. The app is always eager loaded the first\n    time, then lazily loaded in the reloader. The reloader always prints\n    errors immediately but continues serving. Remove the internal\n    ``DispatchingApp`` middleware used by the previous implementation.\n    :issue:`4715`\n\n\nVersion 2.1.3\n-------------\n\nReleased 2022-07-13\n\n-   Inline some optional imports that are only used for certain CLI\n    commands. :pr:`4606`\n-   Relax type annotation for ``after_request`` functions. :issue:`4600`\n-   ``instance_path`` for namespace packages uses the path closest to\n    the imported submodule. :issue:`4610`\n-   Clearer error message when ``render_template`` and\n    ``render_template_string`` are used outside an application context.\n    :pr:`4693`\n\n\nVersion 2.1.2\n-------------\n\nReleased 2022-04-28\n\n-   Fix type annotation for ``json.loads``, it accepts str or bytes.\n    :issue:`4519`\n-   The ``--cert`` and ``--key`` options on ``flask run`` can be given\n    in either order. :issue:`4459`\n\n\nVersion 2.1.1\n-------------\n\nReleased on 2022-03-30\n\n-   Set the minimum required version of importlib_metadata to 3.6.0,\n    which is required on Python < 3.10. :issue:`4502`\n\n\nVersion 2.1.0\n-------------\n\nReleased 2022-03-28\n\n-   Drop support for Python 3.6. :pr:`4335`\n-   Update Click dependency to >= 8.0. :pr:`4008`\n-   Remove previously deprecated code. :pr:`4337`\n\n    -   The CLI does not pass ``script_info`` to app factory functions.\n    -   ``config.from_json`` is replaced by\n        ``config.from_file(name, load=json.load)``.\n    -   ``json`` functions no longer take an ``encoding`` parameter.\n    -   ``safe_join`` is removed, use ``werkzeug.utils.safe_join``\n        instead.\n    -   ``total_seconds`` is removed, use ``timedelta.total_seconds``\n        instead.\n    -   The same blueprint cannot be registered with the same name. Use\n        ``name=`` when registering to specify a unique name.\n    -   The test client's ``as_tuple`` parameter is removed. Use\n        ``response.request.environ`` instead. :pr:`4417`\n\n-   Some parameters in ``send_file`` and ``send_from_directory`` were\n    renamed in 2.0. The deprecation period for the old names is extended\n    to 2.2. Be sure to test with deprecation warnings visible.\n\n    -   ``attachment_filename`` is renamed to ``download_name``.\n    -   ``cache_timeout`` is renamed to ``max_age``.\n    -   ``add_etags`` is renamed to ``etag``.\n    -   ``filename`` is renamed to ``path``.\n\n-   The ``RequestContext.g`` property is deprecated. Use ``g`` directly\n    or ``AppContext.g`` instead. :issue:`3898`\n-   ``copy_current_request_context`` can decorate async functions.\n    :pr:`4303`\n-   The CLI uses ``importlib.metadata`` instead of ``pkg_resources`` to\n    load command entry points. :issue:`4419`\n-   Overriding ``FlaskClient.open`` will not cause an error on redirect.\n    :issue:`3396`\n-   Add an ``--exclude-patterns`` option to the ``flask run`` CLI\n    command to specify patterns that will be ignored by the reloader.\n    :issue:`4188`\n-   When using lazy loading (the default with the debugger), the Click\n    context from the ``flask run`` command remains available in the\n    loader thread. :issue:`4460`\n-   Deleting the session cookie uses the ``httponly`` flag.\n    :issue:`4485`\n-   Relax typing for ``errorhandler`` to allow the user to use more\n    precise types and decorate the same function multiple times.\n    :issue:`4095, 4295, 4297`\n-   Fix typing for ``__exit__`` methods for better compatibility with\n    ``ExitStack``. :issue:`4474`\n-   From Werkzeug, for redirect responses the ``Location`` header URL\n    will remain relative, and exclude the scheme and domain, by default.\n    :pr:`4496`\n-   Add ``Config.from_prefixed_env()`` to load config values from\n    environment variables that start with ``FLASK_`` or another prefix.\n    This parses values as JSON by default, and allows setting keys in\n    nested dicts. :pr:`4479`\n\n\nVersion 2.0.3\n-------------\n\nReleased 2022-02-14\n\n-   The test client's ``as_tuple`` parameter is deprecated and will be\n    removed in Werkzeug 2.1. It is now also deprecated in Flask, to be\n    removed in Flask 2.1, while remaining compatible with both in\n    2.0.x. Use ``response.request.environ`` instead. :pr:`4341`\n-   Fix type annotation for ``errorhandler`` decorator. :issue:`4295`\n-   Revert a change to the CLI that caused it to hide ``ImportError``\n    tracebacks when importing the application. :issue:`4307`\n-   ``app.json_encoder`` and ``json_decoder`` are only passed to\n    ``dumps`` and ``loads`` if they have custom behavior. This improves\n    performance, mainly on PyPy. :issue:`4349`\n-   Clearer error message when ``after_this_request`` is used outside a\n    request context. :issue:`4333`\n\n\nVersion 2.0.2\n-------------\n\nReleased 2021-10-04\n\n-   Fix type annotation for ``teardown_*`` methods. :issue:`4093`\n-   Fix type annotation for ``before_request`` and ``before_app_request``\n    decorators. :issue:`4104`\n-   Fixed the issue where typing requires template global\n    decorators to accept functions with no arguments. :issue:`4098`\n-   Support View and MethodView instances with async handlers. :issue:`4112`\n-   Enhance typing of ``app.errorhandler`` decorator. :issue:`4095`\n-   Fix registering a blueprint twice with differing names. :issue:`4124`\n-   Fix the type of ``static_folder`` to accept ``pathlib.Path``.\n    :issue:`4150`\n-   ``jsonify`` handles ``decimal.Decimal`` by encoding to ``str``.\n    :issue:`4157`\n-   Correctly handle raising deferred errors in CLI lazy loading.\n    :issue:`4096`\n-   The CLI loader handles ``**kwargs`` in a ``create_app`` function.\n    :issue:`4170`\n-   Fix the order of ``before_request`` and other callbacks that trigger\n    before the view returns. They are called from the app down to the\n    closest nested blueprint. :issue:`4229`\n\n\nVersion 2.0.1\n-------------\n\nReleased 2021-05-21\n\n-   Re-add the ``filename`` parameter in ``send_from_directory``. The\n    ``filename`` parameter has been renamed to ``path``, the old name\n    is deprecated. :pr:`4019`\n-   Mark top-level names as exported so type checking understands\n    imports in user projects. :issue:`4024`\n-   Fix type annotation for ``g`` and inform mypy that it is a namespace\n    object that has arbitrary attributes. :issue:`4020`\n-   Fix some types that weren't available in Python 3.6.0. :issue:`4040`\n-   Improve typing for ``send_file``, ``send_from_directory``, and\n    ``get_send_file_max_age``. :issue:`4044`, :pr:`4026`\n-   Show an error when a blueprint name contains a dot. The ``.`` has\n    special meaning, it is used to separate (nested) blueprint names and\n    the endpoint name. :issue:`4041`\n-   Combine URL prefixes when nesting blueprints that were created with\n    a ``url_prefix`` value. :issue:`4037`\n-   Revert a change to the order that URL matching was done. The\n    URL is again matched after the session is loaded, so the session is\n    available in custom URL converters. :issue:`4053`\n-   Re-add deprecated ``Config.from_json``, which was accidentally\n    removed early. :issue:`4078`\n-   Improve typing for some functions using ``Callable`` in their type\n    signatures, focusing on decorator factories. :issue:`4060`\n-   Nested blueprints are registered with their dotted name. This allows\n    different blueprints with the same name to be nested at different\n    locations. :issue:`4069`\n-   ``register_blueprint`` takes a ``name`` option to change the\n    (pre-dotted) name the blueprint is registered with. This allows the\n    same blueprint to be registered multiple times with unique names for\n    ``url_for``. Registering the same blueprint with the same name\n    multiple times is deprecated. :issue:`1091`\n-   Improve typing for ``stream_with_context``. :issue:`4052`\n\n\nVersion 2.0.0\n-------------\n\nReleased 2021-05-11\n\n-   Drop support for Python 2 and 3.5.\n-   Bump minimum versions of other Pallets projects: Werkzeug >= 2,\n    Jinja2 >= 3, MarkupSafe >= 2, ItsDangerous >= 2, Click >= 8. Be sure\n    to check the change logs for each project. For better compatibility\n    with other applications (e.g. Celery) that still require Click 7,\n    there is no hard dependency on Click 8 yet, but using Click 7 will\n    trigger a DeprecationWarning and Flask 2.1 will depend on Click 8.\n-   JSON support no longer uses simplejson. To use another JSON module,\n    override ``app.json_encoder`` and ``json_decoder``. :issue:`3555`\n-   The ``encoding`` option to JSON functions is deprecated. :pr:`3562`\n-   Passing ``script_info`` to app factory functions is deprecated. This\n    was not portable outside the ``flask`` command. Use\n    ``click.get_current_context().obj`` if it's needed. :issue:`3552`\n-   The CLI shows better error messages when the app failed to load\n    when looking up commands. :issue:`2741`\n-   Add ``SessionInterface.get_cookie_name`` to allow setting the\n    session cookie name dynamically. :pr:`3369`\n-   Add ``Config.from_file`` to load config using arbitrary file\n    loaders, such as ``toml.load`` or ``json.load``.\n    ``Config.from_json`` is deprecated in favor of this. :pr:`3398`\n-   The ``flask run`` command will only defer errors on reload. Errors\n    present during the initial call will cause the server to exit with\n    the traceback immediately. :issue:`3431`\n-   ``send_file`` raises a ``ValueError`` when passed an ``io`` object\n    in text mode. Previously, it would respond with 200 OK and an empty\n    file. :issue:`3358`\n-   When using ad-hoc certificates, check for the cryptography library\n    instead of PyOpenSSL. :pr:`3492`\n-   When specifying a factory function with ``FLASK_APP``, keyword\n    argument can be passed. :issue:`3553`\n-   When loading a ``.env`` or ``.flaskenv`` file, the current working\n    directory is no longer changed to the location of the file.\n    :pr:`3560`\n-   When returning a ``(response, headers)`` tuple from a view, the\n    headers replace rather than extend existing headers on the response.\n    For example, this allows setting the ``Content-Type`` for\n    ``jsonify()``. Use ``response.headers.extend()`` if extending is\n    desired. :issue:`3628`\n-   The ``Scaffold`` class provides a common API for the ``Flask`` and\n    ``Blueprint`` classes. ``Blueprint`` information is stored in\n    attributes just like ``Flask``, rather than opaque lambda functions.\n    This is intended to improve consistency and maintainability.\n    :issue:`3215`\n-   Include ``samesite`` and ``secure`` options when removing the\n    session cookie. :pr:`3726`\n-   Support passing a ``pathlib.Path`` to ``static_folder``. :pr:`3579`\n-   ``send_file`` and ``send_from_directory`` are wrappers around the\n    implementations in ``werkzeug.utils``. :pr:`3828`\n-   Some ``send_file`` parameters have been renamed, the old names are\n    deprecated. ``attachment_filename`` is renamed to ``download_name``.\n    ``cache_timeout`` is renamed to ``max_age``. ``add_etags`` is\n    renamed to ``etag``. :pr:`3828, 3883`\n-   ``send_file`` passes ``download_name`` even if\n    ``as_attachment=False`` by using ``Content-Disposition: inline``.\n    :pr:`3828`\n-   ``send_file`` sets ``conditional=True`` and ``max_age=None`` by\n    default. ``Cache-Control`` is set to ``no-cache`` if ``max_age`` is\n    not set, otherwise ``public``. This tells browsers to validate\n    conditional requests instead of using a timed cache. :pr:`3828`\n-   ``helpers.safe_join`` is deprecated. Use\n    ``werkzeug.utils.safe_join`` instead. :pr:`3828`\n-   The request context does route matching before opening the session.\n    This could allow a session interface to change behavior based on\n    ``request.endpoint``. :issue:`3776`\n-   Use Jinja's implementation of the ``|tojson`` filter. :issue:`3881`\n-   Add route decorators for common HTTP methods. For example,\n    ``@app.post(\"/login\")`` is a shortcut for\n    ``@app.route(\"/login\", methods=[\"POST\"])``. :pr:`3907`\n-   Support async views, error handlers, before and after request, and\n    teardown functions. :pr:`3412`\n-   Support nesting blueprints. :issue:`593, 1548`, :pr:`3923`\n-   Set the default encoding to \"UTF-8\" when loading ``.env`` and\n    ``.flaskenv`` files to allow to use non-ASCII characters. :issue:`3931`\n-   ``flask shell`` sets up tab and history completion like the default\n    ``python`` shell if ``readline`` is installed. :issue:`3941`\n-   ``helpers.total_seconds()`` is deprecated. Use\n    ``timedelta.total_seconds()`` instead. :pr:`3962`\n-   Add type hinting. :pr:`3973`.\n\n\nVersion 1.1.4\n-------------\n\nReleased 2021-05-13\n\n-   Update ``static_folder`` to use ``_compat.fspath`` instead of\n    ``os.fspath`` to continue supporting Python < 3.6 :issue:`4050`\n\n\nVersion 1.1.3\n-------------\n\nReleased 2021-05-13\n\n-   Set maximum versions of Werkzeug, Jinja, Click, and ItsDangerous.\n    :issue:`4043`\n-   Re-add support for passing a ``pathlib.Path`` for ``static_folder``.\n    :pr:`3579`\n\n\nVersion 1.1.2\n-------------\n\nReleased 2020-04-03\n\n-   Work around an issue when running the ``flask`` command with an\n    external debugger on Windows. :issue:`3297`\n-   The static route will not catch all URLs if the ``Flask``\n    ``static_folder`` argument ends with a slash. :issue:`3452`\n\n\nVersion 1.1.1\n-------------\n\nReleased 2019-07-08\n\n-   The ``flask.json_available`` flag was added back for compatibility\n    with some extensions. It will raise a deprecation warning when used,\n    and will be removed in version 2.0.0. :issue:`3288`\n\n\nVersion 1.1.0\n-------------\n\nReleased 2019-07-04\n\n-   Bump minimum Werkzeug version to >= 0.15.\n-   Drop support for Python 3.4.\n-   Error handlers for ``InternalServerError`` or ``500`` will always be\n    passed an instance of ``InternalServerError``. If they are invoked\n    due to an unhandled exception, that original exception is now\n    available as ``e.original_exception`` rather than being passed\n    directly to the handler. The same is true if the handler is for the\n    base ``HTTPException``. This makes error handler behavior more\n    consistent. :pr:`3266`\n\n    -   ``Flask.finalize_request`` is called for all unhandled\n        exceptions even if there is no ``500`` error handler.\n\n-   ``Flask.logger`` takes the same name as ``Flask.name`` (the value\n    passed as ``Flask(import_name)``. This reverts 1.0's behavior of\n    always logging to ``\"flask.app\"``, in order to support multiple apps\n    in the same process. A warning will be shown if old configuration is\n    detected that needs to be moved. :issue:`2866`\n-   ``RequestContext.copy`` includes the current session object in the\n    request context copy. This prevents ``session`` pointing to an\n    out-of-date object. :issue:`2935`\n-   Using built-in RequestContext, unprintable Unicode characters in\n    Host header will result in a HTTP 400 response and not HTTP 500 as\n    previously. :pr:`2994`\n-   ``send_file`` supports ``PathLike`` objects as described in\n    :pep:`519`, to support ``pathlib`` in Python 3. :pr:`3059`\n-   ``send_file`` supports ``BytesIO`` partial content.\n    :issue:`2957`\n-   ``open_resource`` accepts the \"rt\" file mode. This still does the\n    same thing as \"r\". :issue:`3163`\n-   The ``MethodView.methods`` attribute set in a base class is used by\n    subclasses. :issue:`3138`\n-   ``Flask.jinja_options`` is a ``dict`` instead of an\n    ``ImmutableDict`` to allow easier configuration. Changes must still\n    be made before creating the environment. :pr:`3190`\n-   Flask's ``JSONMixin`` for the request and response wrappers was\n    moved into Werkzeug. Use Werkzeug's version with Flask-specific\n    support. This bumps the Werkzeug dependency to >= 0.15.\n    :issue:`3125`\n-   The ``flask`` command entry point is simplified to take advantage\n    of Werkzeug 0.15's better reloader support. This bumps the Werkzeug\n    dependency to >= 0.15. :issue:`3022`\n-   Support ``static_url_path`` that ends with a forward slash.\n    :issue:`3134`\n-   Support empty ``static_folder`` without requiring setting an empty\n    ``static_url_path`` as well. :pr:`3124`\n-   ``jsonify`` supports ``dataclass`` objects. :pr:`3195`\n-   Allow customizing the ``Flask.url_map_class`` used for routing.\n    :pr:`3069`\n-   The development server port can be set to 0, which tells the OS to\n    pick an available port. :issue:`2926`\n-   The return value from ``cli.load_dotenv`` is more consistent with\n    the documentation. It will return ``False`` if python-dotenv is not\n    installed, or if the given path isn't a file. :issue:`2937`\n-   Signaling support has a stub for the ``connect_via`` method when\n    the Blinker library is not installed. :pr:`3208`\n-   Add an ``--extra-files`` option to the ``flask run`` CLI command to\n    specify extra files that will trigger the reloader on change.\n    :issue:`2897`\n-   Allow returning a dictionary from a view function. Similar to how\n    returning a string will produce a ``text/html`` response, returning\n    a dict will call ``jsonify`` to produce a ``application/json``\n    response. :pr:`3111`\n-   Blueprints have a ``cli`` Click group like ``app.cli``. CLI commands\n    registered with a blueprint will be available as a group under the\n    ``flask`` command. :issue:`1357`.\n-   When using the test client as a context manager (``with client:``),\n    all preserved request contexts are popped when the block exits,\n    ensuring nested contexts are cleaned up correctly. :pr:`3157`\n-   Show a better error message when the view return type is not\n    supported. :issue:`3214`\n-   ``flask.testing.make_test_environ_builder()`` has been deprecated in\n    favour of a new class ``flask.testing.EnvironBuilder``. :pr:`3232`\n-   The ``flask run`` command no longer fails if Python is not built\n    with SSL support. Using the ``--cert`` option will show an\n    appropriate error message. :issue:`3211`\n-   URL matching now occurs after the request context is pushed, rather\n    than when it's created. This allows custom URL converters to access\n    the app and request contexts, such as to query a database for an id.\n    :issue:`3088`\n\n\nVersion 1.0.4\n-------------\n\nReleased 2019-07-04\n\n-   The key information for ``BadRequestKeyError`` is no longer cleared\n    outside debug mode, so error handlers can still access it. This\n    requires upgrading to Werkzeug 0.15.5. :issue:`3249`\n-   ``send_file`` url quotes the \":\" and \"/\" characters for more\n    compatible UTF-8 filename support in some browsers. :issue:`3074`\n-   Fixes for :pep:`451` import loaders and pytest 5.x. :issue:`3275`\n-   Show message about dotenv on stderr instead of stdout. :issue:`3285`\n\n\nVersion 1.0.3\n-------------\n\nReleased 2019-05-17\n\n-   ``send_file`` encodes filenames as ASCII instead of Latin-1\n    (ISO-8859-1). This fixes compatibility with Gunicorn, which is\n    stricter about header encodings than :pep:`3333`. :issue:`2766`\n-   Allow custom CLIs using ``FlaskGroup`` to set the debug flag without\n    it always being overwritten based on environment variables.\n    :pr:`2765`\n-   ``flask --version`` outputs Werkzeug's version and simplifies the\n    Python version. :pr:`2825`\n-   ``send_file`` handles an ``attachment_filename`` that is a native\n    Python 2 string (bytes) with UTF-8 coded bytes. :issue:`2933`\n-   A catch-all error handler registered for ``HTTPException`` will not\n    handle ``RoutingException``, which is used internally during\n    routing. This fixes the unexpected behavior that had been introduced\n    in 1.0. :pr:`2986`\n-   Passing the ``json`` argument to ``app.test_client`` does not\n    push/pop an extra app context. :issue:`2900`\n\n\nVersion 1.0.2\n-------------\n\nReleased 2018-05-02\n\n-   Fix more backwards compatibility issues with merging slashes between\n    a blueprint prefix and route. :pr:`2748`\n-   Fix error with ``flask routes`` command when there are no routes.\n    :issue:`2751`\n\n\nVersion 1.0.1\n-------------\n\nReleased 2018-04-29\n\n-   Fix registering partials (with no ``__name__``) as view functions.\n    :pr:`2730`\n-   Don't treat lists returned from view functions the same as tuples.\n    Only tuples are interpreted as response data. :issue:`2736`\n-   Extra slashes between a blueprint's ``url_prefix`` and a route URL\n    are merged. This fixes some backwards compatibility issues with the\n    change in 1.0. :issue:`2731`, :issue:`2742`\n-   Only trap ``BadRequestKeyError`` errors in debug mode, not all\n    ``BadRequest`` errors. This allows ``abort(400)`` to continue\n    working as expected. :issue:`2735`\n-   The ``FLASK_SKIP_DOTENV`` environment variable can be set to ``1``\n    to skip automatically loading dotenv files. :issue:`2722`\n\n\nVersion 1.0\n-----------\n\nReleased 2018-04-26\n\n-   Python 2.6 and 3.3 are no longer supported.\n-   Bump minimum dependency versions to the latest stable versions:\n    Werkzeug >= 0.14, Jinja >= 2.10, itsdangerous >= 0.24, Click >= 5.1.\n    :issue:`2586`\n-   Skip ``app.run`` when a Flask application is run from the command\n    line. This avoids some behavior that was confusing to debug.\n-   Change the default for ``JSONIFY_PRETTYPRINT_REGULAR`` to\n    ``False``. ``~json.jsonify`` returns a compact format by default,\n    and an indented format in debug mode. :pr:`2193`\n-   ``Flask.__init__`` accepts the ``host_matching`` argument and sets\n    it on ``Flask.url_map``. :issue:`1559`\n-   ``Flask.__init__`` accepts the ``static_host`` argument and passes\n    it as the ``host`` argument when defining the static route.\n    :issue:`1559`\n-   ``send_file`` supports Unicode in ``attachment_filename``.\n    :pr:`2223`\n-   Pass ``_scheme`` argument from ``url_for`` to\n    ``Flask.handle_url_build_error``. :pr:`2017`\n-   ``Flask.add_url_rule`` accepts the ``provide_automatic_options``\n    argument to disable adding the ``OPTIONS`` method. :pr:`1489`\n-   ``MethodView`` subclasses inherit method handlers from base classes.\n    :pr:`1936`\n-   Errors caused while opening the session at the beginning of the\n    request are handled by the app's error handlers. :pr:`2254`\n-   Blueprints gained ``Blueprint.json_encoder`` and\n    ``Blueprint.json_decoder`` attributes to override the app's\n    encoder and decoder. :pr:`1898`\n-   ``Flask.make_response`` raises ``TypeError`` instead of\n    ``ValueError`` for bad response types. The error messages have been\n    improved to describe why the type is invalid. :pr:`2256`\n-   Add ``routes`` CLI command to output routes registered on the\n    application. :pr:`2259`\n-   Show warning when session cookie domain is a bare hostname or an IP\n    address, as these may not behave properly in some browsers, such as\n    Chrome. :pr:`2282`\n-   Allow IP address as exact session cookie domain. :pr:`2282`\n-   ``SESSION_COOKIE_DOMAIN`` is set if it is detected through\n    ``SERVER_NAME``. :pr:`2282`\n-   Auto-detect zero-argument app factory called ``create_app`` or\n    ``make_app`` from ``FLASK_APP``. :pr:`2297`\n-   Factory functions are not required to take a ``script_info``\n    parameter to work with the ``flask`` command. If they take a single\n    parameter or a parameter named ``script_info``, the ``ScriptInfo``\n    object will be passed. :pr:`2319`\n-   ``FLASK_APP`` can be set to an app factory, with arguments if\n    needed, for example ``FLASK_APP=myproject.app:create_app('dev')``.\n    :pr:`2326`\n-   ``FLASK_APP`` can point to local packages that are not installed in\n    editable mode, although ``pip install -e`` is still preferred.\n    :pr:`2414`\n-   The ``View`` class attribute\n    ``View.provide_automatic_options`` is set in ``View.as_view``, to be\n    detected by ``Flask.add_url_rule``. :pr:`2316`\n-   Error handling will try handlers registered for ``blueprint, code``,\n    ``app, code``, ``blueprint, exception``, ``app, exception``.\n    :pr:`2314`\n-   ``Cookie`` is added to the response's ``Vary`` header if the session\n    is accessed at all during the request (and not deleted). :pr:`2288`\n-   ``Flask.test_request_context`` accepts ``subdomain`` and\n    ``url_scheme`` arguments for use when building the base URL.\n    :pr:`1621`\n-   Set ``APPLICATION_ROOT`` to ``'/'`` by default. This was already the\n    implicit default when it was set to ``None``.\n-   ``TRAP_BAD_REQUEST_ERRORS`` is enabled by default in debug mode.\n    ``BadRequestKeyError`` has a message with the bad key in debug mode\n    instead of the generic bad request message. :pr:`2348`\n-   Allow registering new tags with ``TaggedJSONSerializer`` to support\n    storing other types in the session cookie. :pr:`2352`\n-   Only open the session if the request has not been pushed onto the\n    context stack yet. This allows ``stream_with_context`` generators to\n    access the same session that the containing view uses. :pr:`2354`\n-   Add ``json`` keyword argument for the test client request methods.\n    This will dump the given object as JSON and set the appropriate\n    content type. :pr:`2358`\n-   Extract JSON handling to a mixin applied to both the ``Request`` and\n    ``Response`` classes. This adds the ``Response.is_json`` and\n    ``Response.get_json`` methods to the response to make testing JSON\n    response much easier. :pr:`2358`\n-   Removed error handler caching because it caused unexpected results\n    for some exception inheritance hierarchies. Register handlers\n    explicitly for each exception if you want to avoid traversing the\n    MRO. :pr:`2362`\n-   Fix incorrect JSON encoding of aware, non-UTC datetimes. :pr:`2374`\n-   Template auto reloading will honor debug mode even if\n    ``Flask.jinja_env`` was already accessed. :pr:`2373`\n-   The following old deprecated code was removed. :issue:`2385`\n\n    -   ``flask.ext`` - import extensions directly by their name instead\n        of through the ``flask.ext`` namespace. For example,\n        ``import flask.ext.sqlalchemy`` becomes\n        ``import flask_sqlalchemy``.\n    -   ``Flask.init_jinja_globals`` - extend\n        ``Flask.create_jinja_environment`` instead.\n    -   ``Flask.error_handlers`` - tracked by\n        ``Flask.error_handler_spec``, use ``Flask.errorhandler``\n        to register handlers.\n    -   ``Flask.request_globals_class`` - use\n        ``Flask.app_ctx_globals_class`` instead.\n    -   ``Flask.static_path`` - use ``Flask.static_url_path`` instead.\n    -   ``Request.module`` - use ``Request.blueprint`` instead.\n\n-   The ``Request.json`` property is no longer deprecated. :issue:`1421`\n-   Support passing a ``EnvironBuilder`` or ``dict`` to\n    ``test_client.open``. :pr:`2412`\n-   The ``flask`` command and ``Flask.run`` will load environment\n    variables from ``.env`` and ``.flaskenv`` files if python-dotenv is\n    installed. :pr:`2416`\n-   When passing a full URL to the test client, the scheme in the URL is\n    used instead of ``PREFERRED_URL_SCHEME``. :pr:`2430`\n-   ``Flask.logger`` has been simplified. ``LOGGER_NAME`` and\n    ``LOGGER_HANDLER_POLICY`` config was removed. The logger is always\n    named ``flask.app``. The level is only set on first access, it\n    doesn't check ``Flask.debug`` each time. Only one format is used,\n    not different ones depending on ``Flask.debug``. No handlers are\n    removed, and a handler is only added if no handlers are already\n    configured. :pr:`2436`\n-   Blueprint view function names may not contain dots. :pr:`2450`\n-   Fix a ``ValueError`` caused by invalid ``Range`` requests in some\n    cases. :issue:`2526`\n-   The development server uses threads by default. :pr:`2529`\n-   Loading config files with ``silent=True`` will ignore ``ENOTDIR``\n    errors. :pr:`2581`\n-   Pass ``--cert`` and ``--key`` options to ``flask run`` to run the\n    development server over HTTPS. :pr:`2606`\n-   Added ``SESSION_COOKIE_SAMESITE`` to control the ``SameSite``\n    attribute on the session cookie. :pr:`2607`\n-   Added ``Flask.test_cli_runner`` to create a Click runner that can\n    invoke Flask CLI commands for testing. :pr:`2636`\n-   Subdomain matching is disabled by default and setting\n    ``SERVER_NAME`` does not implicitly enable it. It can be enabled by\n    passing ``subdomain_matching=True`` to the ``Flask`` constructor.\n    :pr:`2635`\n-   A single trailing slash is stripped from the blueprint\n    ``url_prefix`` when it is registered with the app. :pr:`2629`\n-   ``Request.get_json`` doesn't cache the result if parsing fails when\n    ``silent`` is true. :issue:`2651`\n-   ``Request.get_json`` no longer accepts arbitrary encodings. Incoming\n    JSON should be encoded using UTF-8 per :rfc:`8259`, but Flask will\n    autodetect UTF-8, -16, or -32. :pr:`2691`\n-   Added ``MAX_COOKIE_SIZE`` and ``Response.max_cookie_size`` to\n    control when Werkzeug warns about large cookies that browsers may\n    ignore. :pr:`2693`\n-   Updated documentation theme to make docs look better in small\n    windows. :pr:`2709`\n-   Rewrote the tutorial docs and example project to take a more\n    structured approach to help new users avoid common pitfalls.\n    :pr:`2676`\n\n\nVersion 0.12.5\n--------------\n\nReleased 2020-02-10\n\n-   Pin Werkzeug to < 1.0.0. :issue:`3497`\n\n\nVersion 0.12.4\n--------------\n\nReleased 2018-04-29\n\n-   Repackage 0.12.3 to fix package layout issue. :issue:`2728`\n\n\nVersion 0.12.3\n--------------\n\nReleased 2018-04-26\n\n-   ``Request.get_json`` no longer accepts arbitrary encodings.\n    Incoming JSON should be encoded using UTF-8 per :rfc:`8259`, but\n    Flask will autodetect UTF-8, -16, or -32. :issue:`2692`\n-   Fix a Python warning about imports when using ``python -m flask``.\n    :issue:`2666`\n-   Fix a ``ValueError`` caused by invalid ``Range`` requests in some\n    cases.\n\n\nVersion 0.12.2\n--------------\n\nReleased 2017-05-16\n\n-   Fix a bug in ``safe_join`` on Windows.\n\n\nVersion 0.12.1\n--------------\n\nReleased 2017-03-31\n\n-   Prevent ``flask run`` from showing a ``NoAppException`` when an\n    ``ImportError`` occurs within the imported application module.\n-   Fix encoding behavior of ``app.config.from_pyfile`` for Python 3.\n    :issue:`2118`\n-   Use the ``SERVER_NAME`` config if it is present as default values\n    for ``app.run``. :issue:`2109`, :pr:`2152`\n-   Call ``ctx.auto_pop`` with the exception object instead of ``None``,\n    in the event that a ``BaseException`` such as ``KeyboardInterrupt``\n    is raised in a request handler.\n\n\nVersion 0.12\n------------\n\nReleased 2016-12-21, codename Punsch\n\n-   The cli command now responds to ``--version``.\n-   Mimetype guessing and ETag generation for file-like objects in\n    ``send_file`` has been removed. :issue:`104`, :pr`1849`\n-   Mimetype guessing in ``send_file`` now fails loudly and doesn't fall\n    back to ``application/octet-stream``. :pr:`1988`\n-   Make ``flask.safe_join`` able to join multiple paths like\n    ``os.path.join`` :pr:`1730`\n-   Revert a behavior change that made the dev server crash instead of\n    returning an Internal Server Error. :pr:`2006`\n-   Correctly invoke response handlers for both regular request\n    dispatching as well as error handlers.\n-   Disable logger propagation by default for the app logger.\n-   Add support for range requests in ``send_file``.\n-   ``app.test_client`` includes preset default environment, which can\n    now be directly set, instead of per ``client.get``.\n-   Fix crash when running under PyPy3. :pr:`1814`\n\n\nVersion 0.11.1\n--------------\n\nReleased 2016-06-07\n\n-   Fixed a bug that prevented ``FLASK_APP=foobar/__init__.py`` from\n    working. :pr:`1872`\n\n\nVersion 0.11\n------------\n\nReleased 2016-05-29, codename Absinthe\n\n-   Added support to serializing top-level arrays to ``jsonify``. This\n    introduces a security risk in ancient browsers.\n-   Added before_render_template signal.\n-   Added ``**kwargs`` to ``Flask.test_client`` to support passing\n    additional keyword arguments to the constructor of\n    ``Flask.test_client_class``.\n-   Added ``SESSION_REFRESH_EACH_REQUEST`` config key that controls the\n    set-cookie behavior. If set to ``True`` a permanent session will be\n    refreshed each request and get their lifetime extended, if set to\n    ``False`` it will only be modified if the session actually modifies.\n    Non permanent sessions are not affected by this and will always\n    expire if the browser window closes.\n-   Made Flask support custom JSON mimetypes for incoming data.\n-   Added support for returning tuples in the form ``(response,\n    headers)`` from a view function.\n-   Added ``Config.from_json``.\n-   Added ``Flask.config_class``.\n-   Added ``Config.get_namespace``.\n-   Templates are no longer automatically reloaded outside of debug\n    mode. This can be configured with the new ``TEMPLATES_AUTO_RELOAD``\n    config key.\n-   Added a workaround for a limitation in Python 3.3's namespace\n    loader.\n-   Added support for explicit root paths when using Python 3.3's\n    namespace packages.\n-   Added ``flask`` and the ``flask.cli`` module to start the\n    local debug server through the click CLI system. This is recommended\n    over the old ``flask.run()`` method as it works faster and more\n    reliable due to a different design and also replaces\n    ``Flask-Script``.\n-   Error handlers that match specific classes are now checked first,\n    thereby allowing catching exceptions that are subclasses of HTTP\n    exceptions (in ``werkzeug.exceptions``). This makes it possible for\n    an extension author to create exceptions that will by default result\n    in the HTTP error of their choosing, but may be caught with a custom\n    error handler if desired.\n-   Added ``Config.from_mapping``.\n-   Flask will now log by default even if debug is disabled. The log\n    format is now hardcoded but the default log handling can be disabled\n    through the ``LOGGER_HANDLER_POLICY`` configuration key.\n-   Removed deprecated module functionality.\n-   Added the ``EXPLAIN_TEMPLATE_LOADING`` config flag which when\n    enabled will instruct Flask to explain how it locates templates.\n    This should help users debug when the wrong templates are loaded.\n-   Enforce blueprint handling in the order they were registered for\n    template loading.\n-   Ported test suite to py.test.\n-   Deprecated ``request.json`` in favour of ``request.get_json()``.\n-   Add \"pretty\" and \"compressed\" separators definitions in jsonify()\n    method. Reduces JSON response size when\n    ``JSONIFY_PRETTYPRINT_REGULAR=False`` by removing unnecessary white\n    space included by default after separators.\n-   JSON responses are now terminated with a newline character, because\n    it is a convention that UNIX text files end with a newline and some\n    clients don't deal well when this newline is missing. :pr:`1262`\n-   The automatically provided ``OPTIONS`` method is now correctly\n    disabled if the user registered an overriding rule with the\n    lowercase-version ``options``. :issue:`1288`\n-   ``flask.json.jsonify`` now supports the ``datetime.date`` type.\n    :pr:`1326`\n-   Don't leak exception info of already caught exceptions to context\n    teardown handlers. :pr:`1393`\n-   Allow custom Jinja environment subclasses. :pr:`1422`\n-   Updated extension dev guidelines.\n-   ``flask.g`` now has ``pop()`` and ``setdefault`` methods.\n-   Turn on autoescape for ``flask.templating.render_template_string``\n    by default. :pr:`1515`\n-   ``flask.ext`` is now deprecated. :pr:`1484`\n-   ``send_from_directory`` now raises BadRequest if the filename is\n    invalid on the server OS. :pr:`1763`\n-   Added the ``JSONIFY_MIMETYPE`` configuration variable. :pr:`1728`\n-   Exceptions during teardown handling will no longer leave bad\n    application contexts lingering around.\n-   Fixed broken ``test_appcontext_signals()`` test case.\n-   Raise an ``AttributeError`` in ``helpers.find_package`` with a\n    useful message explaining why it is raised when a :pep:`302` import\n    hook is used without an ``is_package()`` method.\n-   Fixed an issue causing exceptions raised before entering a request\n    or app context to be passed to teardown handlers.\n-   Fixed an issue with query parameters getting removed from requests\n    in the test client when absolute URLs were requested.\n-   Made ``@before_first_request`` into a decorator as intended.\n-   Fixed an etags bug when sending a file streams with a name.\n-   Fixed ``send_from_directory`` not expanding to the application root\n    path correctly.\n-   Changed logic of before first request handlers to flip the flag\n    after invoking. This will allow some uses that are potentially\n    dangerous but should probably be permitted.\n-   Fixed Python 3 bug when a handler from\n    ``app.url_build_error_handlers`` reraises the ``BuildError``.\n\n\nVersion 0.10.1\n--------------\n\nReleased 2013-06-14\n\n-   Fixed an issue where ``|tojson`` was not quoting single quotes which\n    made the filter not work properly in HTML attributes. Now it's\n    possible to use that filter in single quoted attributes. This should\n    make using that filter with angular.js easier.\n-   Added support for byte strings back to the session system. This\n    broke compatibility with the common case of people putting binary\n    data for token verification into the session.\n-   Fixed an issue where registering the same method twice for the same\n    endpoint would trigger an exception incorrectly.\n\n\nVersion 0.10\n------------\n\nReleased 2013-06-13, codename Limoncello\n\n-   Changed default cookie serialization format from pickle to JSON to\n    limit the impact an attacker can do if the secret key leaks.\n-   Added ``template_test`` methods in addition to the already existing\n    ``template_filter`` method family.\n-   Added ``template_global`` methods in addition to the already\n    existing ``template_filter`` method family.\n-   Set the content-length header for x-sendfile.\n-   ``tojson`` filter now does not escape script blocks in HTML5\n    parsers.\n-   ``tojson`` used in templates is now safe by default. This was\n    allowed due to the different escaping behavior.\n-   Flask will now raise an error if you attempt to register a new\n    function on an already used endpoint.\n-   Added wrapper module around simplejson and added default\n    serialization of datetime objects. This allows much easier\n    customization of how JSON is handled by Flask or any Flask\n    extension.\n-   Removed deprecated internal ``flask.session`` module alias. Use\n    ``flask.sessions`` instead to get the session module. This is not to\n    be confused with ``flask.session`` the session proxy.\n-   Templates can now be rendered without request context. The behavior\n    is slightly different as the ``request``, ``session`` and ``g``\n    objects will not be available and blueprint's context processors are\n    not called.\n-   The config object is now available to the template as a real global\n    and not through a context processor which makes it available even in\n    imported templates by default.\n-   Added an option to generate non-ascii encoded JSON which should\n    result in less bytes being transmitted over the network. It's\n    disabled by default to not cause confusion with existing libraries\n    that might expect ``flask.json.dumps`` to return bytes by default.\n-   ``flask.g`` is now stored on the app context instead of the request\n    context.\n-   ``flask.g`` now gained a ``get()`` method for not erroring out on\n    non existing items.\n-   ``flask.g`` now can be used with the ``in`` operator to see what's\n    defined and it now is iterable and will yield all attributes stored.\n-   ``flask.Flask.request_globals_class`` got renamed to\n    ``flask.Flask.app_ctx_globals_class`` which is a better name to what\n    it does since 0.10.\n-   ``request``, ``session`` and ``g`` are now also added as proxies to\n    the template context which makes them available in imported\n    templates. One has to be very careful with those though because\n    usage outside of macros might cause caching.\n-   Flask will no longer invoke the wrong error handlers if a proxy\n    exception is passed through.\n-   Added a workaround for chrome's cookies in localhost not working as\n    intended with domain names.\n-   Changed logic for picking defaults for cookie values from sessions\n    to work better with Google Chrome.\n-   Added ``message_flashed`` signal that simplifies flashing testing.\n-   Added support for copying of request contexts for better working\n    with greenlets.\n-   Removed custom JSON HTTP exception subclasses. If you were relying\n    on them you can reintroduce them again yourself trivially. Using\n    them however is strongly discouraged as the interface was flawed.\n-   Python requirements changed: requiring Python 2.6 or 2.7 now to\n    prepare for Python 3.3 port.\n-   Changed how the teardown system is informed about exceptions. This\n    is now more reliable in case something handles an exception halfway\n    through the error handling process.\n-   Request context preservation in debug mode now keeps the exception\n    information around which means that teardown handlers are able to\n    distinguish error from success cases.\n-   Added the ``JSONIFY_PRETTYPRINT_REGULAR`` configuration variable.\n-   Flask now orders JSON keys by default to not trash HTTP caches due\n    to different hash seeds between different workers.\n-   Added ``appcontext_pushed`` and ``appcontext_popped`` signals.\n-   The builtin run method now takes the ``SERVER_NAME`` into account\n    when picking the default port to run on.\n-   Added ``flask.request.get_json()`` as a replacement for the old\n    ``flask.request.json`` property.\n\n\nVersion 0.9\n-----------\n\nReleased 2012-07-01, codename Campari\n\n-   The ``Request.on_json_loading_failed`` now returns a JSON formatted\n    response by default.\n-   The ``url_for`` function now can generate anchors to the generated\n    links.\n-   The ``url_for`` function now can also explicitly generate URL rules\n    specific to a given HTTP method.\n-   Logger now only returns the debug log setting if it was not set\n    explicitly.\n-   Unregister a circular dependency between the WSGI environment and\n    the request object when shutting down the request. This means that\n    environ ``werkzeug.request`` will be ``None`` after the response was\n    returned to the WSGI server but has the advantage that the garbage\n    collector is not needed on CPython to tear down the request unless\n    the user created circular dependencies themselves.\n-   Session is now stored after callbacks so that if the session payload\n    is stored in the session you can still modify it in an after request\n    callback.\n-   The ``Flask`` class will avoid importing the provided import name if\n    it can (the required first parameter), to benefit tools which build\n    Flask instances programmatically. The Flask class will fall back to\n    using import on systems with custom module hooks, e.g. Google App\n    Engine, or when the import name is inside a zip archive (usually an\n    egg) prior to Python 2.7.\n-   Blueprints now have a decorator to add custom template filters\n    application wide, ``Blueprint.app_template_filter``.\n-   The Flask and Blueprint classes now have a non-decorator method for\n    adding custom template filters application wide,\n    ``Flask.add_template_filter`` and\n    ``Blueprint.add_app_template_filter``.\n-   The ``get_flashed_messages`` function now allows rendering flashed\n    message categories in separate blocks, through a ``category_filter``\n    argument.\n-   The ``Flask.run`` method now accepts ``None`` for ``host`` and\n    ``port`` arguments, using default values when ``None``. This allows\n    for calling run using configuration values, e.g.\n    ``app.run(app.config.get('MYHOST'), app.config.get('MYPORT'))``,\n    with proper behavior whether or not a config file is provided.\n-   The ``render_template`` method now accepts a either an iterable of\n    template names or a single template name. Previously, it only\n    accepted a single template name. On an iterable, the first template\n    found is rendered.\n-   Added ``Flask.app_context`` which works very similar to the request\n    context but only provides access to the current application. This\n    also adds support for URL generation without an active request\n    context.\n-   View functions can now return a tuple with the first instance being\n    an instance of ``Response``. This allows for returning\n    ``jsonify(error=\"error msg\"), 400`` from a view function.\n-   ``Flask`` and ``Blueprint`` now provide a ``get_send_file_max_age``\n    hook for subclasses to override behavior of serving static files\n    from Flask when using ``Flask.send_static_file`` (used for the\n    default static file handler) and ``helpers.send_file``. This hook is\n    provided a filename, which for example allows changing cache\n    controls by file extension. The default max-age for ``send_file``\n    and static files can be configured through a new\n    ``SEND_FILE_MAX_AGE_DEFAULT`` configuration variable, which is used\n    in the default ``get_send_file_max_age`` implementation.\n-   Fixed an assumption in sessions implementation which could break\n    message flashing on sessions implementations which use external\n    storage.\n-   Changed the behavior of tuple return values from functions. They are\n    no longer arguments to the response object, they now have a defined\n    meaning.\n-   Added ``Flask.request_globals_class`` to allow a specific class to\n    be used on creation of the ``g`` instance of each request.\n-   Added ``required_methods`` attribute to view functions to force-add\n    methods on registration.\n-   Added ``flask.after_this_request``.\n-   Added ``flask.stream_with_context`` and the ability to push contexts\n    multiple times without producing unexpected behavior.\n\n\nVersion 0.8.1\n-------------\n\nReleased 2012-07-01\n\n-   Fixed an issue with the undocumented ``flask.session`` module to not\n    work properly on Python 2.5. It should not be used but did cause\n    some problems for package managers.\n\n\nVersion 0.8\n-----------\n\nReleased 2011-09-29, codename Rakija\n\n-   Refactored session support into a session interface so that the\n    implementation of the sessions can be changed without having to\n    override the Flask class.\n-   Empty session cookies are now deleted properly automatically.\n-   View functions can now opt out of getting the automatic OPTIONS\n    implementation.\n-   HTTP exceptions and Bad Request errors can now be trapped so that\n    they show up normally in the traceback.\n-   Flask in debug mode is now detecting some common problems and tries\n    to warn you about them.\n-   Flask in debug mode will now complain with an assertion error if a\n    view was attached after the first request was handled. This gives\n    earlier feedback when users forget to import view code ahead of\n    time.\n-   Added the ability to register callbacks that are only triggered once\n    at the beginning of the first request with\n    ``Flask.before_first_request``.\n-   Malformed JSON data will now trigger a bad request HTTP exception\n    instead of a value error which usually would result in a 500\n    internal server error if not handled. This is a backwards\n    incompatible change.\n-   Applications now not only have a root path where the resources and\n    modules are located but also an instance path which is the\n    designated place to drop files that are modified at runtime (uploads\n    etc.). Also this is conceptually only instance depending and outside\n    version control so it's the perfect place to put configuration files\n    etc.\n-   Added the ``APPLICATION_ROOT`` configuration variable.\n-   Implemented ``TestClient.session_transaction`` to easily modify\n    sessions from the test environment.\n-   Refactored test client internally. The ``APPLICATION_ROOT``\n    configuration variable as well as ``SERVER_NAME`` are now properly\n    used by the test client as defaults.\n-   Added ``View.decorators`` to support simpler decorating of pluggable\n    (class-based) views.\n-   Fixed an issue where the test client if used with the \"with\"\n    statement did not trigger the execution of the teardown handlers.\n-   Added finer control over the session cookie parameters.\n-   HEAD requests to a method view now automatically dispatch to the\n    ``get`` method if no handler was implemented.\n-   Implemented the virtual ``flask.ext`` package to import extensions\n    from.\n-   The context preservation on exceptions is now an integral component\n    of Flask itself and no longer of the test client. This cleaned up\n    some internal logic and lowers the odds of runaway request contexts\n    in unittests.\n-   Fixed the Jinja environment's ``list_templates`` method not\n    returning the correct names when blueprints or modules were\n    involved.\n\n\nVersion 0.7.2\n-------------\n\nReleased 2011-07-06\n\n-   Fixed an issue with URL processors not properly working on\n    blueprints.\n\n\nVersion 0.7.1\n-------------\n\nReleased 2011-06-29\n\n-   Added missing future import that broke 2.5 compatibility.\n-   Fixed an infinite redirect issue with blueprints.\n\n\nVersion 0.7\n-----------\n\nReleased 2011-06-28, codename Grappa\n\n-   Added ``Flask.make_default_options_response`` which can be used by\n    subclasses to alter the default behavior for ``OPTIONS`` responses.\n-   Unbound locals now raise a proper ``RuntimeError`` instead of an\n    ``AttributeError``.\n-   Mimetype guessing and etag support based on file objects is now\n    deprecated for ``send_file`` because it was unreliable. Pass\n    filenames instead or attach your own etags and provide a proper\n    mimetype by hand.\n-   Static file handling for modules now requires the name of the static\n    folder to be supplied explicitly. The previous autodetection was not\n    reliable and caused issues on Google's App Engine. Until 1.0 the old\n    behavior will continue to work but issue dependency warnings.\n-   Fixed a problem for Flask to run on jython.\n-   Added a ``PROPAGATE_EXCEPTIONS`` configuration variable that can be\n    used to flip the setting of exception propagation which previously\n    was linked to ``DEBUG`` alone and is now linked to either ``DEBUG``\n    or ``TESTING``.\n-   Flask no longer internally depends on rules being added through the\n    ``add_url_rule`` function and can now also accept regular werkzeug\n    rules added to the url map.\n-   Added an ``endpoint`` method to the flask application object which\n    allows one to register a callback to an arbitrary endpoint with a\n    decorator.\n-   Use Last-Modified for static file sending instead of Date which was\n    incorrectly introduced in 0.6.\n-   Added ``create_jinja_loader`` to override the loader creation\n    process.\n-   Implemented a silent flag for ``config.from_pyfile``.\n-   Added ``teardown_request`` decorator, for functions that should run\n    at the end of a request regardless of whether an exception occurred.\n    Also the behavior for ``after_request`` was changed. It's now no\n    longer executed when an exception is raised.\n-   Implemented ``has_request_context``.\n-   Deprecated ``init_jinja_globals``. Override the\n    ``Flask.create_jinja_environment`` method instead to achieve the\n    same functionality.\n-   Added ``safe_join``.\n-   The automatic JSON request data unpacking now looks at the charset\n    mimetype parameter.\n-   Don't modify the session on ``get_flashed_messages`` if there are no\n    messages in the session.\n-   ``before_request`` handlers are now able to abort requests with\n    errors.\n-   It is not possible to define user exception handlers. That way you\n    can provide custom error messages from a central hub for certain\n    errors that might occur during request processing (for instance\n    database connection errors, timeouts from remote resources etc.).\n-   Blueprints can provide blueprint specific error handlers.\n-   Implemented generic class-based views.\n\n\nVersion 0.6.1\n-------------\n\nReleased 2010-12-31\n\n-   Fixed an issue where the default ``OPTIONS`` response was not\n    exposing all valid methods in the ``Allow`` header.\n-   Jinja template loading syntax now allows \"./\" in front of a\n    template load path. Previously this caused issues with module\n    setups.\n-   Fixed an issue where the subdomain setting for modules was ignored\n    for the static folder.\n-   Fixed a security problem that allowed clients to download arbitrary\n    files if the host server was a windows based operating system and\n    the client uses backslashes to escape the directory the files where\n    exposed from.\n\n\nVersion 0.6\n-----------\n\nReleased 2010-07-27, codename Whisky\n\n-   After request functions are now called in reverse order of\n    registration.\n-   OPTIONS is now automatically implemented by Flask unless the\n    application explicitly adds 'OPTIONS' as method to the URL rule. In\n    this case no automatic OPTIONS handling kicks in.\n-   Static rules are now even in place if there is no static folder for\n    the module. This was implemented to aid GAE which will remove the\n    static folder if it's part of a mapping in the .yml file.\n-   ``Flask.config`` is now available in the templates as ``config``.\n-   Context processors will no longer override values passed directly to\n    the render function.\n-   Added the ability to limit the incoming request data with the new\n    ``MAX_CONTENT_LENGTH`` configuration value.\n-   The endpoint for the ``Module.add_url_rule`` method is now optional\n    to be consistent with the function of the same name on the\n    application object.\n-   Added a ``make_response`` function that simplifies creating response\n    object instances in views.\n-   Added signalling support based on blinker. This feature is currently\n    optional and supposed to be used by extensions and applications. If\n    you want to use it, make sure to have ``blinker`` installed.\n-   Refactored the way URL adapters are created. This process is now\n    fully customizable with the ``Flask.create_url_adapter`` method.\n-   Modules can now register for a subdomain instead of just an URL\n    prefix. This makes it possible to bind a whole module to a\n    configurable subdomain.\n\n\nVersion 0.5.2\n-------------\n\nReleased 2010-07-15\n\n-   Fixed another issue with loading templates from directories when\n    modules were used.\n\n\nVersion 0.5.1\n-------------\n\nReleased 2010-07-06\n\n-   Fixes an issue with template loading from directories when modules\n    where used.\n\n\nVersion 0.5\n-----------\n\nReleased 2010-07-06, codename Calvados\n\n-   Fixed a bug with subdomains that was caused by the inability to\n    specify the server name. The server name can now be set with the\n    ``SERVER_NAME`` config key. This key is now also used to set the\n    session cookie cross-subdomain wide.\n-   Autoescaping is no longer active for all templates. Instead it is\n    only active for ``.html``, ``.htm``, ``.xml`` and ``.xhtml``. Inside\n    templates this behavior can be changed with the ``autoescape`` tag.\n-   Refactored Flask internally. It now consists of more than a single\n    file.\n-   ``send_file`` now emits etags and has the ability to do conditional\n    responses builtin.\n-   (temporarily) dropped support for zipped applications. This was a\n    rarely used feature and led to some confusing behavior.\n-   Added support for per-package template and static-file directories.\n-   Removed support for ``create_jinja_loader`` which is no longer used\n    in 0.5 due to the improved module support.\n-   Added a helper function to expose files from any directory.\n\n\nVersion 0.4\n-----------\n\nReleased 2010-06-18, codename Rakia\n\n-   Added the ability to register application wide error handlers from\n    modules.\n-   ``Flask.after_request`` handlers are now also invoked if the request\n    dies with an exception and an error handling page kicks in.\n-   Test client has not the ability to preserve the request context for\n    a little longer. This can also be used to trigger custom requests\n    that do not pop the request stack for testing.\n-   Because the Python standard library caches loggers, the name of the\n    logger is configurable now to better support unittests.\n-   Added ``TESTING`` switch that can activate unittesting helpers.\n-   The logger switches to ``DEBUG`` mode now if debug is enabled.\n\n\nVersion 0.3.1\n-------------\n\nReleased 2010-05-28\n\n-   Fixed a error reporting bug with ``Config.from_envvar``.\n-   Removed some unused code.\n-   Release does no longer include development leftover files (.git\n    folder for themes, built documentation in zip and pdf file and some\n    .pyc files)\n\n\nVersion 0.3\n-----------\n\nReleased 2010-05-28, codename Schnaps\n\n-   Added support for categories for flashed messages.\n-   The application now configures a ``logging.Handler`` and will log\n    request handling exceptions to that logger when not in debug mode.\n    This makes it possible to receive mails on server errors for\n    example.\n-   Added support for context binding that does not require the use of\n    the with statement for playing in the console.\n-   The request context is now available within the with statement\n    making it possible to further push the request context or pop it.\n-   Added support for configurations.\n\n\nVersion 0.2\n-----------\n\nReleased 2010-05-12, codename J?germeister\n\n-   Various bugfixes\n-   Integrated JSON support\n-   Added ``get_template_attribute`` helper function.\n-   ``Flask.add_url_rule`` can now also register a view function.\n-   Refactored internal request dispatching.\n-   Server listens on 127.0.0.1 by default now to fix issues with\n    chrome.\n-   Added external URL support.\n-   Added support for ``send_file``.\n-   Module support and internal request handling refactoring to better\n    support pluggable applications.\n-   Sessions can be set to be permanent now on a per-session basis.\n-   Better error reporting on missing secret keys.\n-   Added support for Google Appengine.\n\n\nVersion 0.1\n-----------\n\nReleased 2010-04-16\n\n-   First public preview release.\n", "LICENSE.txt": "Copyright 2010 Pallets\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n1.  Redistributions of source code must retain the above copyright\n    notice, this list of conditions and the following disclaimer.\n\n2.  Redistributions in binary form must reproduce the above copyright\n    notice, this list of conditions and the following disclaimer in the\n    documentation and/or other materials provided with the distribution.\n\n3.  Neither the name of the copyright holder nor the names of its\n    contributors may be used to endorse or promote products derived from\n    this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\nPARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nHOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED\nTO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\nPROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\nLIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\nNEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n", "README.md": "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/pallets/flask/refs/heads/stable/docs/_static/flask-name.svg\" alt=\"\" height=\"150\"></div>\n\n# Flask\n\nFlask is a lightweight [WSGI] web application framework. It is designed\nto make getting started quick and easy, with the ability to scale up to\ncomplex applications. It began as a simple wrapper around [Werkzeug]\nand [Jinja], and has become one of the most popular Python web\napplication frameworks.\n\nFlask offers suggestions, but doesn't enforce any dependencies or\nproject layout. It is up to the developer to choose the tools and\nlibraries they want to use. There are many extensions provided by the\ncommunity that make adding new functionality easy.\n\n[WSGI]: https://wsgi.readthedocs.io/\n[Werkzeug]: https://werkzeug.palletsprojects.com/\n[Jinja]: https://jinja.palletsprojects.com/\n\n## A Simple Example\n\n```python\n# save this as app.py\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello():\n    return \"Hello, World!\"\n```\n\n```\n$ flask run\n  * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n```\n\n## Donate\n\nThe Pallets organization develops and supports Flask and the libraries\nit uses. In order to grow the community of contributors and users, and\nallow the maintainers to devote more time to the projects, [please\ndonate today].\n\n[please donate today]: https://palletsprojects.com/donate\n\n## Contributing\n\nSee our [detailed contributing documentation][contrib] for many ways to\ncontribute, including reporting issues, requesting features, asking or answering\nquestions, and making PRs.\n\n[contrib]: https://palletsprojects.com/contributing/\n", "docs/api.rst": "API\n===\n\n.. module:: flask\n\nThis part of the documentation covers all the interfaces of Flask. For\nparts where Flask depends on external libraries, we document the most\nimportant right here and provide links to the canonical documentation.\n\n\nApplication Object\n------------------\n\n.. autoclass:: Flask\n   :members:\n   :inherited-members:\n\n\nBlueprint Objects\n-----------------\n\n.. autoclass:: Blueprint\n   :members:\n   :inherited-members:\n\nIncoming Request Data\n---------------------\n\n.. autoclass:: Request\n    :members:\n    :inherited-members:\n    :exclude-members: json_module\n\n.. data:: request\n\n    A proxy to the request data for the current request, an instance of\n    :class:`.Request`.\n\n    This is only available when a :doc:`request context </appcontext>` is\n    active.\n\n    This is a proxy. See :ref:`context-visibility` for more information.\n\n\nResponse Objects\n----------------\n\n.. autoclass:: flask.Response\n    :members:\n    :inherited-members:\n    :exclude-members: json_module\n\nSessions\n--------\n\nIf you have set :attr:`Flask.secret_key` (or configured it from\n:data:`SECRET_KEY`) you can use sessions in Flask applications. A session makes\nit possible to remember information from one request to another. The way Flask\ndoes this is by using a signed cookie. The user can look at the session\ncontents, but can't modify it unless they know the secret key, so make sure to\nset that to something complex and unguessable.\n\nTo access the current session you can use the :data:`.session` proxy.\n\n.. data:: session\n\n    A proxy to the session data for the current request, an instance of\n    :class:`.SessionMixin`.\n\n    This is only available when a :doc:`request context </appcontext>` is\n    active.\n\n    This is a proxy. See :ref:`context-visibility` for more information.\n\n    The session object works like a dict but tracks assignment and access to its\n    keys. It cannot track modifications to mutable values, you need to set\n    :attr:`~.SessionMixin.modified` manually when modifying a list, dict, etc.\n\n    .. code-block:: python\n\n          # appending to a list is not detected\n          session[\"numbers\"].append(42)\n          # so mark it as modified yourself\n          session.modified = True\n\n    The session is persisted across requests using a cookie. By default the\n    users's browser will clear the cookie when it is closed. Set\n    :attr:`~.SessionMixin.permanent` to ``True`` to persist the cookie for\n    :data:`PERMANENT_SESSION_LIFETIME`.\n\n\nSession Interface\n-----------------\n\n.. versionadded:: 0.8\n\nThe session interface provides a simple way to replace the session\nimplementation that Flask is using.\n\n.. currentmodule:: flask.sessions\n\n.. autoclass:: SessionInterface\n   :members:\n\n.. autoclass:: SecureCookieSessionInterface\n   :members:\n\n.. autoclass:: SecureCookieSession\n   :members:\n\n.. autoclass:: NullSession\n   :members:\n\n.. autoclass:: SessionMixin\n   :members:\n\n.. admonition:: Notice\n\n    The :data:`PERMANENT_SESSION_LIFETIME` config can be an integer or ``timedelta``.\n    The :attr:`~flask.Flask.permanent_session_lifetime` attribute is always a\n    ``timedelta``.\n\n\nTest Client\n-----------\n\n.. currentmodule:: flask.testing\n\n.. autoclass:: FlaskClient\n   :members:\n\n\nTest CLI Runner\n---------------\n\n.. currentmodule:: flask.testing\n\n.. autoclass:: FlaskCliRunner\n    :members:\n\n\nApplication Globals\n-------------------\n\n.. currentmodule:: flask\n\nTo share data that is valid for one request only from one function to\nanother, a global variable is not good enough because it would break in\nthreaded environments. Flask provides you with a special object that\nensures it is only valid for the active request and that will return\ndifferent values for each request. In a nutshell: it does the right\nthing, like it does for :data:`.request` and :data:`.session`.\n\n.. data:: g\n\n    A proxy to a namespace object used to store data during a single request or\n    app context. An instance of :attr:`.Flask.app_ctx_globals_class`, which\n    defaults to :class:`._AppCtxGlobals`.\n\n    This is a good place to store resources during a request. For example, a\n    :meth:`~.Flask.before_request` function could load a user object from a\n    session id, then set ``g.user`` to be used in the view function.\n\n    This is only available when an :doc:`app context </appcontext>` is active.\n\n    This is a proxy. See :ref:`context-visibility` for more information.\n\n    .. versionchanged:: 0.10\n        Bound to the application context instead of the request context.\n\n.. autoclass:: flask.ctx._AppCtxGlobals\n    :members:\n\n\nUseful Functions and Classes\n----------------------------\n\n.. data:: current_app\n\n    A proxy to the :class:`.Flask` application handling the current request or\n    other activity.\n\n    This is useful to access the application without needing to import it, or if\n    it can't be imported, such as when using the application factory pattern or\n    in blueprints and extensions.\n\n    This is only available when an :doc:`app context </appcontext>` is active.\n\n    This is a proxy. See :ref:`context-visibility` for more information.\n\n.. autofunction:: has_request_context\n\n.. autofunction:: copy_current_request_context\n\n.. autofunction:: has_app_context\n\n.. autofunction:: url_for\n\n.. autofunction:: abort\n\n.. autofunction:: redirect\n\n.. autofunction:: make_response\n\n.. autofunction:: after_this_request\n\n.. autofunction:: send_file\n\n.. autofunction:: send_from_directory\n\n\nMessage Flashing\n----------------\n\n.. autofunction:: flash\n\n.. autofunction:: get_flashed_messages\n\n\nJSON Support\n------------\n\n.. module:: flask.json\n\nFlask uses Python's built-in :mod:`json` module for handling JSON by\ndefault. The JSON implementation can be changed by assigning a different\nprovider to :attr:`flask.Flask.json_provider_class` or\n:attr:`flask.Flask.json`. The functions provided by ``flask.json`` will\nuse methods on ``app.json`` if an app context is active.\n\nJinja's ``|tojson`` filter is configured to use the app's JSON provider.\nThe filter marks the output with ``|safe``. Use it to render data inside\nHTML ``<script>`` tags.\n\n.. sourcecode:: html+jinja\n\n    <script>\n        const names = {{ names|tojson }};\n        renderChart(names, {{ axis_data|tojson }});\n    </script>\n\n.. autofunction:: jsonify\n\n.. autofunction:: dumps\n\n.. autofunction:: dump\n\n.. autofunction:: loads\n\n.. autofunction:: load\n\n.. autoclass:: flask.json.provider.JSONProvider\n    :members:\n    :member-order: bysource\n\n.. autoclass:: flask.json.provider.DefaultJSONProvider\n    :members:\n    :member-order: bysource\n\n.. automodule:: flask.json.tag\n\n\nTemplate Rendering\n------------------\n\n.. currentmodule:: flask\n\n.. autofunction:: render_template\n\n.. autofunction:: render_template_string\n\n.. autofunction:: stream_template\n\n.. autofunction:: stream_template_string\n\n.. autofunction:: get_template_attribute\n\nConfiguration\n-------------\n\n.. autoclass:: Config\n   :members:\n\n\nStream Helpers\n--------------\n\n.. autofunction:: stream_with_context\n\nUseful Internals\n----------------\n\n.. autoclass:: flask.ctx.AppContext\n   :members:\n\n.. data:: flask.globals.app_ctx\n\n    A proxy to the active :class:`.AppContext`.\n\n    This is an internal object that is essential to how Flask handles requests.\n    Accessing this should not be needed in most cases. Most likely you want\n    :data:`.current_app`, :data:`.g`, :data:`.request`, and :data:`.session` instead.\n\n    This is only available when a :doc:`request context </appcontext>` is\n    active.\n\n    This is a proxy. See :ref:`context-visibility` for more information.\n\n.. class:: flask.ctx.RequestContext\n\n    .. deprecated:: 3.2\n        Merged with :class:`AppContext`. This alias will be removed in Flask 4.0.\n\n.. data:: flask.globals.request_ctx\n\n    .. deprecated:: 3.2\n        Merged with :data:`.app_ctx`. This alias will be removed in Flask 4.0.\n\n.. autoclass:: flask.blueprints.BlueprintSetupState\n   :members:\n\n.. _core-signals-list:\n\nSignals\n-------\n\nSignals are provided by the `Blinker`_ library. See :doc:`signals` for an introduction.\n\n.. _blinker: https://blinker.readthedocs.io/\n\n.. data:: template_rendered\n\n   This signal is sent when a template was successfully rendered. The\n   signal is invoked with the instance of the template as `template`\n   and the context as dictionary (named `context`).\n\n   Example subscriber::\n\n        def log_template_renders(sender, template, context, **extra):\n            sender.logger.debug('Rendering template \"%s\" with context %s',\n                                template.name or 'string template',\n                                context)\n\n        from flask import template_rendered\n        template_rendered.connect(log_template_renders, app)\n\n.. data:: flask.before_render_template\n   :noindex:\n\n   This signal is sent before template rendering process. The\n   signal is invoked with the instance of the template as `template`\n   and the context as dictionary (named `context`).\n\n   Example subscriber::\n\n        def log_template_renders(sender, template, context, **extra):\n            sender.logger.debug('Rendering template \"%s\" with context %s',\n                                template.name or 'string template',\n                                context)\n\n        from flask import before_render_template\n        before_render_template.connect(log_template_renders, app)\n\n.. data:: request_started\n\n   This signal is sent when the request context is set up, before\n   any request processing happens. Because the request context is already\n   bound, the subscriber can access the request with the standard global\n   proxies such as :class:`~flask.request`.\n\n   Example subscriber::\n\n        def log_request(sender, **extra):\n            sender.logger.debug('Request context is set up')\n\n        from flask import request_started\n        request_started.connect(log_request, app)\n\n.. data:: request_finished\n\n   This signal is sent right before the response is sent to the client.\n   It is passed the response to be sent named `response`.\n\n   Example subscriber::\n\n        def log_response(sender, response, **extra):\n            sender.logger.debug('Request context is about to close down. '\n                                'Response: %s', response)\n\n        from flask import request_finished\n        request_finished.connect(log_response, app)\n\n.. data:: got_request_exception\n\n    This signal is sent when an unhandled exception happens during\n    request processing, including when debugging. The exception is\n    passed to the subscriber as ``exception``.\n\n    This signal is not sent for\n    :exc:`~werkzeug.exceptions.HTTPException`, or other exceptions that\n    have error handlers registered, unless the exception was raised from\n    an error handler.\n\n    This example shows how to do some extra logging if a theoretical\n    ``SecurityException`` was raised:\n\n    .. code-block:: python\n\n        from flask import got_request_exception\n\n        def log_security_exception(sender, exception, **extra):\n            if not isinstance(exception, SecurityException):\n                return\n\n            security_logger.exception(\n                f\"SecurityException at {request.url!r}\",\n                exc_info=exception,\n            )\n\n        got_request_exception.connect(log_security_exception, app)\n\n.. data:: request_tearing_down\n\n   This signal is sent when the request is tearing down. This is always\n   called, even if an exception is caused. Currently functions listening\n   to this signal are called after the regular teardown handlers, but this\n   is not something you can rely on.\n\n   Example subscriber::\n\n        def close_db_connection(sender, **extra):\n            session.close()\n\n        from flask import request_tearing_down\n        request_tearing_down.connect(close_db_connection, app)\n\n   As of Flask 0.9, this will also be passed an `exc` keyword argument\n   that has a reference to the exception that caused the teardown if\n   there was one.\n\n.. data:: appcontext_tearing_down\n\n   This signal is sent when the app context is tearing down. This is always\n   called, even if an exception is caused. Currently functions listening\n   to this signal are called after the regular teardown handlers, but this\n   is not something you can rely on.\n\n   Example subscriber::\n\n        def close_db_connection(sender, **extra):\n            session.close()\n\n        from flask import appcontext_tearing_down\n        appcontext_tearing_down.connect(close_db_connection, app)\n\n   This will also be passed an `exc` keyword argument that has a reference\n   to the exception that caused the teardown if there was one.\n\n.. data:: appcontext_pushed\n\n   This signal is sent when an application context is pushed. The sender\n   is the application. This is usually useful for unittests in order to\n   temporarily hook in information. For instance it can be used to\n   set a resource early onto the `g` object.\n\n   Example usage::\n\n        from contextlib import contextmanager\n        from flask import appcontext_pushed\n\n        @contextmanager\n        def user_set(app, user):\n            def handler(sender, **kwargs):\n                g.user = user\n            with appcontext_pushed.connected_to(handler, app):\n                yield\n\n   And in the testcode::\n\n        def test_user_me(self):\n            with user_set(app, 'john'):\n                c = app.test_client()\n                resp = c.get('/users/me')\n                assert resp.data == 'username=john'\n\n   .. versionadded:: 0.10\n\n.. data:: appcontext_popped\n\n   This signal is sent when an application context is popped. The sender\n   is the application. This usually falls in line with the\n   :data:`appcontext_tearing_down` signal.\n\n   .. versionadded:: 0.10\n\n.. data:: message_flashed\n\n   This signal is sent when the application is flashing a message. The\n   messages is sent as `message` keyword argument and the category as\n   `category`.\n\n   Example subscriber::\n\n        recorded = []\n        def record(sender, message, category, **extra):\n            recorded.append((message, category))\n\n        from flask import message_flashed\n        message_flashed.connect(record, app)\n\n   .. versionadded:: 0.10\n\n\nClass-Based Views\n-----------------\n\n.. versionadded:: 0.7\n\n.. currentmodule:: None\n\n.. autoclass:: flask.views.View\n   :members:\n\n.. autoclass:: flask.views.MethodView\n   :members:\n\n.. _url-route-registrations:\n\nURL Route Registrations\n-----------------------\n\nGenerally there are three ways to define rules for the routing system:\n\n1.  You can use the :meth:`flask.Flask.route` decorator.\n2.  You can use the :meth:`flask.Flask.add_url_rule` function.\n3.  You can directly access the underlying Werkzeug routing system\n    which is exposed as :attr:`flask.Flask.url_map`.\n\nVariable parts in the route can be specified with angular brackets\n(``/user/<username>``). By default a variable part in the URL accepts any\nstring without a slash however a different converter can be specified as\nwell by using ``<converter:name>``.\n\nVariable parts are passed to the view function as keyword arguments.\n\nThe following converters are available:\n\n=========== ===============================================\n`string`    accepts any text without a slash (the default)\n`int`       accepts integers\n`float`     like `int` but for floating point values\n`path`      like the default but also accepts slashes\n`any`       matches one of the items provided\n`uuid`      accepts UUID strings\n=========== ===============================================\n\nCustom converters can be defined using :attr:`flask.Flask.url_map`.\n\nHere are some examples::\n\n    @app.route('/')\n    def index():\n        pass\n\n    @app.route('/<username>')\n    def show_user(username):\n        pass\n\n    @app.route('/post/<int:post_id>')\n    def show_post(post_id):\n        pass\n\nAn important detail to keep in mind is how Flask deals with trailing\nslashes. The idea is to keep each URL unique so the following rules\napply:\n\n1. If a rule ends with a slash and is requested without a slash by the\n   user, the user is automatically redirected to the same page with a\n   trailing slash attached.\n2. If a rule does not end with a trailing slash and the user requests the\n   page with a trailing slash, a 404 not found is raised.\n\nThis is consistent with how web servers deal with static files. This\nalso makes it possible to use relative link targets safely.\n\nYou can also define multiple rules for the same function. They have to be\nunique however. Defaults can also be specified. Here for example is a\ndefinition for a URL that accepts an optional page::\n\n    @app.route('/users/', defaults={'page': 1})\n    @app.route('/users/page/<int:page>')\n    def show_users(page):\n        pass\n\nThis specifies that ``/users/`` will be the URL for page one and\n``/users/page/N`` will be the URL for page ``N``.\n\nIf a URL contains a default value, it will be redirected to its simpler\nform with a 301 redirect. In the above example, ``/users/page/1`` will\nbe redirected to ``/users/``. If your route handles ``GET`` and ``POST``\nrequests, make sure the default route only handles ``GET``, as redirects\ncan't preserve form data. ::\n\n   @app.route('/region/', defaults={'id': 1})\n   @app.route('/region/<int:id>', methods=['GET', 'POST'])\n   def region(id):\n      pass\n\nHere are the parameters that :meth:`~flask.Flask.route` and\n:meth:`~flask.Flask.add_url_rule` accept. The only difference is that\nwith the route parameter the view function is defined with the decorator\ninstead of the `view_func` parameter.\n\n=============== ==========================================================\n`rule`          the URL rule as string\n`endpoint`      the endpoint for the registered URL rule. Flask itself\n                assumes that the name of the view function is the name\n                of the endpoint if not explicitly stated.\n`view_func`     the function to call when serving a request to the\n                provided endpoint. If this is not provided one can\n                specify the function later by storing it in the\n                :attr:`~flask.Flask.view_functions` dictionary with the\n                endpoint as key.\n`defaults`      A dictionary with defaults for this rule. See the\n                example above for how defaults work.\n`subdomain`     specifies the rule for the subdomain in case subdomain\n                matching is in use. If not specified the default\n                subdomain is assumed.\n`**options`     the options to be forwarded to the underlying\n                :class:`~werkzeug.routing.Rule` object. A change to\n                Werkzeug is handling of method options. methods is a list\n                of methods this rule should be limited to (``GET``, ``POST``\n                etc.). By default a rule just listens for ``GET`` (and\n                implicitly ``HEAD``). Starting with Flask 0.6, ``OPTIONS`` is\n                implicitly added and handled by the standard request\n                handling. They have to be specified as keyword arguments.\n=============== ==========================================================\n\n\nView Function Options\n---------------------\n\nFor internal usage the view functions can have some attributes attached to\ncustomize behavior the view function would normally not have control over.\nThe following attributes can be provided optionally to either override\nsome defaults to :meth:`~flask.Flask.add_url_rule` or general behavior:\n\n-   `__name__`: The name of a function is by default used as endpoint. If\n    endpoint is provided explicitly this value is used. Additionally this\n    will be prefixed with the name of the blueprint by default which\n    cannot be customized from the function itself.\n\n-   `methods`: If methods are not provided when the URL rule is added,\n    Flask will look on the view function object itself if a `methods`\n    attribute exists. If it does, it will pull the information for the\n    methods from there.\n\n-   `provide_automatic_options`: if this attribute is set Flask will\n    either force enable or disable the automatic implementation of the\n    HTTP ``OPTIONS`` response. This can be useful when working with\n    decorators that want to customize the ``OPTIONS`` response on a per-view\n    basis.\n\n-   `required_methods`: if this attribute is set, Flask will always add\n    these methods when registering a URL rule even if the methods were\n    explicitly overridden in the ``route()`` call.\n\nFull example::\n\n    def index():\n        if request.method == 'OPTIONS':\n            # custom options handling here\n            ...\n        return 'Hello World!'\n    index.provide_automatic_options = False\n    index.methods = ['GET', 'OPTIONS']\n\n    app.add_url_rule('/', index)\n\n.. versionadded:: 0.8\n   The `provide_automatic_options` functionality was added.\n\nCommand Line Interface\n----------------------\n\n.. currentmodule:: flask.cli\n\n.. autoclass:: FlaskGroup\n   :members:\n\n.. autoclass:: AppGroup\n   :members:\n\n.. autoclass:: ScriptInfo\n   :members:\n\n.. autofunction:: load_dotenv\n\n.. autofunction:: with_appcontext\n\n.. autofunction:: pass_script_info\n\n   Marks a function so that an instance of :class:`ScriptInfo` is passed\n   as first argument to the click callback.\n\n.. autodata:: run_command\n\n.. autodata:: shell_command\n", "docs/appcontext.rst": "The App and Request Context\n===========================\n\nThe context keeps track of data and objects during a request, CLI command, or\nother activity. Rather than passing this data around to every function, the\n:data:`.current_app`, :data:`.g`, :data:`.request`, and :data:`.session` proxies\nare accessed instead.\n\nWhen handling a request, the context is referred to as the \"request context\"\nbecause it contains request data in addition to application data. Otherwise,\nsuch as during a CLI command, it is referred to as the \"app context\". During an\napp context, :data:`.current_app` and :data:`.g` are available, while during a\nrequest context :data:`.request` and :data:`.session` are also available.\n\n\nPurpose of the Context\n----------------------\n\nThe context and proxies help solve two development issues: circular imports, and\npassing around global data during a request.\n\nThe :class:`.Flask` application object has attributes, such as\n:attr:`~.Flask.config`, that are useful to access within views and other\nfunctions. However, importing the ``app`` instance within the modules in your\nproject is prone to circular import issues. When using the\n:doc:`app factory pattern </patterns/appfactories>` or writing reusable\n:doc:`blueprints </blueprints>` or :doc:`extensions </extensions>` there won't\nbe an ``app`` instance to import at all.\n\nWhen the application handles a request, it creates a :class:`.Request` object.\nBecause a *worker* handles only one request at a time, the request data can be\nconsidered global to that worker during that request. Passing it as an argument\nthrough every function during the request becomes verbose and redundant.\n\nFlask solves these issues with the *active context* pattern. Rather than\nimporting an ``app`` directly, or having to pass it and the request through to\nevery single function, you import and access the proxies, which point to the\ncurrently active application and request data. This is sometimes referred to\nas \"context local\" data.\n\n\nContext During Setup\n--------------------\n\nIf you try to access :data:`.current_app`, :data:`.g`, or anything that uses it,\noutside an app context, you'll get this error message:\n\n.. code-block:: pytb\n\n    RuntimeError: Working outside of application context.\n\n    Attempted to use functionality that expected a current application to be\n    set. To solve this, set up an app context using 'with app.app_context()'.\n    See the documentation on app context for more information.\n\nIf you see that error while configuring your application, such as when\ninitializing an extension, you can push a context manually since you have direct\naccess to the ``app``. Use :meth:`.Flask.app_context` in a ``with`` block.\n\n.. code-block:: python\n\n    def create_app():\n        app = Flask(__name__)\n\n        with app.app_context():\n            init_db()\n\n        return app\n\nIf you see that error somewhere else in your code not related to setting up the\napplication, it most likely indicates that you should move that code into a view\nfunction or CLI command.\n\n\nContext During Testing\n----------------------\n\nSee :doc:`/testing` for detailed information about managing the context during\ntests.\n\nIf you try to access :data:`.request`, :data:`.session`, or anything that uses\nit, outside a request context, you'll get this error message:\n\n.. code-block:: pytb\n\n    RuntimeError: Working outside of request context.\n\n    Attempted to use functionality that expected an active HTTP request. See the\n    documentation on request context for more information.\n\nThis will probably only happen during tests. If you see that error somewhere\nelse in your code not related to testing, it most likely indicates that you\nshould move that code into a view function.\n\nThe primary way to solve this is to use :meth:`.Flask.test_client` to simulate\na full request.\n\nIf you only want to unit test one function, rather than a full request, use\n:meth:`.Flask.test_request_context` in a ``with`` block.\n\n.. code-block:: python\n\n    def generate_report(year):\n        format = request.args.get(\"format\")\n        ...\n\n    with app.test_request_context(\n        \"/make_report/2017\", query_string={\"format\": \"short\"}\n    ):\n        generate_report()\n\n\n.. _context-visibility:\n\nVisibility of the Context\n-------------------------\n\nThe context will have the same lifetime as an activity, such as a request, CLI\ncommand, or ``with`` block. Various callbacks and signals registered with the\napp will be run during the context.\n\nWhen a Flask application handles a request, it pushes a requet context\nto set the active application and request data. When it handles a CLI command,\nit pushes an app context to set the active application. When the activity ends,\nit pops that context. Proxy objects like :data:`.request`, :data:`.session`,\n:data:`.g`, and :data:`.current_app`, are accessible while the context is pushed\nand active, and are not accessible after the context is popped.\n\nThe context is unique to each thread (or other worker type). The proxies cannot\nbe passed to another worker, which has a different context space and will not\nknow about the active context in the parent's space.\n\nBesides being scoped to each worker, the proxy object has a separate type and\nidentity than the proxied real object. In some cases you'll need access to the\nreal object, rather than the proxy. Use the\n:meth:`~.LocalProxy._get_current_object` method in those cases.\n\n.. code-block:: python\n\n    app = current_app._get_current_object()\n    my_signal.send(app)\n\n\nLifcycle of the Context\n-----------------------\n\nFlask dispatches a request in multiple stages which can affect the request,\nresponse, and how errors are handled. See :doc:`/lifecycle` for a list of all\nthe steps, callbacks, and signals during each request. The following are the\nsteps directly related to the context.\n\n-   The app context is pushed, the proxies are available.\n-   The :data:`.appcontext_pushed` signal is sent.\n-   The request is dispatched.\n-   Any :meth:`.Flask.teardown_request` decorated functions are called.\n-   The :data:`.request_tearing_down` signal is sent.\n-   Any :meth:`.Flask.teardown_appcontext` decorated functions are called.\n-   The :data:`.appcontext_tearing_down` signal is sent.\n-   The app context is popped, the proxies are no longer available.\n-   The :data:`.appcontext_popped` signal is sent.\n\nThe teardown callbacks are called by the context when it is popped. They are\ncalled even if there is an unhandled exception during dispatch. They may be\ncalled multiple times in some test scenarios. This means there is no guarantee\nthat any other parts of the request dispatch have run. Be sure to write these\nfunctions in a way that does not depend on other callbacks and will not fail.\n\n\nHow the Context Works\n---------------------\n\nContext locals are implemented using Python's :mod:`contextvars` and Werkzeug's\n:class:`~werkzeug.local.LocalProxy`. Python's contextvars are a low level\nstructure to manage data local to a thread or coroutine. ``LocalProxy`` wraps\nthe contextvar so that access to any attributes and methods is forwarded to the\nobject stored in the contextvar.\n\nThe context is tracked like a stack, with the active context at the top of the\nstack. Flask manages pushing and popping contexts during requests, CLI commands,\ntesting, ``with`` blocks, etc. The proxies access attributes on the active\ncontext.\n\nBecause it is a stack, other contexts may be pushed to change the proxies during\nan already active context. This is not a common pattern, but can be used in\nadvanced use cases. For example, a Flask application can be used as WSGI\nmiddleware, calling another wrapped Flask app from a view.\n", "docs/async-await.rst": ".. _async_await:\n\nUsing ``async`` and ``await``\n=============================\n\n.. versionadded:: 2.0\n\nRoutes, error handlers, before request, after request, and teardown\nfunctions can all be coroutine functions if Flask is installed with the\n``async`` extra (``pip install flask[async]``). This allows views to be\ndefined with ``async def`` and use ``await``.\n\n.. code-block:: python\n\n    @app.route(\"/get-data\")\n    async def get_data():\n        data = await async_db_query(...)\n        return jsonify(data)\n\nPluggable class-based views also support handlers that are implemented as\ncoroutines. This applies to the :meth:`~flask.views.View.dispatch_request`\nmethod in views that inherit from the :class:`flask.views.View` class, as\nwell as all the HTTP method handlers in views that inherit from the\n:class:`flask.views.MethodView` class.\n\n.. admonition:: Using ``async`` with greenlet\n\n    When using gevent or eventlet to serve an application or patch the\n    runtime, greenlet>=1.0 is required. When using PyPy, PyPy>=7.3.7 is\n    required.\n\n\nPerformance\n-----------\n\nAsync functions require an event loop to run. Flask, as a WSGI\napplication, uses one worker to handle one request/response cycle.\nWhen a request comes in to an async view, Flask will start an event loop\nin a thread, run the view function there, then return the result.\n\nEach request still ties up one worker, even for async views. The upside\nis that you can run async code within a view, for example to make\nmultiple concurrent database queries, HTTP requests to an external API,\netc. However, the number of requests your application can handle at one\ntime will remain the same.\n\n**Async is not inherently faster than sync code.** Async is beneficial\nwhen performing concurrent IO-bound tasks, but will probably not improve\nCPU-bound tasks. Traditional Flask views will still be appropriate for\nmost use cases, but Flask's async support enables writing and using\ncode that wasn't possible natively before.\n\n\nBackground tasks\n----------------\n\nAsync functions will run in an event loop until they complete, at\nwhich stage the event loop will stop. This means any additional\nspawned tasks that haven't completed when the async function completes\nwill be cancelled. Therefore you cannot spawn background tasks, for\nexample via ``asyncio.create_task``.\n\nIf you wish to use background tasks it is best to use a task queue to\ntrigger background work, rather than spawn tasks in a view\nfunction. With that in mind you can spawn asyncio tasks by serving\nFlask with an ASGI server and utilising the asgiref WsgiToAsgi adapter\nas described in :doc:`deploying/asgi`. This works as the adapter creates\nan event loop that runs continually.\n\n\nWhen to use Quart instead\n-------------------------\n\nFlask's async support is less performant than async-first frameworks due\nto the way it is implemented. If you have a mainly async codebase it\nwould make sense to consider `Quart`_. Quart is a reimplementation of\nFlask based on the `ASGI`_ standard instead of WSGI. This allows it to\nhandle many concurrent requests, long running requests, and websockets\nwithout requiring multiple worker processes or threads.\n\nIt has also already been possible to run Flask with Gevent or Eventlet\nto get many of the benefits of async request handling. These libraries\npatch low-level Python functions to accomplish this, whereas ``async``/\n``await`` and ASGI use standard, modern Python capabilities. Deciding\nwhether you should use Flask, Quart, or something else is ultimately up\nto understanding the specific needs of your project.\n\n.. _Quart: https://github.com/pallets/quart\n.. _ASGI: https://asgi.readthedocs.io/en/latest/\n\n\nExtensions\n----------\n\nFlask extensions predating Flask's async support do not expect async views.\nIf they provide decorators to add functionality to views, those will probably\nnot work with async views because they will not await the function or be\nawaitable. Other functions they provide will not be awaitable either and\nwill probably be blocking if called within an async view.\n\nExtension authors can support async functions by utilising the\n:meth:`flask.Flask.ensure_sync` method. For example, if the extension\nprovides a view function decorator add ``ensure_sync`` before calling\nthe decorated function,\n\n.. code-block:: python\n\n    def extension(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            ...  # Extension logic\n            return current_app.ensure_sync(func)(*args, **kwargs)\n\n        return wrapper\n\nCheck the changelog of the extension you want to use to see if they've\nimplemented async support, or make a feature request or PR to them.\n\n\nOther event loops\n-----------------\n\nAt the moment Flask only supports :mod:`asyncio`. It's possible to\noverride :meth:`flask.Flask.ensure_sync` to change how async functions\nare wrapped to use a different library.\n", "docs/blueprints.rst": "Modular Applications with Blueprints\n====================================\n\n.. currentmodule:: flask\n\n.. versionadded:: 0.7\n\nFlask uses a concept of *blueprints* for making application components and\nsupporting common patterns within an application or across applications.\nBlueprints can greatly simplify how large applications work and provide a\ncentral means for Flask extensions to register operations on applications.\nA :class:`Blueprint` object works similarly to a :class:`Flask`\napplication object, but it is not actually an application.  Rather it is a\n*blueprint* of how to construct or extend an application.\n\nWhy Blueprints?\n---------------\n\nBlueprints in Flask are intended for these cases:\n\n* Factor an application into a set of blueprints.  This is ideal for\n  larger applications; a project could instantiate an application object,\n  initialize several extensions, and register a collection of blueprints.\n* Register a blueprint on an application at a URL prefix and/or subdomain.\n  Parameters in the URL prefix/subdomain become common view arguments\n  (with defaults) across all view functions in the blueprint.\n* Register a blueprint multiple times on an application with different URL\n  rules.\n* Provide template filters, static files, templates, and other utilities\n  through blueprints.  A blueprint does not have to implement applications\n  or view functions.\n* Register a blueprint on an application for any of these cases when\n  initializing a Flask extension.\n\nA blueprint in Flask is not a pluggable app because it is not actually an\napplication -- it's a set of operations which can be registered on an\napplication, even multiple times.  Why not have multiple application\nobjects?  You can do that (see :doc:`/patterns/appdispatch`), but your\napplications will have separate configs and will be managed at the WSGI\nlayer.\n\nBlueprints instead provide separation at the Flask level, share\napplication config, and can change an application object as necessary with\nbeing registered. The downside is that you cannot unregister a blueprint\nonce an application was created without having to destroy the whole\napplication object.\n\nThe Concept of Blueprints\n-------------------------\n\nThe basic concept of blueprints is that they record operations to execute\nwhen registered on an application.  Flask associates view functions with\nblueprints when dispatching requests and generating URLs from one endpoint\nto another.\n\nMy First Blueprint\n------------------\n\nThis is what a very basic blueprint looks like.  In this case we want to\nimplement a blueprint that does simple rendering of static templates::\n\n    from flask import Blueprint, render_template, abort\n    from jinja2 import TemplateNotFound\n\n    simple_page = Blueprint('simple_page', __name__,\n                            template_folder='templates')\n\n    @simple_page.route('/', defaults={'page': 'index'})\n    @simple_page.route('/<page>')\n    def show(page):\n        try:\n            return render_template(f'pages/{page}.html')\n        except TemplateNotFound:\n            abort(404)\n\nWhen you bind a function with the help of the ``@simple_page.route``\ndecorator, the blueprint will record the intention of registering the\nfunction ``show`` on the application when it's later registered.\nAdditionally it will prefix the endpoint of the function with the\nname of the blueprint which was given to the :class:`Blueprint`\nconstructor (in this case also ``simple_page``). The blueprint's name\ndoes not modify the URL, only the endpoint.\n\nRegistering Blueprints\n----------------------\n\nSo how do you register that blueprint?  Like this::\n\n    from flask import Flask\n    from yourapplication.simple_page import simple_page\n\n    app = Flask(__name__)\n    app.register_blueprint(simple_page)\n\nIf you check the rules registered on the application, you will find\nthese::\n\n    >>> app.url_map\n    Map([<Rule '/static/<filename>' (HEAD, OPTIONS, GET) -> static>,\n     <Rule '/<page>' (HEAD, OPTIONS, GET) -> simple_page.show>,\n     <Rule '/' (HEAD, OPTIONS, GET) -> simple_page.show>])\n\nThe first one is obviously from the application itself for the static\nfiles.  The other two are for the `show` function of the ``simple_page``\nblueprint.  As you can see, they are also prefixed with the name of the\nblueprint and separated by a dot (``.``).\n\nBlueprints however can also be mounted at different locations::\n\n    app.register_blueprint(simple_page, url_prefix='/pages')\n\nAnd sure enough, these are the generated rules::\n\n    >>> app.url_map\n    Map([<Rule '/static/<filename>' (HEAD, OPTIONS, GET) -> static>,\n     <Rule '/pages/<page>' (HEAD, OPTIONS, GET) -> simple_page.show>,\n     <Rule '/pages/' (HEAD, OPTIONS, GET) -> simple_page.show>])\n\nOn top of that you can register blueprints multiple times though not every\nblueprint might respond properly to that.  In fact it depends on how the\nblueprint is implemented if it can be mounted more than once.\n\nNesting Blueprints\n------------------\n\nIt is possible to register a blueprint on another blueprint.\n\n.. code-block:: python\n\n    parent = Blueprint('parent', __name__, url_prefix='/parent')\n    child = Blueprint('child', __name__, url_prefix='/child')\n    parent.register_blueprint(child)\n    app.register_blueprint(parent)\n\nThe child blueprint will gain the parent's name as a prefix to its\nname, and child URLs will be prefixed with the parent's URL prefix.\n\n.. code-block:: python\n\n    url_for('parent.child.create')\n    /parent/child/create\n\nIn addition a child blueprint's will gain their parent's subdomain,\nwith their subdomain as prefix if present i.e.\n\n.. code-block:: python\n\n    parent = Blueprint('parent', __name__, subdomain='parent')\n    child = Blueprint('child', __name__, subdomain='child')\n    parent.register_blueprint(child)\n    app.register_blueprint(parent)\n\n    url_for('parent.child.create', _external=True)\n    \"child.parent.domain.tld\"\n\nBlueprint-specific before request functions, etc. registered with the\nparent will trigger for the child. If a child does not have an error\nhandler that can handle a given exception, the parent's will be tried.\n\n\nBlueprint Resources\n-------------------\n\nBlueprints can provide resources as well.  Sometimes you might want to\nintroduce a blueprint only for the resources it provides.\n\nBlueprint Resource Folder\n`````````````````````````\n\nLike for regular applications, blueprints are considered to be contained\nin a folder.  While multiple blueprints can originate from the same folder,\nit does not have to be the case and it's usually not recommended.\n\nThe folder is inferred from the second argument to :class:`Blueprint` which\nis usually `__name__`.  This argument specifies what logical Python\nmodule or package corresponds to the blueprint.  If it points to an actual\nPython package that package (which is a folder on the filesystem) is the\nresource folder.  If it's a module, the package the module is contained in\nwill be the resource folder.  You can access the\n:attr:`Blueprint.root_path` property to see what the resource folder is::\n\n    >>> simple_page.root_path\n    '/Users/username/TestProject/yourapplication'\n\nTo quickly open sources from this folder you can use the\n:meth:`~Blueprint.open_resource` function::\n\n    with simple_page.open_resource('static/style.css') as f:\n        code = f.read()\n\nStatic Files\n````````````\n\nA blueprint can expose a folder with static files by providing the path\nto the folder on the filesystem with the ``static_folder`` argument.\nIt is either an absolute path or relative to the blueprint's location::\n\n    admin = Blueprint('admin', __name__, static_folder='static')\n\nBy default the rightmost part of the path is where it is exposed on the\nweb. This can be changed with the ``static_url_path`` argument. Because the\nfolder is called ``static`` here it will be available at the\n``url_prefix`` of the blueprint + ``/static``. If the blueprint\nhas the prefix ``/admin``, the static URL will be ``/admin/static``.\n\nThe endpoint is named ``blueprint_name.static``. You can generate URLs\nto it with :func:`url_for` like you would with the static folder of the\napplication::\n\n    url_for('admin.static', filename='style.css')\n\nHowever, if the blueprint does not have a ``url_prefix``, it is not\npossible to access the blueprint's static folder. This is because the\nURL would be ``/static`` in this case, and the application's ``/static``\nroute takes precedence. Unlike template folders, blueprint static\nfolders are not searched if the file does not exist in the application\nstatic folder.\n\nTemplates\n`````````\n\nIf you want the blueprint to expose templates you can do that by providing\nthe `template_folder` parameter to the :class:`Blueprint` constructor::\n\n    admin = Blueprint('admin', __name__, template_folder='templates')\n\nFor static files, the path can be absolute or relative to the blueprint\nresource folder.\n\nThe template folder is added to the search path of templates but with a lower\npriority than the actual application's template folder. That way you can\neasily override templates that a blueprint provides in the actual application.\nThis also means that if you don't want a blueprint template to be accidentally\noverridden, make sure that no other blueprint or actual application template\nhas the same relative path. When multiple blueprints provide the same relative\ntemplate path the first blueprint registered takes precedence over the others.\n\n\nSo if you have a blueprint in the folder ``yourapplication/admin`` and you\nwant to render the template ``'admin/index.html'`` and you have provided\n``templates`` as a `template_folder` you will have to create a file like\nthis: :file:`yourapplication/admin/templates/admin/index.html`. The reason\nfor the extra ``admin`` folder is to avoid getting our template overridden\nby a template named ``index.html`` in the actual application template\nfolder.\n\nTo further reiterate this: if you have a blueprint named ``admin`` and you\nwant to render a template called :file:`index.html` which is specific to this\nblueprint, the best idea is to lay out your templates like this::\n\n    yourpackage/\n        blueprints/\n            admin/\n                templates/\n                    admin/\n                        index.html\n                __init__.py\n\nAnd then when you want to render the template, use :file:`admin/index.html` as\nthe name to look up the template by.  If you encounter problems loading\nthe correct templates enable the ``EXPLAIN_TEMPLATE_LOADING`` config\nvariable which will instruct Flask to print out the steps it goes through\nto locate templates on every ``render_template`` call.\n\nBuilding URLs\n-------------\n\nIf you want to link from one page to another you can use the\n:func:`url_for` function just like you normally would do just that you\nprefix the URL endpoint with the name of the blueprint and a dot (``.``)::\n\n    url_for('admin.index')\n\nAdditionally if you are in a view function of a blueprint or a rendered\ntemplate and you want to link to another endpoint of the same blueprint,\nyou can use relative redirects by prefixing the endpoint with a dot only::\n\n    url_for('.index')\n\nThis will link to ``admin.index`` for instance in case the current request\nwas dispatched to any other admin blueprint endpoint.\n\n\nBlueprint Error Handlers\n------------------------\n\nBlueprints support the ``errorhandler`` decorator just like the :class:`Flask`\napplication object, so it is easy to make Blueprint-specific custom error\npages.\n\nHere is an example for a \"404 Page Not Found\" exception::\n\n    @simple_page.errorhandler(404)\n    def page_not_found(e):\n        return render_template('pages/404.html')\n\nMost errorhandlers will simply work as expected; however, there is a caveat\nconcerning handlers for 404 and 405 exceptions.  These errorhandlers are only\ninvoked from an appropriate ``raise`` statement or a call to ``abort`` in another\nof the blueprint's view functions; they are not invoked by, e.g., an invalid URL\naccess.  This is because the blueprint does not \"own\" a certain URL space, so\nthe application instance has no way of knowing which blueprint error handler it\nshould run if given an invalid URL.  If you would like to execute different\nhandling strategies for these errors based on URL prefixes, they may be defined\nat the application level using the ``request`` proxy object::\n\n    @app.errorhandler(404)\n    @app.errorhandler(405)\n    def _handle_api_error(ex):\n        if request.path.startswith('/api/'):\n            return jsonify(error=str(ex)), ex.code\n        else:\n            return ex\n\nSee :doc:`/errorhandling`.\n", "docs/changes.rst": "Changes\n=======\n\n.. include:: ../CHANGES.rst\n", "docs/cli.rst": ".. currentmodule:: flask\n\nCommand Line Interface\n======================\n\nInstalling Flask installs the ``flask`` script, a `Click`_ command line\ninterface, in your virtualenv. Executed from the terminal, this script gives\naccess to built-in, extension, and application-defined commands. The ``--help``\noption will give more information about any commands and options.\n\n.. _Click: https://click.palletsprojects.com/\n\n\nApplication Discovery\n---------------------\n\nThe ``flask`` command is installed by Flask, not your application; it must be\ntold where to find your application in order to use it. The ``--app``\noption is used to specify how to load the application.\n\nWhile ``--app`` supports a variety of options for specifying your\napplication, most use cases should be simple. Here are the typical values:\n\n(nothing)\n    The name \"app\" or \"wsgi\" is imported (as a \".py\" file, or package),\n    automatically detecting an app (``app`` or ``application``) or\n    factory (``create_app`` or ``make_app``).\n\n``--app hello``\n    The given name is imported, automatically detecting an app (``app``\n    or ``application``) or factory (``create_app`` or ``make_app``).\n\n----\n\n``--app`` has three parts: an optional path that sets the current working\ndirectory, a Python file or dotted import path, and an optional variable\nname of the instance or factory. If the name is a factory, it can optionally\nbe followed by arguments in parentheses. The following values demonstrate these\nparts:\n\n``--app src/hello``\n    Sets the current working directory to ``src`` then imports ``hello``.\n\n``--app hello.web``\n    Imports the path ``hello.web``.\n\n``--app hello:app2``\n    Uses the ``app2`` Flask instance in ``hello``.\n\n``--app 'hello:create_app(\"dev\")'``\n    The ``create_app`` factory in ``hello`` is called with the string ``'dev'``\n    as the argument.\n\nIf ``--app`` is not set, the command will try to import \"app\" or\n\"wsgi\" (as a \".py\" file, or package) and try to detect an application\ninstance or factory.\n\nWithin the given import, the command looks for an application instance named\n``app`` or ``application``, then any application instance. If no instance is\nfound, the command looks for a factory function named ``create_app`` or\n``make_app`` that returns an instance.\n\nIf parentheses follow the factory name, their contents are parsed as\nPython literals and passed as arguments and keyword arguments to the\nfunction. This means that strings must still be in quotes.\n\n\nRun the Development Server\n--------------------------\n\nThe :func:`run <cli.run_command>` command will start the development server. It\nreplaces the :meth:`Flask.run` method in most cases. ::\n\n    $ flask --app hello run\n     * Serving Flask app \"hello\"\n     * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n\n.. warning:: Do not use this command to run your application in production.\n    Only use the development server during development. The development server\n    is provided for convenience, but is not designed to be particularly secure,\n    stable, or efficient. See :doc:`/deploying/index` for how to run in production.\n\nIf another program is already using port 5000, you'll see\n``OSError: [Errno 98]`` or ``OSError: [WinError 10013]`` when the\nserver tries to start. See :ref:`address-already-in-use` for how to\nhandle that.\n\n\nDebug Mode\n~~~~~~~~~~\n\nIn debug mode, the ``flask run`` command will enable the interactive debugger and the\nreloader by default, and make errors easier to see and debug. To enable debug mode, use\nthe ``--debug`` option.\n\n.. code-block:: console\n\n     $ flask --app hello run --debug\n      * Serving Flask app \"hello\"\n      * Debug mode: on\n      * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n      * Restarting with inotify reloader\n      * Debugger is active!\n      * Debugger PIN: 223-456-919\n\nThe ``--debug`` option can also be passed to the top level ``flask`` command to enable\ndebug mode for any command. The following two ``run`` calls are equivalent.\n\n.. code-block:: console\n\n    $ flask --app hello --debug run\n    $ flask --app hello run --debug\n\n\nWatch and Ignore Files with the Reloader\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWhen using debug mode, the reloader will trigger whenever your Python code or imported\nmodules change. The reloader can watch additional files with the ``--extra-files``\noption. Multiple paths are separated with ``:``, or ``;`` on Windows.\n\n.. code-block:: text\n\n    $ flask run --extra-files file1:dirA/file2:dirB/\n     * Running on http://127.0.0.1:8000/\n     * Detected change in '/path/to/file1', reloading\n\nThe reloader can also ignore files using :mod:`fnmatch` patterns with the\n``--exclude-patterns`` option. Multiple patterns are separated with ``:``, or ``;`` on\nWindows.\n\n\nOpen a Shell\n------------\n\nTo explore the data in your application, you can start an interactive Python\nshell with the :func:`shell <cli.shell_command>` command. An application\ncontext will be active, and the app instance will be imported. ::\n\n    $ flask shell\n    Python 3.10.0 (default, Oct 27 2021, 06:59:51) [GCC 11.1.0] on linux\n    App: example [production]\n    Instance: /home/david/Projects/pallets/flask/instance\n    >>>\n\nUse :meth:`~Flask.shell_context_processor` to add other automatic imports.\n\n\n.. _dotenv:\n\nEnvironment Variables From dotenv\n---------------------------------\n\nThe ``flask`` command supports setting any option for any command with\nenvironment variables. The variables are named like ``FLASK_OPTION`` or\n``FLASK_COMMAND_OPTION``, for example ``FLASK_APP`` or\n``FLASK_RUN_PORT``.\n\nRather than passing options every time you run a command, or environment\nvariables every time you open a new terminal, you can use Flask's dotenv\nsupport to set environment variables automatically.\n\nIf `python-dotenv`_ is installed, running the ``flask`` command will set\nenvironment variables defined in the files ``.env`` and ``.flaskenv``.\nYou can also specify an extra file to load with the ``--env-file``\noption. Dotenv files can be used to avoid having to set ``--app`` or\n``FLASK_APP`` manually, and to set configuration using environment\nvariables similar to how some deployment services work.\n\nVariables set on the command line are used over those set in :file:`.env`,\nwhich are used over those set in :file:`.flaskenv`. :file:`.flaskenv` should be\nused for public variables, such as ``FLASK_APP``, while :file:`.env` should not\nbe committed to your repository so that it can set private variables.\n\nDirectories are scanned upwards from the directory you call ``flask``\nfrom to locate the files.\n\nThe files are only loaded by the ``flask`` command or calling\n:meth:`~Flask.run`. If you would like to load these files when running in\nproduction, you should call :func:`~cli.load_dotenv` manually.\n\n.. _python-dotenv: https://github.com/theskumar/python-dotenv#readme\n\n\nSetting Command Options\n~~~~~~~~~~~~~~~~~~~~~~~\n\nClick is configured to load default values for command options from\nenvironment variables. The variables use the pattern\n``FLASK_COMMAND_OPTION``. For example, to set the port for the run\ncommand, instead of ``flask run --port 8000``:\n\n.. tabs::\n\n   .. group-tab:: Bash\n\n      .. code-block:: text\n\n         $ export FLASK_RUN_PORT=8000\n         $ flask run\n          * Running on http://127.0.0.1:8000/\n\n   .. group-tab:: Fish\n\n      .. code-block:: text\n\n         $ set -x FLASK_RUN_PORT 8000\n         $ flask run\n          * Running on http://127.0.0.1:8000/\n\n   .. group-tab:: CMD\n\n      .. code-block:: text\n\n         > set FLASK_RUN_PORT=8000\n         > flask run\n          * Running on http://127.0.0.1:8000/\n\n   .. group-tab:: Powershell\n\n      .. code-block:: text\n\n         > $env:FLASK_RUN_PORT = 8000\n         > flask run\n          * Running on http://127.0.0.1:8000/\n\nThese can be added to the ``.flaskenv`` file just like ``FLASK_APP`` to\ncontrol default command options.\n\n\nDisable dotenv\n~~~~~~~~~~~~~~\n\nThe ``flask`` command will show a message if it detects dotenv files but\npython-dotenv is not installed.\n\n.. code-block:: bash\n\n    $ flask run\n     * Tip: There are .env files present. Do \"pip install python-dotenv\" to use them.\n\nYou can tell Flask not to load dotenv files even when python-dotenv is\ninstalled by setting the ``FLASK_SKIP_DOTENV`` environment variable.\nThis can be useful if you want to load them manually, or if you're using\na project runner that loads them already. Keep in mind that the\nenvironment variables must be set before the app loads or it won't\nconfigure as expected.\n\n.. tabs::\n\n   .. group-tab:: Bash\n\n      .. code-block:: text\n\n         $ export FLASK_SKIP_DOTENV=1\n         $ flask run\n\n   .. group-tab:: Fish\n\n      .. code-block:: text\n\n         $ set -x FLASK_SKIP_DOTENV 1\n         $ flask run\n\n   .. group-tab:: CMD\n\n      .. code-block:: text\n\n         > set FLASK_SKIP_DOTENV=1\n         > flask run\n\n   .. group-tab:: Powershell\n\n      .. code-block:: text\n\n         > $env:FLASK_SKIP_DOTENV = 1\n         > flask run\n\n\nEnvironment Variables From virtualenv\n-------------------------------------\n\nIf you do not want to install dotenv support, you can still set environment\nvariables by adding them to the end of the virtualenv's :file:`activate`\nscript. Activating the virtualenv will set the variables.\n\n.. tabs::\n\n   .. group-tab:: Bash\n\n      Unix Bash, :file:`.venv/bin/activate`::\n\n          $ export FLASK_APP=hello\n\n   .. group-tab:: Fish\n\n      Fish, :file:`.venv/bin/activate.fish`::\n\n          $ set -x FLASK_APP hello\n\n   .. group-tab:: CMD\n\n      Windows CMD, :file:`.venv\\\\Scripts\\\\activate.bat`::\n\n          > set FLASK_APP=hello\n\n   .. group-tab:: Powershell\n\n      Windows Powershell, :file:`.venv\\\\Scripts\\\\activate.ps1`::\n\n          > $env:FLASK_APP = \"hello\"\n\nIt is preferred to use dotenv support over this, since :file:`.flaskenv` can be\ncommitted to the repository so that it works automatically wherever the project\nis checked out.\n\n\nCustom Commands\n---------------\n\nThe ``flask`` command is implemented using `Click`_. See that project's\ndocumentation for full information about writing commands.\n\nThis example adds the command ``create-user`` that takes the argument\n``name``. ::\n\n    import click\n    from flask import Flask\n\n    app = Flask(__name__)\n\n    @app.cli.command(\"create-user\")\n    @click.argument(\"name\")\n    def create_user(name):\n        ...\n\n::\n\n    $ flask create-user admin\n\nThis example adds the same command, but as ``user create``, a command in a\ngroup. This is useful if you want to organize multiple related commands. ::\n\n    import click\n    from flask import Flask\n    from flask.cli import AppGroup\n\n    app = Flask(__name__)\n    user_cli = AppGroup('user')\n\n    @user_cli.command('create')\n    @click.argument('name')\n    def create_user(name):\n        ...\n\n    app.cli.add_command(user_cli)\n\n::\n\n    $ flask user create demo\n\nSee :ref:`testing-cli` for an overview of how to test your custom\ncommands.\n\n\nRegistering Commands with Blueprints\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf your application uses blueprints, you can optionally register CLI\ncommands directly onto them. When your blueprint is registered onto your\napplication, the associated commands will be available to the ``flask``\ncommand. By default, those commands will be nested in a group matching\nthe name of the blueprint.\n\n.. code-block:: python\n\n    from flask import Blueprint\n\n    bp = Blueprint('students', __name__)\n\n    @bp.cli.command('create')\n    @click.argument('name')\n    def create(name):\n        ...\n\n    app.register_blueprint(bp)\n\n.. code-block:: text\n\n    $ flask students create alice\n\nYou can alter the group name by specifying the ``cli_group`` parameter\nwhen creating the :class:`Blueprint` object, or later with\n:meth:`app.register_blueprint(bp, cli_group='...') <Flask.register_blueprint>`.\nThe following are equivalent:\n\n.. code-block:: python\n\n    bp = Blueprint('students', __name__, cli_group='other')\n    # or\n    app.register_blueprint(bp, cli_group='other')\n\n.. code-block:: text\n\n    $ flask other create alice\n\nSpecifying ``cli_group=None`` will remove the nesting and merge the\ncommands directly to the application's level:\n\n.. code-block:: python\n\n    bp = Blueprint('students', __name__, cli_group=None)\n    # or\n    app.register_blueprint(bp, cli_group=None)\n\n.. code-block:: text\n\n    $ flask create alice\n\n\nApplication Context\n~~~~~~~~~~~~~~~~~~~\n\nCommands added using the Flask app's :attr:`~Flask.cli` or\n:class:`~flask.cli.FlaskGroup` :meth:`~cli.AppGroup.command` decorator\nwill be executed with an application context pushed, so your custom\ncommands and parameters have access to the app and its configuration. The\n:func:`~cli.with_appcontext` decorator can be used to get the same\nbehavior, but is not needed in most cases.\n\n.. code-block:: python\n\n    import click\n    from flask.cli import with_appcontext\n\n    @click.command()\n    @with_appcontext\n    def do_work():\n        ...\n\n    app.cli.add_command(do_work)\n\n\nPlugins\n-------\n\nFlask will automatically load commands specified in the ``flask.commands``\n`entry point`_. This is useful for extensions that want to add commands when\nthey are installed. Entry points are specified in :file:`pyproject.toml`:\n\n.. code-block:: toml\n\n    [project.entry-points.\"flask.commands\"]\n    my-command = \"my_extension.commands:cli\"\n\n.. _entry point: https://packaging.python.org/tutorials/packaging-projects/#entry-points\n\nInside :file:`my_extension/commands.py` you can then export a Click\nobject::\n\n    import click\n\n    @click.command()\n    def cli():\n        ...\n\nOnce that package is installed in the same virtualenv as your Flask project,\nyou can run ``flask my-command`` to invoke the command.\n\n\n.. _custom-scripts:\n\nCustom Scripts\n--------------\n\nWhen you are using the app factory pattern, it may be more convenient to define\nyour own Click script. Instead of using ``--app`` and letting Flask load\nyour application, you can create your own Click object and export it as a\n`console script`_ entry point.\n\nCreate an instance of :class:`~cli.FlaskGroup` and pass it the factory::\n\n    import click\n    from flask import Flask\n    from flask.cli import FlaskGroup\n\n    def create_app():\n        app = Flask('wiki')\n        # other setup\n        return app\n\n    @click.group(cls=FlaskGroup, create_app=create_app)\n    def cli():\n        \"\"\"Management script for the Wiki application.\"\"\"\n\nDefine the entry point in :file:`pyproject.toml`:\n\n.. code-block:: toml\n\n    [project.scripts]\n    wiki = \"wiki:cli\"\n\nInstall the application in the virtualenv in editable mode and the custom\nscript is available. Note that you don't need to set ``--app``. ::\n\n    $ pip install -e .\n    $ wiki run\n\n.. admonition:: Errors in Custom Scripts\n\n    When using a custom script, if you introduce an error in your\n    module-level code, the reloader will fail because it can no longer\n    load the entry point.\n\n    The ``flask`` command, being separate from your code, does not have\n    this issue and is recommended in most cases.\n\n.. _console script: https://packaging.python.org/tutorials/packaging-projects/#console-scripts\n\n\nPyCharm Integration\n-------------------\n\nPyCharm Professional provides a special Flask run configuration to run the development\nserver. For the Community Edition, and for other commands besides ``run``, you need to\ncreate a custom run configuration. These instructions should be similar for any other\nIDE you use.\n\nIn PyCharm, with your project open, click on *Run* from the menu bar and go to *Edit\nConfigurations*. You'll see a screen similar to this:\n\n.. image:: _static/pycharm-run-config.png\n    :align: center\n    :class: screenshot\n    :alt: Screenshot of PyCharm run configuration.\n\nOnce you create a configuration for the ``flask run``, you can copy and change it to\ncall any other command.\n\nClick the *+ (Add New Configuration)* button and select *Python*. Give the configuration\na name such as \"flask run\".\n\nClick the *Script path* dropdown and change it to *Module name*, then input ``flask``.\n\nThe *Parameters* field is set to the CLI command to execute along with any arguments.\nThis example uses ``--app hello run --debug``, which will run the development server in\ndebug mode. ``--app hello`` should be the import or file with your Flask app.\n\nIf you installed your project as a package in your virtualenv, you may uncheck the\n*PYTHONPATH* options. This will more accurately match how you deploy later.\n\nClick *OK* to save and close the configuration. Select the configuration in the main\nPyCharm window and click the play button next to it to run the server.\n\nNow that you have a configuration for ``flask run``, you can copy that configuration and\nchange the *Parameters* argument to run a different CLI command.\n", "docs/config.rst": "Configuration Handling\n======================\n\nApplications need some kind of configuration.  There are different settings\nyou might want to change depending on the application environment like\ntoggling the debug mode, setting the secret key, and other such\nenvironment-specific things.\n\nThe way Flask is designed usually requires the configuration to be\navailable when the application starts up.  You can hard code the\nconfiguration in the code, which for many small applications is not\nactually that bad, but there are better ways.\n\nIndependent of how you load your config, there is a config object\navailable which holds the loaded configuration values:\nThe :attr:`~flask.Flask.config` attribute of the :class:`~flask.Flask`\nobject.  This is the place where Flask itself puts certain configuration\nvalues and also where extensions can put their configuration values.  But\nthis is also where you can have your own configuration.\n\n\nConfiguration Basics\n--------------------\n\nThe :attr:`~flask.Flask.config` is actually a subclass of a dictionary and\ncan be modified just like any dictionary::\n\n    app = Flask(__name__)\n    app.config['TESTING'] = True\n\nCertain configuration values are also forwarded to the\n:attr:`~flask.Flask` object so you can read and write them from there::\n\n    app.testing = True\n\nTo update multiple keys at once you can use the :meth:`dict.update`\nmethod::\n\n    app.config.update(\n        TESTING=True,\n        SECRET_KEY='192b9bdd22ab9ed4d12e236c78afcb9a393ec15f71bbf5dc987d54727823bcbf'\n    )\n\n\nDebug Mode\n----------\n\nThe :data:`DEBUG` config value is special because it may behave inconsistently if\nchanged after the app has begun setting up. In order to set debug mode reliably, use the\n``--debug`` option on the ``flask`` or ``flask run`` command. ``flask run`` will use the\ninteractive debugger and reloader by default in debug mode.\n\n.. code-block:: text\n\n    $ flask --app hello run --debug\n\nUsing the option is recommended. While it is possible to set :data:`DEBUG` in your\nconfig or code, this is strongly discouraged. It can't be read early by the\n``flask run`` command, and some systems or extensions may have already configured\nthemselves based on a previous value.\n\n\nBuiltin Configuration Values\n----------------------------\n\nThe following configuration values are used internally by Flask:\n\n.. py:data:: DEBUG\n\n    Whether debug mode is enabled. When using ``flask run`` to start the development\n    server, an interactive debugger will be shown for unhandled exceptions, and the\n    server will be reloaded when code changes. The :attr:`~flask.Flask.debug` attribute\n    maps to this config key. This is set with the ``FLASK_DEBUG`` environment variable.\n    It may not behave as expected if set in code.\n\n    **Do not enable debug mode when deploying in production.**\n\n    Default: ``False``\n\n.. py:data:: TESTING\n\n    Enable testing mode. Exceptions are propagated rather than handled by the\n    the app's error handlers. Extensions may also change their behavior to\n    facilitate easier testing. You should enable this in your own tests.\n\n    Default: ``False``\n\n.. py:data:: PROPAGATE_EXCEPTIONS\n\n    Exceptions are re-raised rather than being handled by the app's error\n    handlers. If not set, this is implicitly true if ``TESTING`` or ``DEBUG``\n    is enabled.\n\n    Default: ``None``\n\n.. py:data:: TRAP_HTTP_EXCEPTIONS\n\n    If there is no handler for an ``HTTPException``-type exception, re-raise it\n    to be handled by the interactive debugger instead of returning it as a\n    simple error response.\n\n    Default: ``False``\n\n.. py:data:: TRAP_BAD_REQUEST_ERRORS\n\n    Trying to access a key that doesn't exist from request dicts like ``args``\n    and ``form`` will return a 400 Bad Request error page. Enable this to treat\n    the error as an unhandled exception instead so that you get the interactive\n    debugger. This is a more specific version of ``TRAP_HTTP_EXCEPTIONS``. If\n    unset, it is enabled in debug mode.\n\n    Default: ``None``\n\n.. py:data:: SECRET_KEY\n\n    A secret key that will be used for securely signing the session cookie\n    and can be used for any other security related needs by extensions or your\n    application. It should be a long random ``bytes`` or ``str``. For\n    example, copy the output of this to your config::\n\n        $ python -c 'import secrets; print(secrets.token_hex())'\n        '192b9bdd22ab9ed4d12e236c78afcb9a393ec15f71bbf5dc987d54727823bcbf'\n\n    **Do not reveal the secret key when posting questions or committing code.**\n\n    Default: ``None``\n\n.. py:data:: SECRET_KEY_FALLBACKS\n\n    A list of old secret keys that can still be used for unsigning. This allows\n    a project to implement key rotation without invalidating active sessions or\n    other recently-signed secrets.\n\n    Keys should be removed after an appropriate period of time, as checking each\n    additional key adds some overhead.\n\n    Order should not matter, but the default implementation will test the last\n    key in the list first, so it might make sense to order oldest to newest.\n\n    Flask's built-in secure cookie session supports this. Extensions that use\n    :data:`SECRET_KEY` may not support this yet.\n\n    Default: ``None``\n\n    .. versionadded:: 3.1\n\n.. py:data:: SESSION_COOKIE_NAME\n\n    The name of the session cookie. Can be changed in case you already have a\n    cookie with the same name.\n\n    Default: ``'session'``\n\n.. py:data:: SESSION_COOKIE_DOMAIN\n\n    The value of the ``Domain`` parameter on the session cookie. If not set, browsers\n    will only send the cookie to the exact domain it was set from. Otherwise, they\n    will send it to any subdomain of the given value as well.\n\n    Not setting this value is more restricted and secure than setting it.\n\n    Default: ``None``\n\n    .. warning::\n        If this is changed after the browser created a cookie is created with\n        one setting, it may result in another being created. Browsers may send\n        send both in an undefined order. In that case, you may want to change\n        :data:`SESSION_COOKIE_NAME` as well or otherwise invalidate old sessions.\n\n    .. versionchanged:: 2.3\n        Not set by default, does not fall back to ``SERVER_NAME``.\n\n.. py:data:: SESSION_COOKIE_PATH\n\n    The path that the session cookie will be valid for. If not set, the cookie\n    will be valid underneath ``APPLICATION_ROOT`` or ``/`` if that is not set.\n\n    Default: ``None``\n\n.. py:data:: SESSION_COOKIE_HTTPONLY\n\n    Browsers will not allow JavaScript access to cookies marked as \"HTTP only\"\n    for security.\n\n    Default: ``True``\n\n.. py:data:: SESSION_COOKIE_SECURE\n\n    Browsers will only send cookies with requests over HTTPS if the cookie is\n    marked \"secure\". The application must be served over HTTPS for this to make\n    sense.\n\n    Default: ``False``\n\n.. py:data:: SESSION_COOKIE_PARTITIONED\n\n    Browsers will send cookies based on the top-level document's domain, rather\n    than only the domain of the document setting the cookie. This prevents third\n    party cookies set in iframes from \"leaking\" between separate sites.\n\n    Browsers are beginning to disallow non-partitioned third party cookies, so\n    you need to mark your cookies partitioned if you expect them to work in such\n    embedded situations.\n\n    Enabling this implicitly enables :data:`SESSION_COOKIE_SECURE` as well, as\n    it is only valid when served over HTTPS.\n\n    Default: ``False``\n\n    .. versionadded:: 3.1\n\n.. py:data:: SESSION_COOKIE_SAMESITE\n\n    Restrict how cookies are sent with requests from external sites. Can\n    be set to ``'Lax'`` (recommended) or ``'Strict'``.\n    See :ref:`security-cookie`.\n\n    Default: ``None``\n\n    .. versionadded:: 1.0\n\n.. py:data:: PERMANENT_SESSION_LIFETIME\n\n    If ``session.permanent`` is true, the cookie's expiration will be set this\n    number of seconds in the future. Can either be a\n    :class:`datetime.timedelta` or an ``int``.\n\n    Flask's default cookie implementation validates that the cryptographic\n    signature is not older than this value.\n\n    Default: ``timedelta(days=31)`` (``2678400`` seconds)\n\n.. py:data:: SESSION_REFRESH_EACH_REQUEST\n\n    Control whether the cookie is sent with every response when\n    ``session.permanent`` is true. Sending the cookie every time (the default)\n    can more reliably keep the session from expiring, but uses more bandwidth.\n    Non-permanent sessions are not affected.\n\n    Default: ``True``\n\n.. py:data:: USE_X_SENDFILE\n\n    When serving files, set the ``X-Sendfile`` header instead of serving the\n    data with Flask. Some web servers, such as Apache, recognize this and serve\n    the data more efficiently. This only makes sense when using such a server.\n\n    Default: ``False``\n\n.. py:data:: SEND_FILE_MAX_AGE_DEFAULT\n\n    When serving files, set the cache control max age to this number of\n    seconds. Can be a :class:`datetime.timedelta` or an ``int``.\n    Override this value on a per-file basis using\n    :meth:`~flask.Flask.get_send_file_max_age` on the application or\n    blueprint.\n\n    If ``None``, ``send_file`` tells the browser to use conditional\n    requests will be used instead of a timed cache, which is usually\n    preferable.\n\n    Default: ``None``\n\n.. py:data:: TRUSTED_HOSTS\n\n    Validate :attr:`.Request.host` and other attributes that use it against\n    these trusted values. Raise a :exc:`~werkzeug.exceptions.SecurityError` if\n    the host is invalid, which results in a 400 error. If it is ``None``, all\n    hosts are valid. Each value is either an exact match, or can start with\n    a dot ``.`` to match any subdomain.\n\n    Validation is done during routing against this value. ``before_request`` and\n    ``after_request`` callbacks will still be called.\n\n    Default: ``None``\n\n    .. versionadded:: 3.1\n\n.. py:data:: SERVER_NAME\n\n    Inform the application what host and port it is bound to.\n\n    Must be set if ``subdomain_matching`` is enabled, to be able to extract the\n    subdomain from the request.\n\n    Must be set for ``url_for`` to generate external URLs outside of a\n    request context.\n\n    Default: ``None``\n\n    .. versionchanged:: 3.1\n        Does not restrict requests to only this domain, for both\n        ``subdomain_matching`` and ``host_matching``.\n\n    .. versionchanged:: 1.0\n        Does not implicitly enable ``subdomain_matching``.\n\n    .. versionchanged:: 2.3\n        Does not affect ``SESSION_COOKIE_DOMAIN``.\n\n.. py:data:: APPLICATION_ROOT\n\n    Inform the application what path it is mounted under by the application /\n    web server.  This is used for generating URLs outside the context of a\n    request (inside a request, the dispatcher is responsible for setting\n    ``SCRIPT_NAME`` instead; see :doc:`/patterns/appdispatch`\n    for examples of dispatch configuration).\n\n    Will be used for the session cookie path if ``SESSION_COOKIE_PATH`` is not\n    set.\n\n    Default: ``'/'``\n\n.. py:data:: PREFERRED_URL_SCHEME\n\n    Use this scheme for generating external URLs when not in a request context.\n\n    Default: ``'http'``\n\n.. py:data:: MAX_CONTENT_LENGTH\n\n    The maximum number of bytes that will be read during this request. If\n    this limit is exceeded, a 413 :exc:`~werkzeug.exceptions.RequestEntityTooLarge`\n    error is raised. If it is set to ``None``, no limit is enforced at the\n    Flask application level. However, if it is ``None`` and the request has no\n    ``Content-Length`` header and the WSGI server does not indicate that it\n    terminates the stream, then no data is read to avoid an infinite stream.\n\n    Each request defaults to this config. It can be set on a specific\n    :attr:`.Request.max_content_length` to apply the limit to that specific\n    view. This should be set appropriately based on an application's or view's\n    specific needs.\n\n    Default: ``None``\n\n    .. versionadded:: 0.6\n\n.. py:data:: MAX_FORM_MEMORY_SIZE\n\n    The maximum size in bytes any non-file form field may be in a\n    ``multipart/form-data`` body. If this limit is exceeded, a 413\n    :exc:`~werkzeug.exceptions.RequestEntityTooLarge` error is raised. If it is\n    set to ``None``, no limit is enforced at the Flask application level.\n\n    Each request defaults to this config. It can be set on a specific\n    :attr:`.Request.max_form_memory_parts` to apply the limit to that specific\n    view. This should be set appropriately based on an application's or view's\n    specific needs.\n\n    Default: ``500_000``\n\n    .. versionadded:: 3.1\n\n.. py:data:: MAX_FORM_PARTS\n\n    The maximum number of fields that may be present in a\n    ``multipart/form-data`` body. If this limit is exceeded, a 413\n    :exc:`~werkzeug.exceptions.RequestEntityTooLarge` error is raised. If it\n    is set to ``None``, no limit is enforced at the Flask application level.\n\n    Each request defaults to this config. It can be set on a specific\n    :attr:`.Request.max_form_parts` to apply the limit to that specific view.\n    This should be set appropriately based on an application's or view's\n    specific needs.\n\n    Default: ``1_000``\n\n    .. versionadded:: 3.1\n\n.. py:data:: TEMPLATES_AUTO_RELOAD\n\n    Reload templates when they are changed. If not set, it will be enabled in\n    debug mode.\n\n    Default: ``None``\n\n.. py:data:: EXPLAIN_TEMPLATE_LOADING\n\n    Log debugging information tracing how a template file was loaded. This can\n    be useful to figure out why a template was not loaded or the wrong file\n    appears to be loaded.\n\n    Default: ``False``\n\n.. py:data:: MAX_COOKIE_SIZE\n\n    Warn if cookie headers are larger than this many bytes. Defaults to\n    ``4093``. Larger cookies may be silently ignored by browsers. Set to\n    ``0`` to disable the warning.\n\n.. py:data:: PROVIDE_AUTOMATIC_OPTIONS\n\n    Set to ``False`` to disable the automatic addition of OPTIONS\n    responses. This can be overridden per route by altering the\n    ``provide_automatic_options`` attribute.\n\n.. versionadded:: 0.4\n   ``LOGGER_NAME``\n\n.. versionadded:: 0.5\n   ``SERVER_NAME``\n\n.. versionadded:: 0.6\n   ``MAX_CONTENT_LENGTH``\n\n.. versionadded:: 0.7\n   ``PROPAGATE_EXCEPTIONS``, ``PRESERVE_CONTEXT_ON_EXCEPTION``\n\n.. versionadded:: 0.8\n   ``TRAP_BAD_REQUEST_ERRORS``, ``TRAP_HTTP_EXCEPTIONS``,\n   ``APPLICATION_ROOT``, ``SESSION_COOKIE_DOMAIN``,\n   ``SESSION_COOKIE_PATH``, ``SESSION_COOKIE_HTTPONLY``,\n   ``SESSION_COOKIE_SECURE``\n\n.. versionadded:: 0.9\n   ``PREFERRED_URL_SCHEME``\n\n.. versionadded:: 0.10\n   ``JSON_AS_ASCII``, ``JSON_SORT_KEYS``, ``JSONIFY_PRETTYPRINT_REGULAR``\n\n.. versionadded:: 0.11\n   ``SESSION_REFRESH_EACH_REQUEST``, ``TEMPLATES_AUTO_RELOAD``,\n   ``LOGGER_HANDLER_POLICY``, ``EXPLAIN_TEMPLATE_LOADING``\n\n.. versionchanged:: 1.0\n    ``LOGGER_NAME`` and ``LOGGER_HANDLER_POLICY`` were removed. See\n    :doc:`/logging` for information about configuration.\n\n    Added :data:`ENV` to reflect the :envvar:`FLASK_ENV` environment\n    variable.\n\n    Added :data:`SESSION_COOKIE_SAMESITE` to control the session\n    cookie's ``SameSite`` option.\n\n    Added :data:`MAX_COOKIE_SIZE` to control a warning from Werkzeug.\n\n.. versionchanged:: 2.2\n    Removed ``PRESERVE_CONTEXT_ON_EXCEPTION``.\n\n.. versionchanged:: 2.3\n    ``JSON_AS_ASCII``, ``JSON_SORT_KEYS``, ``JSONIFY_MIMETYPE``, and\n    ``JSONIFY_PRETTYPRINT_REGULAR`` were removed. The default ``app.json`` provider has\n    equivalent attributes instead.\n\n.. versionchanged:: 2.3\n    ``ENV`` was removed.\n\n.. versionadded:: 3.10\n    Added :data:`PROVIDE_AUTOMATIC_OPTIONS` to control the default\n    addition of autogenerated OPTIONS responses.\n\n\nConfiguring from Python Files\n-----------------------------\n\nConfiguration becomes more useful if you can store it in a separate file, ideally\nlocated outside the actual application package. You can deploy your application, then\nseparately configure it for the specific deployment.\n\nA common pattern is this::\n\n    app = Flask(__name__)\n    app.config.from_object('yourapplication.default_settings')\n    app.config.from_envvar('YOURAPPLICATION_SETTINGS')\n\nThis first loads the configuration from the\n`yourapplication.default_settings` module and then overrides the values\nwith the contents of the file the :envvar:`YOURAPPLICATION_SETTINGS`\nenvironment variable points to.  This environment variable can be set\nin the shell before starting the server:\n\n.. tabs::\n\n   .. group-tab:: Bash\n\n      .. code-block:: text\n\n         $ export YOURAPPLICATION_SETTINGS=/path/to/settings.cfg\n         $ flask run\n          * Running on http://127.0.0.1:5000/\n\n   .. group-tab:: Fish\n\n      .. code-block:: text\n\n         $ set -x YOURAPPLICATION_SETTINGS /path/to/settings.cfg\n         $ flask run\n          * Running on http://127.0.0.1:5000/\n\n   .. group-tab:: CMD\n\n      .. code-block:: text\n\n         > set YOURAPPLICATION_SETTINGS=\\path\\to\\settings.cfg\n         > flask run\n          * Running on http://127.0.0.1:5000/\n\n   .. group-tab:: Powershell\n\n      .. code-block:: text\n\n         > $env:YOURAPPLICATION_SETTINGS = \"\\path\\to\\settings.cfg\"\n         > flask run\n          * Running on http://127.0.0.1:5000/\n\nThe configuration files themselves are actual Python files.  Only values\nin uppercase are actually stored in the config object later on.  So make\nsure to use uppercase letters for your config keys.\n\nHere is an example of a configuration file::\n\n    # Example configuration\n    SECRET_KEY = '192b9bdd22ab9ed4d12e236c78afcb9a393ec15f71bbf5dc987d54727823bcbf'\n\nMake sure to load the configuration very early on, so that extensions have\nthe ability to access the configuration when starting up.  There are other\nmethods on the config object as well to load from individual files.  For a\ncomplete reference, read the :class:`~flask.Config` object's\ndocumentation.\n\n\nConfiguring from Data Files\n---------------------------\n\nIt is also possible to load configuration from a file in a format of\nyour choice using :meth:`~flask.Config.from_file`. For example to load\nfrom a TOML file:\n\n.. code-block:: python\n\n    import tomllib\n    app.config.from_file(\"config.toml\", load=tomllib.load, text=False)\n\nOr from a JSON file:\n\n.. code-block:: python\n\n    import json\n    app.config.from_file(\"config.json\", load=json.load)\n\n\nConfiguring from Environment Variables\n--------------------------------------\n\nIn addition to pointing to configuration files using environment\nvariables, you may find it useful (or necessary) to control your\nconfiguration values directly from the environment. Flask can be\ninstructed to load all environment variables starting with a specific\nprefix into the config using :meth:`~flask.Config.from_prefixed_env`.\n\nEnvironment variables can be set in the shell before starting the\nserver:\n\n.. tabs::\n\n   .. group-tab:: Bash\n\n      .. code-block:: text\n\n         $ export FLASK_SECRET_KEY=\"5f352379324c22463451387a0aec5d2f\"\n         $ export FLASK_MAIL_ENABLED=false\n         $ flask run\n          * Running on http://127.0.0.1:5000/\n\n   .. group-tab:: Fish\n\n      .. code-block:: text\n\n         $ set -x FLASK_SECRET_KEY \"5f352379324c22463451387a0aec5d2f\"\n         $ set -x FLASK_MAIL_ENABLED false\n         $ flask run\n          * Running on http://127.0.0.1:5000/\n\n   .. group-tab:: CMD\n\n      .. code-block:: text\n\n         > set FLASK_SECRET_KEY=\"5f352379324c22463451387a0aec5d2f\"\n         > set FLASK_MAIL_ENABLED=false\n         > flask run\n          * Running on http://127.0.0.1:5000/\n\n   .. group-tab:: Powershell\n\n      .. code-block:: text\n\n         > $env:FLASK_SECRET_KEY = \"5f352379324c22463451387a0aec5d2f\"\n         > $env:FLASK_MAIL_ENABLED = \"false\"\n         > flask run\n          * Running on http://127.0.0.1:5000/\n\nThe variables can then be loaded and accessed via the config with a key\nequal to the environment variable name without the prefix i.e.\n\n.. code-block:: python\n\n    app.config.from_prefixed_env()\n    app.config[\"SECRET_KEY\"]  # Is \"5f352379324c22463451387a0aec5d2f\"\n\nThe prefix is ``FLASK_`` by default. This is configurable via the\n``prefix`` argument of :meth:`~flask.Config.from_prefixed_env`.\n\nValues will be parsed to attempt to convert them to a more specific type\nthan strings. By default :func:`json.loads` is used, so any valid JSON\nvalue is possible, including lists and dicts. This is configurable via\nthe ``loads`` argument of :meth:`~flask.Config.from_prefixed_env`.\n\nWhen adding a boolean value with the default JSON parsing, only \"true\"\nand \"false\", lowercase, are valid values. Keep in mind that any\nnon-empty string is considered ``True`` by Python.\n\nIt is possible to set keys in nested dictionaries by separating the\nkeys with double underscore (``__``). Any intermediate keys that don't\nexist on the parent dict will be initialized to an empty dict.\n\n.. code-block:: text\n\n    $ export FLASK_MYAPI__credentials__username=user123\n\n.. code-block:: python\n\n    app.config[\"MYAPI\"][\"credentials\"][\"username\"]  # Is \"user123\"\n\nOn Windows, environment variable keys are always uppercase, therefore\nthe above example would end up as ``MYAPI__CREDENTIALS__USERNAME``.\n\nFor even more config loading features, including merging and\ncase-insensitive Windows support, try a dedicated library such as\nDynaconf_, which includes integration with Flask.\n\n.. _Dynaconf: https://www.dynaconf.com/\n\n\nConfiguration Best Practices\n----------------------------\n\nThe downside with the approach mentioned earlier is that it makes testing\na little harder.  There is no single 100% solution for this problem in\ngeneral, but there are a couple of things you can keep in mind to improve\nthat experience:\n\n1.  Create your application in a function and register blueprints on it.\n    That way you can create multiple instances of your application with\n    different configurations attached which makes unit testing a lot\n    easier.  You can use this to pass in configuration as needed.\n\n2.  Do not write code that needs the configuration at import time.  If you\n    limit yourself to request-only accesses to the configuration you can\n    reconfigure the object later on as needed.\n\n3.  Make sure to load the configuration very early on, so that\n    extensions can access the configuration when calling ``init_app``.\n\n\n.. _config-dev-prod:\n\nDevelopment / Production\n------------------------\n\nMost applications need more than one configuration.  There should be at\nleast separate configurations for the production server and the one used\nduring development.  The easiest way to handle this is to use a default\nconfiguration that is always loaded and part of the version control, and a\nseparate configuration that overrides the values as necessary as mentioned\nin the example above::\n\n    app = Flask(__name__)\n    app.config.from_object('yourapplication.default_settings')\n    app.config.from_envvar('YOURAPPLICATION_SETTINGS')\n\nThen you just have to add a separate :file:`config.py` file and export\n``YOURAPPLICATION_SETTINGS=/path/to/config.py`` and you are done.  However\nthere are alternative ways as well.  For example you could use imports or\nsubclassing.\n\nWhat is very popular in the Django world is to make the import explicit in\nthe config file by adding ``from yourapplication.default_settings\nimport *`` to the top of the file and then overriding the changes by hand.\nYou could also inspect an environment variable like\n``YOURAPPLICATION_MODE`` and set that to `production`, `development` etc\nand import different hard-coded files based on that.\n\nAn interesting pattern is also to use classes and inheritance for\nconfiguration::\n\n    class Config(object):\n        TESTING = False\n\n    class ProductionConfig(Config):\n        DATABASE_URI = 'mysql://user@localhost/foo'\n\n    class DevelopmentConfig(Config):\n        DATABASE_URI = \"sqlite:////tmp/foo.db\"\n\n    class TestingConfig(Config):\n        DATABASE_URI = 'sqlite:///:memory:'\n        TESTING = True\n\nTo enable such a config you just have to call into\n:meth:`~flask.Config.from_object`::\n\n    app.config.from_object('configmodule.ProductionConfig')\n\nNote that :meth:`~flask.Config.from_object` does not instantiate the class\nobject. If you need to instantiate the class, such as to access a property,\nthen you must do so before calling :meth:`~flask.Config.from_object`::\n\n    from configmodule import ProductionConfig\n    app.config.from_object(ProductionConfig())\n\n    # Alternatively, import via string:\n    from werkzeug.utils import import_string\n    cfg = import_string('configmodule.ProductionConfig')()\n    app.config.from_object(cfg)\n\nInstantiating the configuration object allows you to use ``@property`` in\nyour configuration classes::\n\n    class Config(object):\n        \"\"\"Base config, uses staging database server.\"\"\"\n        TESTING = False\n        DB_SERVER = '192.168.1.56'\n\n        @property\n        def DATABASE_URI(self):  # Note: all caps\n            return f\"mysql://user@{self.DB_SERVER}/foo\"\n\n    class ProductionConfig(Config):\n        \"\"\"Uses production database server.\"\"\"\n        DB_SERVER = '192.168.19.32'\n\n    class DevelopmentConfig(Config):\n        DB_SERVER = 'localhost'\n\n    class TestingConfig(Config):\n        DB_SERVER = 'localhost'\n        DATABASE_URI = 'sqlite:///:memory:'\n\nThere are many different ways and it's up to you how you want to manage\nyour configuration files.  However here a list of good recommendations:\n\n-   Keep a default configuration in version control.  Either populate the\n    config with this default configuration or import it in your own\n    configuration files before overriding values.\n-   Use an environment variable to switch between the configurations.\n    This can be done from outside the Python interpreter and makes\n    development and deployment much easier because you can quickly and\n    easily switch between different configs without having to touch the\n    code at all.  If you are working often on different projects you can\n    even create your own script for sourcing that activates a virtualenv\n    and exports the development configuration for you.\n-   Use a tool like `fabric`_ to push code and configuration separately\n    to the production server(s).\n\n.. _fabric: https://www.fabfile.org/\n\n\n.. _instance-folders:\n\nInstance Folders\n----------------\n\n.. versionadded:: 0.8\n\nFlask 0.8 introduces instance folders.  Flask for a long time made it\npossible to refer to paths relative to the application's folder directly\n(via :attr:`Flask.root_path`).  This was also how many developers loaded\nconfigurations stored next to the application.  Unfortunately however this\nonly works well if applications are not packages in which case the root\npath refers to the contents of the package.\n\nWith Flask 0.8 a new attribute was introduced:\n:attr:`Flask.instance_path`.  It refers to a new concept called the\n\u201cinstance folder\u201d.  The instance folder is designed to not be under\nversion control and be deployment specific.  It's the perfect place to\ndrop things that either change at runtime or configuration files.\n\nYou can either explicitly provide the path of the instance folder when\ncreating the Flask application or you can let Flask autodetect the\ninstance folder.  For explicit configuration use the `instance_path`\nparameter::\n\n    app = Flask(__name__, instance_path='/path/to/instance/folder')\n\nPlease keep in mind that this path *must* be absolute when provided.\n\nIf the `instance_path` parameter is not provided the following default\nlocations are used:\n\n-   Uninstalled module::\n\n        /myapp.py\n        /instance\n\n-   Uninstalled package::\n\n        /myapp\n            /__init__.py\n        /instance\n\n-   Installed module or package::\n\n        $PREFIX/lib/pythonX.Y/site-packages/myapp\n        $PREFIX/var/myapp-instance\n\n    ``$PREFIX`` is the prefix of your Python installation.  This can be\n    ``/usr`` or the path to your virtualenv.  You can print the value of\n    ``sys.prefix`` to see what the prefix is set to.\n\nSince the config object provided loading of configuration files from\nrelative filenames we made it possible to change the loading via filenames\nto be relative to the instance path if wanted.  The behavior of relative\npaths in config files can be flipped between \u201crelative to the application\nroot\u201d (the default) to \u201crelative to instance folder\u201d via the\n`instance_relative_config` switch to the application constructor::\n\n    app = Flask(__name__, instance_relative_config=True)\n\nHere is a full example of how to configure Flask to preload the config\nfrom a module and then override the config from a file in the instance\nfolder if it exists::\n\n    app = Flask(__name__, instance_relative_config=True)\n    app.config.from_object('yourapplication.default_settings')\n    app.config.from_pyfile('application.cfg', silent=True)\n\nThe path to the instance folder can be found via the\n:attr:`Flask.instance_path`.  Flask also provides a shortcut to open a\nfile from the instance folder with :meth:`Flask.open_instance_resource`.\n\nExample usage for both::\n\n    filename = os.path.join(app.instance_path, 'application.cfg')\n    with open(filename) as f:\n        config = f.read()\n\n    # or via open_instance_resource:\n    with app.open_instance_resource('application.cfg') as f:\n        config = f.read()\n", "docs/contributing.rst": "Contributing\n============\n\nSee the Pallets `detailed contributing documentation <contrib_>`_ for many ways\nto contribute, including reporting issues, requesting features, asking or\nanswering questions, and making PRs.\n\n.. _contrib: https://palletsprojects.com/contributing/\n", "docs/debugging.rst": "Debugging Application Errors\n============================\n\n\nIn Production\n-------------\n\n**Do not run the development server, or enable the built-in debugger, in\na production environment.** The debugger allows executing arbitrary\nPython code from the browser. It's protected by a pin, but that should\nnot be relied on for security.\n\nUse an error logging tool, such as Sentry, as described in\n:ref:`error-logging-tools`, or enable logging and notifications as\ndescribed in :doc:`/logging`.\n\nIf you have access to the server, you could add some code to start an\nexternal debugger if ``request.remote_addr`` matches your IP. Some IDE\ndebuggers also have a remote mode so breakpoints on the server can be\ninteracted with locally. Only enable a debugger temporarily.\n\n\nThe Built-In Debugger\n---------------------\n\nThe built-in Werkzeug development server provides a debugger which shows\nan interactive traceback in the browser when an unhandled error occurs\nduring a request. This debugger should only be used during development.\n\n.. image:: _static/debugger.png\n   :align: center\n   :class: screenshot\n   :alt: screenshot of debugger in action\n\n.. warning::\n\n    The debugger allows executing arbitrary Python code from the\n    browser. It is protected by a pin, but still represents a major\n    security risk. Do not run the development server or debugger in a\n    production environment.\n\nThe debugger is enabled by default when the development server is run in debug mode.\n\n.. code-block:: text\n\n    $ flask --app hello run --debug\n\nWhen running from Python code, passing ``debug=True`` enables debug mode, which is\nmostly equivalent.\n\n.. code-block:: python\n\n    app.run(debug=True)\n\n:doc:`/server` and :doc:`/cli` have more information about running the debugger and\ndebug mode. More information about the debugger can be found in the `Werkzeug\ndocumentation <https://werkzeug.palletsprojects.com/debug/>`__.\n\n\nExternal Debuggers\n------------------\n\nExternal debuggers, such as those provided by IDEs, can offer a more\npowerful debugging experience than the built-in debugger. They can also\nbe used to step through code during a request before an error is raised,\nor if no error is raised. Some even have a remote mode so you can debug\ncode running on another machine.\n\nWhen using an external debugger, the app should still be in debug mode, otherwise Flask\nturns unhandled errors into generic 500 error pages. However, the built-in debugger and\nreloader should be disabled so they don't interfere with the external debugger.\n\n.. code-block:: text\n\n    $ flask --app hello run --debug --no-debugger --no-reload\n\nWhen running from Python:\n\n.. code-block:: python\n\n    app.run(debug=True, use_debugger=False, use_reloader=False)\n\nDisabling these isn't required, an external debugger will continue to work with the\nfollowing caveats.\n\n-   If the built-in debugger is not disabled, it will catch unhandled exceptions before\n    the external debugger can.\n-   If the reloader is not disabled, it could cause an unexpected reload if code changes\n    during a breakpoint.\n-   The development server will still catch unhandled exceptions if the built-in\n    debugger is disabled, otherwise it would crash on any error. If you want that (and\n    usually you don't) pass ``passthrough_errors=True`` to ``app.run``.\n\n    .. code-block:: python\n\n        app.run(\n            debug=True, passthrough_errors=True,\n            use_debugger=False, use_reloader=False\n        )\n", "docs/deploying/apache-httpd.rst": "Apache httpd\n============\n\n`Apache httpd`_ is a fast, production level HTTP server. When serving\nyour application with one of the WSGI servers listed in :doc:`index`, it\nis often good or necessary to put a dedicated HTTP server in front of\nit. This \"reverse proxy\" can handle incoming requests, TLS, and other\nsecurity and performance concerns better than the WSGI server.\n\nhttpd can be installed using your system package manager, or a pre-built\nexecutable for Windows. Installing and running httpd itself is outside\nthe scope of this doc. This page outlines the basics of configuring\nhttpd to proxy your application. Be sure to read its documentation to\nunderstand what features are available.\n\n.. _Apache httpd: https://httpd.apache.org/\n\n\nDomain Name\n-----------\n\nAcquiring and configuring a domain name is outside the scope of this\ndoc. In general, you will buy a domain name from a registrar, pay for\nserver space with a hosting provider, and then point your registrar\nat the hosting provider's name servers.\n\nTo simulate this, you can also edit your ``hosts`` file, located at\n``/etc/hosts`` on Linux. Add a line that associates a name with the\nlocal IP.\n\nModern Linux systems may be configured to treat any domain name that\nends with ``.localhost`` like this without adding it to the ``hosts``\nfile.\n\n.. code-block:: python\n    :caption: ``/etc/hosts``\n\n    127.0.0.1 hello.localhost\n\n\nConfiguration\n-------------\n\nThe httpd configuration is located at ``/etc/httpd/conf/httpd.conf`` on\nLinux. It may be different depending on your operating system. Check the\ndocs and look for ``httpd.conf``.\n\nRemove or comment out any existing ``DocumentRoot`` directive. Add the\nconfig lines below. We'll assume the WSGI server is listening locally at\n``http://127.0.0.1:8000``.\n\n.. code-block:: apache\n    :caption: ``/etc/httpd/conf/httpd.conf``\n\n    LoadModule proxy_module modules/mod_proxy.so\n    LoadModule proxy_http_module modules/mod_proxy_http.so\n    ProxyPass / http://127.0.0.1:8000/\n    RequestHeader set X-Forwarded-Proto http\n    RequestHeader set X-Forwarded-Prefix /\n\nThe ``LoadModule`` lines might already exist. If so, make sure they are\nuncommented instead of adding them manually.\n\nThen :doc:`proxy_fix` so that your application uses the ``X-Forwarded``\nheaders. ``X-Forwarded-For`` and ``X-Forwarded-Host`` are automatically\nset by ``ProxyPass``.\n", "docs/deploying/asgi.rst": "ASGI\n====\n\nIf you'd like to use an ASGI server you will need to utilise WSGI to\nASGI middleware. The asgiref\n`WsgiToAsgi <https://github.com/django/asgiref#wsgi-to-asgi-adapter>`_\nadapter is recommended as it integrates with the event loop used for\nFlask's :ref:`async_await` support. You can use the adapter by\nwrapping the Flask app,\n\n.. code-block:: python\n\n    from asgiref.wsgi import WsgiToAsgi\n    from flask import Flask\n\n    app = Flask(__name__)\n\n    ...\n\n    asgi_app = WsgiToAsgi(app)\n\nand then serving the ``asgi_app`` with the ASGI server, e.g. using\n`Hypercorn <https://github.com/pgjones/hypercorn>`_,\n\n.. sourcecode:: text\n\n    $ hypercorn module:asgi_app\n", "docs/deploying/eventlet.rst": "eventlet\n========\n\nPrefer using :doc:`gunicorn` with eventlet workers rather than using\n`eventlet`_ directly. Gunicorn provides a much more configurable and\nproduction-tested server.\n\n`eventlet`_ allows writing asynchronous, coroutine-based code that looks\nlike standard synchronous Python. It uses `greenlet`_ to enable task\nswitching without writing ``async/await`` or using ``asyncio``.\n\n:doc:`gevent` is another library that does the same thing. Certain\ndependencies you have, or other considerations, may affect which of the\ntwo you choose to use.\n\neventlet provides a WSGI server that can handle many connections at once\ninstead of one per worker process. You must actually use eventlet in\nyour own code to see any benefit to using the server.\n\n.. _eventlet: https://eventlet.net/\n.. _greenlet: https://greenlet.readthedocs.io/en/latest/\n\n\nInstalling\n----------\n\nWhen using eventlet, greenlet>=1.0 is required, otherwise context locals\nsuch as ``request`` will not work as expected. When using PyPy,\nPyPy>=7.3.7 is required.\n\nCreate a virtualenv, install your application, then install\n``eventlet``.\n\n.. code-block:: text\n\n    $ cd hello-app\n    $ python -m venv .venv\n    $ . .venv/bin/activate\n    $ pip install .  # install your application\n    $ pip install eventlet\n\n\nRunning\n-------\n\nTo use eventlet to serve your application, write a script that imports\nits ``wsgi.server``, as well as your app or app factory.\n\n.. code-block:: python\n    :caption: ``wsgi.py``\n\n    import eventlet\n    from eventlet import wsgi\n    from hello import create_app\n\n    app = create_app()\n    wsgi.server(eventlet.listen((\"127.0.0.1\", 8000)), app)\n\n.. code-block:: text\n\n    $ python wsgi.py\n    (x) wsgi starting up on http://127.0.0.1:8000\n\n\nBinding Externally\n------------------\n\neventlet should not be run as root because it would cause your\napplication code to run as root, which is not secure. However, this\nmeans it will not be possible to bind to port 80 or 443. Instead, a\nreverse proxy such as :doc:`nginx` or :doc:`apache-httpd` should be used\nin front of eventlet.\n\nYou can bind to all external IPs on a non-privileged port by using\n``0.0.0.0`` in the server arguments shown in the previous section.\nDon't do this when using a reverse proxy setup, otherwise it will be\npossible to bypass the proxy.\n\n``0.0.0.0`` is not a valid address to navigate to, you'd use a specific\nIP address in your browser.\n", "docs/deploying/gevent.rst": "gevent\n======\n\nPrefer using :doc:`gunicorn` or :doc:`uwsgi` with gevent workers rather\nthan using `gevent`_ directly. Gunicorn and uWSGI provide much more\nconfigurable and production-tested servers.\n\n`gevent`_ allows writing asynchronous, coroutine-based code that looks\nlike standard synchronous Python. It uses `greenlet`_ to enable task\nswitching without writing ``async/await`` or using ``asyncio``.\n\n:doc:`eventlet` is another library that does the same thing. Certain\ndependencies you have, or other considerations, may affect which of the\ntwo you choose to use.\n\ngevent provides a WSGI server that can handle many connections at once\ninstead of one per worker process. You must actually use gevent in your\nown code to see any benefit to using the server.\n\n.. _gevent: https://www.gevent.org/\n.. _greenlet: https://greenlet.readthedocs.io/en/latest/\n\n\nInstalling\n----------\n\nWhen using gevent, greenlet>=1.0 is required, otherwise context locals\nsuch as ``request`` will not work as expected. When using PyPy,\nPyPy>=7.3.7 is required.\n\nCreate a virtualenv, install your application, then install ``gevent``.\n\n.. code-block:: text\n\n    $ cd hello-app\n    $ python -m venv .venv\n    $ . .venv/bin/activate\n    $ pip install .  # install your application\n    $ pip install gevent\n\n\nRunning\n-------\n\nTo use gevent to serve your application, write a script that imports its\n``WSGIServer``, as well as your app or app factory.\n\n.. code-block:: python\n    :caption: ``wsgi.py``\n\n    from gevent.pywsgi import WSGIServer\n    from hello import create_app\n\n    app = create_app()\n    http_server = WSGIServer((\"127.0.0.1\", 8000), app)\n    http_server.serve_forever()\n\n.. code-block:: text\n\n    $ python wsgi.py\n\nNo output is shown when the server starts.\n\n\nBinding Externally\n------------------\n\ngevent should not be run as root because it would cause your\napplication code to run as root, which is not secure. However, this\nmeans it will not be possible to bind to port 80 or 443. Instead, a\nreverse proxy such as :doc:`nginx` or :doc:`apache-httpd` should be used\nin front of gevent.\n\nYou can bind to all external IPs on a non-privileged port by using\n``0.0.0.0`` in the server arguments shown in the previous section. Don't\ndo this when using a reverse proxy setup, otherwise it will be possible\nto bypass the proxy.\n\n``0.0.0.0`` is not a valid address to navigate to, you'd use a specific\nIP address in your browser.\n", "docs/deploying/gunicorn.rst": "Gunicorn\n========\n\n`Gunicorn`_ is a pure Python WSGI server with simple configuration and\nmultiple worker implementations for performance tuning.\n\n*   It tends to integrate easily with hosting platforms.\n*   It does not support Windows (but does run on WSL).\n*   It is easy to install as it does not require additional dependencies\n    or compilation.\n*   It has built-in async worker support using gevent or eventlet.\n\nThis page outlines the basics of running Gunicorn. Be sure to read its\n`documentation`_ and use ``gunicorn --help`` to understand what features\nare available.\n\n.. _Gunicorn: https://gunicorn.org/\n.. _documentation: https://docs.gunicorn.org/\n\n\nInstalling\n----------\n\nGunicorn is easy to install, as it does not require external\ndependencies or compilation. It runs on Windows only under WSL.\n\nCreate a virtualenv, install your application, then install\n``gunicorn``.\n\n.. code-block:: text\n\n    $ cd hello-app\n    $ python -m venv .venv\n    $ . .venv/bin/activate\n    $ pip install .  # install your application\n    $ pip install gunicorn\n\n\nRunning\n-------\n\nThe only required argument to Gunicorn tells it how to load your Flask\napplication. The syntax is ``{module_import}:{app_variable}``.\n``module_import`` is the dotted import name to the module with your\napplication. ``app_variable`` is the variable with the application. It\ncan also be a function call (with any arguments) if you're using the\napp factory pattern.\n\n.. code-block:: text\n\n    # equivalent to 'from hello import app'\n    $ gunicorn -w 4 'hello:app'\n\n    # equivalent to 'from hello import create_app; create_app()'\n    $ gunicorn -w 4 'hello:create_app()'\n\n    Starting gunicorn 20.1.0\n    Listening at: http://127.0.0.1:8000 (x)\n    Using worker: sync\n    Booting worker with pid: x\n    Booting worker with pid: x\n    Booting worker with pid: x\n    Booting worker with pid: x\n\nThe ``-w`` option specifies the number of processes to run; a starting\nvalue could be ``CPU * 2``. The default is only 1 worker, which is\nprobably not what you want for the default worker type.\n\nLogs for each request aren't shown by default, only worker info and\nerrors are shown. To show access logs on stdout, use the\n``--access-logfile=-`` option.\n\n\nBinding Externally\n------------------\n\nGunicorn should not be run as root because it would cause your\napplication code to run as root, which is not secure. However, this\nmeans it will not be possible to bind to port 80 or 443. Instead, a\nreverse proxy such as :doc:`nginx` or :doc:`apache-httpd` should be used\nin front of Gunicorn.\n\nYou can bind to all external IPs on a non-privileged port using the\n``-b 0.0.0.0`` option. Don't do this when using a reverse proxy setup,\notherwise it will be possible to bypass the proxy.\n\n.. code-block:: text\n\n    $ gunicorn -w 4 -b 0.0.0.0 'hello:create_app()'\n    Listening at: http://0.0.0.0:8000 (x)\n\n``0.0.0.0`` is not a valid address to navigate to, you'd use a specific\nIP address in your browser.\n\n\nAsync with gevent or eventlet\n-----------------------------\n\nThe default sync worker is appropriate for many use cases. If you need\nasynchronous support, Gunicorn provides workers using either `gevent`_\nor `eventlet`_. This is not the same as Python's ``async/await``, or the\nASGI server spec. You must actually use gevent/eventlet in your own code\nto see any benefit to using the workers.\n\nWhen using either gevent or eventlet, greenlet>=1.0 is required,\notherwise context locals such as ``request`` will not work as expected.\nWhen using PyPy, PyPy>=7.3.7 is required.\n\nTo use gevent:\n\n.. code-block:: text\n\n    $ gunicorn -k gevent 'hello:create_app()'\n    Starting gunicorn 20.1.0\n    Listening at: http://127.0.0.1:8000 (x)\n    Using worker: gevent\n    Booting worker with pid: x\n\nTo use eventlet:\n\n.. code-block:: text\n\n    $ gunicorn -k eventlet 'hello:create_app()'\n    Starting gunicorn 20.1.0\n    Listening at: http://127.0.0.1:8000 (x)\n    Using worker: eventlet\n    Booting worker with pid: x\n\n.. _gevent: https://www.gevent.org/\n.. _eventlet: https://eventlet.net/\n", "docs/deploying/index.rst": "Deploying to Production\n=======================\n\nAfter developing your application, you'll want to make it available\npublicly to other users. When you're developing locally, you're probably\nusing the built-in development server, debugger, and reloader. These\nshould not be used in production. Instead, you should use a dedicated\nWSGI server or hosting platform, some of which will be described here.\n\n\"Production\" means \"not development\", which applies whether you're\nserving your application publicly to millions of users or privately /\nlocally to a single user. **Do not use the development server when\ndeploying to production. It is intended for use only during local\ndevelopment. It is not designed to be particularly secure, stable, or\nefficient.**\n\nSelf-Hosted Options\n-------------------\n\nFlask is a WSGI *application*. A WSGI *server* is used to run the\napplication, converting incoming HTTP requests to the standard WSGI\nenviron, and converting outgoing WSGI responses to HTTP responses.\n\nThe primary goal of these docs is to familiarize you with the concepts\ninvolved in running a WSGI application using a production WSGI server\nand HTTP server. There are many WSGI servers and HTTP servers, with many\nconfiguration possibilities. The pages below discuss the most common\nservers, and show the basics of running each one. The next section\ndiscusses platforms that can manage this for you.\n\n.. toctree::\n    :maxdepth: 1\n\n    gunicorn\n    waitress\n    mod_wsgi\n    uwsgi\n    gevent\n    eventlet\n    asgi\n\nWSGI servers have HTTP servers built-in. However, a dedicated HTTP\nserver may be safer, more efficient, or more capable. Putting an HTTP\nserver in front of the WSGI server is called a \"reverse proxy.\"\n\n.. toctree::\n    :maxdepth: 1\n\n    proxy_fix\n    nginx\n    apache-httpd\n\nThis list is not exhaustive, and you should evaluate these and other\nservers based on your application's needs. Different servers will have\ndifferent capabilities, configuration, and support.\n\n\nHosting Platforms\n-----------------\n\nThere are many services available for hosting web applications without\nneeding to maintain your own server, networking, domain, etc. Some\nservices may have a free tier up to a certain time or bandwidth. Many of\nthese services use one of the WSGI servers described above, or a similar\ninterface. The links below are for some of the most common platforms,\nwhich have instructions for Flask, WSGI, or Python.\n\n- `PythonAnywhere <https://help.pythonanywhere.com/pages/Flask/>`_\n- `Google App Engine <https://cloud.google.com/appengine/docs/standard/python3/building-app>`_\n- `Google Cloud Run <https://cloud.google.com/run/docs/quickstarts/build-and-deploy/deploy-python-service>`_\n- `AWS Elastic Beanstalk <https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create-deploy-python-flask.html>`_\n- `Microsoft Azure <https://docs.microsoft.com/en-us/azure/app-service/quickstart-python>`_\n\nThis list is not exhaustive, and you should evaluate these and other\nservices based on your application's needs. Different services will have\ndifferent capabilities, configuration, pricing, and support.\n\nYou'll probably need to :doc:`proxy_fix` when using most hosting\nplatforms.\n", "docs/deploying/mod_wsgi.rst": "mod_wsgi\n========\n\n`mod_wsgi`_ is a WSGI server integrated with the `Apache httpd`_ server.\nThe modern `mod_wsgi-express`_ command makes it easy to configure and\nstart the server without needing to write Apache httpd configuration.\n\n*   Tightly integrated with Apache httpd.\n*   Supports Windows directly.\n*   Requires a compiler and the Apache development headers to install.\n*   Does not require a reverse proxy setup.\n\nThis page outlines the basics of running mod_wsgi-express, not the more\ncomplex installation and configuration with httpd. Be sure to read the\n`mod_wsgi-express`_, `mod_wsgi`_, and `Apache httpd`_ documentation to\nunderstand what features are available.\n\n.. _mod_wsgi-express: https://pypi.org/project/mod-wsgi/\n.. _mod_wsgi: https://modwsgi.readthedocs.io/\n.. _Apache httpd: https://httpd.apache.org/\n\n\nInstalling\n----------\n\nInstalling mod_wsgi requires a compiler and the Apache server and\ndevelopment headers installed. You will get an error if they are not.\nHow to install them depends on the OS and package manager that you use.\n\nCreate a virtualenv, install your application, then install\n``mod_wsgi``.\n\n.. code-block:: text\n\n    $ cd hello-app\n    $ python -m venv .venv\n    $ . .venv/bin/activate\n    $ pip install .  # install your application\n    $ pip install mod_wsgi\n\n\nRunning\n-------\n\nThe only argument to ``mod_wsgi-express`` specifies a script containing\nyour Flask application, which must be called ``application``. You can\nwrite a small script to import your app with this name, or to create it\nif using the app factory pattern.\n\n.. code-block:: python\n    :caption: ``wsgi.py``\n\n    from hello import app\n\n    application = app\n\n.. code-block:: python\n    :caption: ``wsgi.py``\n\n    from hello import create_app\n\n    application = create_app()\n\nNow run the ``mod_wsgi-express start-server`` command.\n\n.. code-block:: text\n\n    $ mod_wsgi-express start-server wsgi.py --processes 4\n\nThe ``--processes`` option specifies the number of worker processes to\nrun; a starting value could be ``CPU * 2``.\n\nLogs for each request aren't show in the terminal. If an error occurs,\nits information is written to the error log file shown when starting the\nserver.\n\n\nBinding Externally\n------------------\n\nUnlike the other WSGI servers in these docs, mod_wsgi can be run as\nroot to bind to privileged ports like 80 and 443. However, it must be\nconfigured to drop permissions to a different user and group for the\nworker processes.\n\nFor example, if you created a ``hello`` user and group, you should\ninstall your virtualenv and application as that user, then tell\nmod_wsgi to drop to that user after starting.\n\n.. code-block:: text\n\n    $ sudo /home/hello/.venv/bin/mod_wsgi-express start-server \\\n        /home/hello/wsgi.py \\\n        --user hello --group hello --port 80 --processes 4\n", "docs/deploying/nginx.rst": "nginx\n=====\n\n`nginx`_ is a fast, production level HTTP server. When serving your\napplication with one of the WSGI servers listed in :doc:`index`, it is\noften good or necessary to put a dedicated HTTP server in front of it.\nThis \"reverse proxy\" can handle incoming requests, TLS, and other\nsecurity and performance concerns better than the WSGI server.\n\nNginx can be installed using your system package manager, or a pre-built\nexecutable for Windows. Installing and running Nginx itself is outside\nthe scope of this doc. This page outlines the basics of configuring\nNginx to proxy your application. Be sure to read its documentation to\nunderstand what features are available.\n\n.. _nginx: https://nginx.org/\n\n\nDomain Name\n-----------\n\nAcquiring and configuring a domain name is outside the scope of this\ndoc. In general, you will buy a domain name from a registrar, pay for\nserver space with a hosting provider, and then point your registrar\nat the hosting provider's name servers.\n\nTo simulate this, you can also edit your ``hosts`` file, located at\n``/etc/hosts`` on Linux. Add a line that associates a name with the\nlocal IP.\n\nModern Linux systems may be configured to treat any domain name that\nends with ``.localhost`` like this without adding it to the ``hosts``\nfile.\n\n.. code-block:: python\n    :caption: ``/etc/hosts``\n\n    127.0.0.1 hello.localhost\n\n\nConfiguration\n-------------\n\nThe nginx configuration is located at ``/etc/nginx/nginx.conf`` on\nLinux. It may be different depending on your operating system. Check the\ndocs and look for ``nginx.conf``.\n\nRemove or comment out any existing ``server`` section. Add a ``server``\nsection and use the ``proxy_pass`` directive to point to the address the\nWSGI server is listening on. We'll assume the WSGI server is listening\nlocally at ``http://127.0.0.1:8000``.\n\n.. code-block:: nginx\n    :caption: ``/etc/nginx.conf``\n\n    server {\n        listen 80;\n        server_name _;\n\n        location / {\n            proxy_pass http://127.0.0.1:8000/;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n            proxy_set_header X-Forwarded-Host $host;\n            proxy_set_header X-Forwarded-Prefix /;\n        }\n    }\n\nThen :doc:`proxy_fix` so that your application uses these headers.\n", "docs/deploying/proxy_fix.rst": "Tell Flask it is Behind a Proxy\n===============================\n\nWhen using a reverse proxy, or many Python hosting platforms, the proxy\nwill intercept and forward all external requests to the local WSGI\nserver.\n\nFrom the WSGI server and Flask application's perspectives, requests are\nnow coming from the HTTP server to the local address, rather than from\nthe remote address to the external server address.\n\nHTTP servers should set ``X-Forwarded-`` headers to pass on the real\nvalues to the application. The application can then be told to trust and\nuse those values by wrapping it with the\n:doc:`werkzeug:middleware/proxy_fix` middleware provided by Werkzeug.\n\nThis middleware should only be used if the application is actually\nbehind a proxy, and should be configured with the number of proxies that\nare chained in front of it. Not all proxies set all the headers. Since\nincoming headers can be faked, you must set how many proxies are setting\neach header so the middleware knows what to trust.\n\n.. code-block:: python\n\n    from werkzeug.middleware.proxy_fix import ProxyFix\n\n    app.wsgi_app = ProxyFix(\n        app.wsgi_app, x_for=1, x_proto=1, x_host=1, x_prefix=1\n    )\n\nRemember, only apply this middleware if you are behind a proxy, and set\nthe correct number of proxies that set each header. It can be a security\nissue if you get this configuration wrong.\n", "docs/deploying/uwsgi.rst": "uWSGI\n=====\n\n`uWSGI`_ is a fast, compiled server suite with extensive configuration\nand capabilities beyond a basic server.\n\n*   It can be very performant due to being a compiled program.\n*   It is complex to configure beyond the basic application, and has so\n    many options that it can be difficult for beginners to understand.\n*   It does not support Windows (but does run on WSL).\n*   It requires a compiler to install in some cases.\n\nThis page outlines the basics of running uWSGI. Be sure to read its\ndocumentation to understand what features are available.\n\n.. _uWSGI: https://uwsgi-docs.readthedocs.io/en/latest/\n\n\nInstalling\n----------\n\nuWSGI has multiple ways to install it. The most straightforward is to\ninstall the ``pyuwsgi`` package, which provides precompiled wheels for\ncommon platforms. However, it does not provide SSL support, which can be\nprovided with a reverse proxy instead.\n\nCreate a virtualenv, install your application, then install ``pyuwsgi``.\n\n.. code-block:: text\n\n    $ cd hello-app\n    $ python -m venv .venv\n    $ . .venv/bin/activate\n    $ pip install .  # install your application\n    $ pip install pyuwsgi\n\nIf you have a compiler available, you can install the ``uwsgi`` package\ninstead. Or install the ``pyuwsgi`` package from sdist instead of wheel.\nEither method will include SSL support.\n\n.. code-block:: text\n\n    $ pip install uwsgi\n\n    # or\n    $ pip install --no-binary pyuwsgi pyuwsgi\n\n\nRunning\n-------\n\nThe most basic way to run uWSGI is to tell it to start an HTTP server\nand import your application.\n\n.. code-block:: text\n\n    $ uwsgi --http 127.0.0.1:8000 --master -p 4 -w hello:app\n\n    *** Starting uWSGI 2.0.20 (64bit) on [x] ***\n    *** Operational MODE: preforking ***\n    mounting hello:app on /\n    spawned uWSGI master process (pid: x)\n    spawned uWSGI worker 1 (pid: x, cores: 1)\n    spawned uWSGI worker 2 (pid: x, cores: 1)\n    spawned uWSGI worker 3 (pid: x, cores: 1)\n    spawned uWSGI worker 4 (pid: x, cores: 1)\n    spawned uWSGI http 1 (pid: x)\n\nIf you're using the app factory pattern, you'll need to create a small\nPython file to create the app, then point uWSGI at that.\n\n.. code-block:: python\n    :caption: ``wsgi.py``\n\n    from hello import create_app\n\n    app = create_app()\n\n.. code-block:: text\n\n    $ uwsgi --http 127.0.0.1:8000 --master -p 4 -w wsgi:app\n\nThe ``--http`` option starts an HTTP server at 127.0.0.1 port 8000. The\n``--master`` option specifies the standard worker manager. The ``-p``\noption starts 4 worker processes; a starting value could be ``CPU * 2``.\nThe ``-w`` option tells uWSGI how to import your application\n\n\nBinding Externally\n------------------\n\nuWSGI should not be run as root with the configuration shown in this doc\nbecause it would cause your application code to run as root, which is\nnot secure. However, this means it will not be possible to bind to port\n80 or 443. Instead, a reverse proxy such as :doc:`nginx` or\n:doc:`apache-httpd` should be used in front of uWSGI. It is possible to\nrun uWSGI as root securely, but that is beyond the scope of this doc.\n\nuWSGI has optimized integration with `Nginx uWSGI`_ and\n`Apache mod_proxy_uwsgi`_, and possibly other servers, instead of using\na standard HTTP proxy. That configuration is beyond the scope of this\ndoc, see the links for more information.\n\n.. _Nginx uWSGI: https://uwsgi-docs.readthedocs.io/en/latest/Nginx.html\n.. _Apache mod_proxy_uwsgi: https://uwsgi-docs.readthedocs.io/en/latest/Apache.html#mod-proxy-uwsgi\n\nYou can bind to all external IPs on a non-privileged port using the\n``--http 0.0.0.0:8000`` option. Don't do this when using a reverse proxy\nsetup, otherwise it will be possible to bypass the proxy.\n\n.. code-block:: text\n\n    $ uwsgi --http 0.0.0.0:8000 --master -p 4 -w wsgi:app\n\n``0.0.0.0`` is not a valid address to navigate to, you'd use a specific\nIP address in your browser.\n\n\nAsync with gevent\n-----------------\n\nThe default sync worker is appropriate for many use cases. If you need\nasynchronous support, uWSGI provides a `gevent`_ worker. This is not the\nsame as Python's ``async/await``, or the ASGI server spec. You must\nactually use gevent in your own code to see any benefit to using the\nworker.\n\nWhen using gevent, greenlet>=1.0 is required, otherwise context locals\nsuch as ``request`` will not work as expected. When using PyPy,\nPyPy>=7.3.7 is required.\n\n.. code-block:: text\n\n    $ uwsgi --http 127.0.0.1:8000 --master --gevent 100 -w wsgi:app\n\n    *** Starting uWSGI 2.0.20 (64bit) on [x] ***\n    *** Operational MODE: async ***\n    mounting hello:app on /\n    spawned uWSGI master process (pid: x)\n    spawned uWSGI worker 1 (pid: x, cores: 100)\n    spawned uWSGI http 1 (pid: x)\n    *** running gevent loop engine [addr:x] ***\n\n\n.. _gevent: https://www.gevent.org/\n"}, "files_index": [{"path": ".devcontainer", "type": "tree", "size": null}, {"path": ".devcontainer/devcontainer.json", "type": "blob", "size": 434}, {"path": ".devcontainer/on-create-command.sh", "type": "blob", "size": 165}, {"path": ".editorconfig", "type": "blob", "size": 233}, {"path": ".github", "type": "tree", "size": null}, {"path": ".github/ISSUE_TEMPLATE", "type": "tree", "size": null}, {"path": ".github/ISSUE_TEMPLATE/bug-report.md", "type": "blob", "size": 615}, {"path": ".github/ISSUE_TEMPLATE/config.yml", "type": "blob", "size": 511}, {"path": ".github/ISSUE_TEMPLATE/feature-request.md", "type": "blob", "size": 416}, {"path": ".github/pull_request_template.md", "type": "blob", "size": 822}, {"path": ".github/workflows", "type": "tree", "size": null}, {"path": ".github/workflows/lock.yaml", "type": "blob", "size": 682}, {"path": ".github/workflows/pre-commit.yaml", "type": "blob", "size": 983}, {"path": ".github/workflows/publish.yaml", "type": "blob", "size": 1946}, {"path": ".github/workflows/tests.yaml", "type": "blob", "size": 1937}, {"path": ".gitignore", "type": "blob", "size": 74}, {"path": ".pre-commit-config.yaml", "type": "blob", "size": 629}, {"path": ".readthedocs.yaml", "type": "blob", "size": 242}, {"path": "CHANGES.rst", "type": "blob", "size": 72137}, {"path": "LICENSE.txt", "type": "blob", "size": 1475}, {"path": "README.md", "type": "blob", "size": 1639}, {"path": "docs", "type": "tree", "size": null}, {"path": "docs/Makefile", "type": "blob", "size": 634}, {"path": "docs/_static", "type": "tree", "size": null}, {"path": "docs/_static/debugger.png", "type": "blob", "size": 207889}, {"path": "docs/_static/flask-icon.svg", "type": "blob", "size": 1999}, {"path": "docs/_static/flask-logo.svg", "type": "blob", "size": 3455}, {"path": "docs/_static/flask-name.svg", "type": "blob", "size": 5311}, {"path": "docs/_static/pycharm-run-config.png", "type": "blob", "size": 99654}, {"path": "docs/api.rst", "type": "blob", "size": 21212}, {"path": "docs/appcontext.rst", "type": "blob", "size": 7653}, {"path": "docs/async-await.rst", "type": "blob", "size": 4870}, {"path": "docs/blueprints.rst", "type": "blob", "size": 12559}, {"path": "docs/changes.rst", "type": "blob", "size": 45}, {"path": "docs/cli.rst", "type": "blob", "size": 16701}, {"path": "docs/conf.py", "type": "blob", "size": 3386}, {"path": "docs/config.rst", "type": "blob", "size": 29011}, {"path": "docs/contributing.rst", "type": "blob", "size": 274}, {"path": "docs/debugging.rst", "type": "blob", "size": 3462}, {"path": "docs/deploying", "type": "tree", "size": null}, {"path": "docs/deploying/apache-httpd.rst", "type": "blob", "size": 2364}, {"path": "docs/deploying/asgi.rst", "type": "blob", "size": 673}, {"path": "docs/deploying/eventlet.rst", "type": "blob", "size": 2405}, {"path": "docs/deploying/gevent.rst", "type": "blob", "size": 2417}, {"path": "docs/deploying/gunicorn.rst", "type": "blob", "size": 4042}, {"path": "docs/deploying/index.rst", "type": "blob", "size": 3186}, {"path": "docs/deploying/mod_wsgi.rst", "type": "blob", "size": 2813}, {"path": "docs/deploying/nginx.rst", "type": "blob", "size": 2316}, {"path": "docs/deploying/proxy_fix.rst", "type": "blob", "size": 1365}, {"path": "docs/deploying/uwsgi.rst", "type": "blob", "size": 4768}, {"path": "docs/deploying/waitress.rst", "type": "blob", "size": 2337}, {"path": "docs/design.rst", "type": "blob", "size": 11214}, {"path": "docs/errorhandling.rst", "type": "blob", "size": 18373}, {"path": "docs/extensiondev.rst", "type": "blob", "size": 12918}, {"path": "docs/extensions.rst", "type": "blob", "size": 1372}, {"path": "docs/index.rst", "type": "blob", "size": 2055}, {"path": "docs/installation.rst", "type": "blob", "size": 3772}, {"path": "docs/license.rst", "type": "blob", "size": 98}, {"path": "docs/lifecycle.rst", "type": "blob", "size": 8987}, {"path": "docs/logging.rst", "type": "blob", "size": 5927}, {"path": "docs/make.bat", "type": "blob", "size": 760}, {"path": "docs/patterns", "type": "tree", "size": null}, {"path": "docs/patterns/appdispatch.rst", "type": "blob", "size": 6507}, {"path": "docs/patterns/appfactories.rst", "type": "blob", "size": 4098}, {"path": "docs/patterns/caching.rst", "type": "blob", "size": 653}, {"path": "docs/patterns/celery.rst", "type": "blob", "size": 8670}, {"path": "docs/patterns/deferredcallbacks.rst", "type": "blob", "size": 1848}, {"path": "docs/patterns/favicon.rst", "type": "blob", "size": 2101}, {"path": "docs/patterns/fileuploads.rst", "type": "blob", "size": 7297}, {"path": "docs/patterns/flashing.rst", "type": "blob", "size": 4536}, {"path": "docs/patterns/index.rst", "type": "blob", "size": 961}, {"path": "docs/patterns/javascript.rst", "type": "blob", "size": 8964}, {"path": "docs/patterns/jquery.rst", "type": "blob", "size": 96}, {"path": "docs/patterns/lazyloading.rst", "type": "blob", "size": 3847}, {"path": "docs/patterns/methodoverrides.rst", "type": "blob", "size": 1456}, {"path": "docs/patterns/mongoengine.rst", "type": "blob", "size": 2857}, {"path": "docs/patterns/packages.rst", "type": "blob", "size": 4105}, {"path": "docs/patterns/requestchecksum.rst", "type": "blob", "size": 1861}, {"path": "docs/patterns/singlepageapplications.rst", "type": "blob", "size": 731}, {"path": "docs/patterns/sqlalchemy.rst", "type": "blob", "size": 7265}, {"path": "docs/patterns/sqlite3.rst", "type": "blob", "size": 5028}, {"path": "docs/patterns/streaming.rst", "type": "blob", "size": 2654}, {"path": "docs/patterns/subclassing.rst", "type": "blob", "size": 676}, {"path": "docs/patterns/templateinheritance.rst", "type": "blob", "size": 2197}, {"path": "docs/patterns/urlprocessors.rst", "type": "blob", "size": 4306}, {"path": "docs/patterns/viewdecorators.rst", "type": "blob", "size": 6318}, {"path": "docs/patterns/wtforms.rst", "type": "blob", "size": 4708}, {"path": "docs/quickstart.rst", "type": "blob", "size": 30055}, {"path": "docs/reqcontext.rst", "type": "blob", "size": 93}, {"path": "docs/server.rst", "type": "blob", "size": 3712}, {"path": "docs/shell.rst", "type": "blob", "size": 2853}, {"path": "docs/signals.rst", "type": "blob", "size": 6107}, {"path": "docs/templating.rst", "type": "blob", "size": 8611}, {"path": "docs/testing.rst", "type": "blob", "size": 10426}, {"path": "docs/tutorial", "type": "tree", "size": null}, {"path": "docs/tutorial/blog.rst", "type": "blob", "size": 11388}, {"path": "docs/tutorial/database.rst", "type": "blob", "size": 7028}, {"path": "docs/tutorial/deploy.rst", "type": "blob", "size": 3699}, {"path": "docs/tutorial/factory.rst", "type": "blob", "size": 5921}, {"path": "docs/tutorial/flaskr_edit.png", "type": "blob", "size": 13259}, {"path": "docs/tutorial/flaskr_index.png", "type": "blob", "size": 11675}, {"path": "docs/tutorial/flaskr_login.png", "type": "blob", "size": 7455}, {"path": "docs/tutorial/index.rst", "type": "blob", "size": 2061}, {"path": "docs/tutorial/install.rst", "type": "blob", "size": 2838}, {"path": "docs/tutorial/layout.rst", "type": "blob", "size": 2942}, {"path": "docs/tutorial/next.rst", "type": "blob", "size": 1631}, {"path": "docs/tutorial/static.rst", "type": "blob", "size": 3170}, {"path": "docs/tutorial/templates.rst", "type": "blob", "size": 6962}, {"path": "docs/tutorial/tests.rst", "type": "blob", "size": 18268}, {"path": "docs/tutorial/views.rst", "type": "blob", "size": 10924}, {"path": "docs/views.rst", "type": "blob", "size": 10270}, {"path": "docs/web-security.rst", "type": "blob", "size": 13085}, {"path": "examples", "type": "tree", "size": null}, {"path": "examples/celery", "type": "tree", "size": null}, {"path": "examples/celery/README.md", "type": "blob", "size": 950}, {"path": "examples/celery/make_celery.py", "type": "blob", "size": 102}, {"path": "examples/celery/pyproject.toml", "type": "blob", "size": 381}, {"path": "examples/celery/requirements.txt", "type": "blob", "size": 1073}, {"path": "examples/celery/src", "type": "tree", "size": null}, {"path": "examples/celery/src/task_app", "type": "tree", "size": null}, {"path": "examples/celery/src/task_app/__init__.py", "type": "blob", "size": 1019}, {"path": "examples/celery/src/task_app/tasks.py", "type": "blob", "size": 484}, {"path": "examples/celery/src/task_app/templates", "type": "tree", "size": null}, {"path": "examples/celery/src/task_app/templates/index.html", "type": "blob", "size": 2909}, {"path": "examples/celery/src/task_app/views.py", "type": "blob", "size": 954}, {"path": "examples/javascript", "type": "tree", "size": null}, {"path": "examples/javascript/.gitignore", "type": "blob", "size": 119}, {"path": "examples/javascript/LICENSE.txt", "type": "blob", "size": 1475}, {"path": "examples/javascript/README.rst", "type": "blob", "size": 1086}, {"path": "examples/javascript/js_example", "type": "tree", "size": null}, {"path": "examples/javascript/js_example/__init__.py", "type": "blob", "size": 97}, {"path": "examples/javascript/js_example/templates", "type": "tree", "size": null}, {"path": "examples/javascript/js_example/templates/base.html", "type": "blob", "size": 1178}, {"path": "examples/javascript/js_example/templates/fetch.html", "type": "blob", "size": 852}, {"path": "examples/javascript/js_example/templates/jquery.html", "type": "blob", "size": 666}, {"path": "examples/javascript/js_example/templates/xhr.html", "type": "blob", "size": 933}, {"path": "examples/javascript/js_example/views.py", "type": "blob", "size": 429}, {"path": "examples/javascript/pyproject.toml", "type": "blob", "size": 736}, {"path": "examples/javascript/tests", "type": "tree", "size": null}, {"path": "examples/javascript/tests/conftest.py", "type": "blob", "size": 216}, {"path": "examples/javascript/tests/test_js_example.py", "type": "blob", "size": 727}, {"path": "examples/tutorial", "type": "tree", "size": null}, {"path": "examples/tutorial/.gitignore", "type": "blob", "size": 119}, {"path": "examples/tutorial/LICENSE.txt", "type": "blob", "size": 1475}, {"path": "examples/tutorial/README.rst", "type": "blob", "size": 1273}, {"path": "examples/tutorial/flaskr", "type": "tree", "size": null}, {"path": "examples/tutorial/flaskr/__init__.py", "type": "blob", "size": 1437}, {"path": "examples/tutorial/flaskr/auth.py", "type": "blob", "size": 3296}, {"path": "examples/tutorial/flaskr/blog.py", "type": "blob", "size": 3305}, {"path": "examples/tutorial/flaskr/db.py", "type": "blob", "size": 1317}, {"path": "examples/tutorial/flaskr/schema.sql", "type": "blob", "size": 498}, {"path": "examples/tutorial/flaskr/static", "type": "tree", "size": null}, {"path": "examples/tutorial/flaskr/static/style.css", "type": "blob", "size": 1696}, {"path": "examples/tutorial/flaskr/templates", "type": "tree", "size": null}, {"path": "examples/tutorial/flaskr/templates/auth", "type": "tree", "size": null}, {"path": "examples/tutorial/flaskr/templates/auth/login.html", "type": "blob", "size": 424}, {"path": "examples/tutorial/flaskr/templates/auth/register.html", "type": "blob", "size": 428}, {"path": "examples/tutorial/flaskr/templates/base.html", "type": "blob", "size": 752}, {"path": "examples/tutorial/flaskr/templates/blog", "type": "tree", "size": null}, {"path": "examples/tutorial/flaskr/templates/blog/create.html", "type": "blob", "size": 447}, {"path": "examples/tutorial/flaskr/templates/blog/index.html", "type": "blob", "size": 790}, {"path": "examples/tutorial/flaskr/templates/blog/update.html", "type": "blob", "size": 690}, {"path": "examples/tutorial/pyproject.toml", "type": "blob", "size": 771}, {"path": "examples/tutorial/tests", "type": "tree", "size": null}, {"path": "examples/tutorial/tests/conftest.py", "type": "blob", "size": 1454}, {"path": "examples/tutorial/tests/data.sql", "type": "blob", "size": 394}, {"path": "examples/tutorial/tests/test_auth.py", "type": "blob", "size": 2070}, {"path": "examples/tutorial/tests/test_blog.py", "type": "blob", "size": 2503}, {"path": "examples/tutorial/tests/test_db.py", "type": "blob", "size": 616}, {"path": "examples/tutorial/tests/test_factory.py", "type": "blob", "size": 298}, {"path": "pyproject.toml", "type": "blob", "size": 6531}, {"path": "src", "type": "tree", "size": null}, {"path": "src/flask", "type": "tree", "size": null}, {"path": "src/flask/__init__.py", "type": "blob", "size": 2072}, {"path": "src/flask/__main__.py", "type": "blob", "size": 30}, {"path": "src/flask/app.py", "type": "blob", "size": 60738}, {"path": "src/flask/blueprints.py", "type": "blob", "size": 4541}, {"path": "src/flask/cli.py", "type": "blob", "size": 36808}, {"path": "src/flask/config.py", "type": "blob", "size": 13219}, {"path": "src/flask/ctx.py", "type": "blob", "size": 17234}, {"path": "src/flask/debughelpers.py", "type": "blob", "size": 6070}, {"path": "src/flask/globals.py", "type": "blob", "size": 2414}, {"path": "src/flask/helpers.py", "type": "blob", "size": 23064}, {"path": "src/flask/json", "type": "tree", "size": null}, {"path": "src/flask/json/__init__.py", "type": "blob", "size": 5583}, {"path": "src/flask/json/provider.py", "type": "blob", "size": 7644}, {"path": "src/flask/json/tag.py", "type": "blob", "size": 9281}, {"path": "src/flask/logging.py", "type": "blob", "size": 2377}, {"path": "src/flask/py.typed", "type": "blob", "size": 0}, {"path": "src/flask/sansio", "type": "tree", "size": null}, {"path": "src/flask/sansio/README.md", "type": "blob", "size": 228}, {"path": "src/flask/sansio/app.py", "type": "blob", "size": 39459}, {"path": "src/flask/sansio/blueprints.py", "type": "blob", "size": 27017}, {"path": "src/flask/sansio/scaffold.py", "type": "blob", "size": 30364}, {"path": "src/flask/sessions.py", "type": "blob", "size": 15480}, {"path": "src/flask/signals.py", "type": "blob", "size": 750}, {"path": "src/flask/templating.py", "type": "blob", "size": 7206}, {"path": "src/flask/testing.py", "type": "blob", "size": 10114}, {"path": "src/flask/typing.py", "type": "blob", "size": 3114}, {"path": "src/flask/views.py", "type": "blob", "size": 6962}, {"path": "src/flask/wrappers.py", "type": "blob", "size": 9406}, {"path": "tests", "type": "tree", "size": null}, {"path": "tests/conftest.py", "type": "blob", "size": 3293}, {"path": "tests/static", "type": "tree", "size": null}, {"path": "tests/static/config.json", "type": "blob", "size": 54}, {"path": "tests/static/config.toml", "type": "blob", "size": 35}, {"path": "tests/static/index.html", "type": "blob", "size": 22}, {"path": "tests/templates", "type": "tree", "size": null}, {"path": "tests/templates/_macro.html", "type": "blob", "size": 55}, {"path": "tests/templates/context_template.html", "type": "blob", "size": 36}, {"path": "tests/templates/escaping_template.html", "type": "blob", "size": 147}, {"path": "tests/templates/mail.txt", "type": "blob", "size": 14}, {"path": "tests/templates/nested", "type": "tree", "size": null}, {"path": "tests/templates/nested/nested.txt", "type": "blob", "size": 11}, {"path": "tests/templates/non_escaping_template.txt", "type": "blob", "size": 169}, {"path": "tests/templates/simple_template.html", "type": "blob", "size": 23}, {"path": "tests/templates/template_filter.html", "type": "blob", "size": 26}, {"path": "tests/templates/template_test.html", "type": "blob", "size": 51}, {"path": "tests/test_appctx.py", "type": "blob", "size": 5090}, {"path": "tests/test_apps", "type": "tree", "size": null}, {"path": "tests/test_apps/.env", "type": "blob", "size": 33}, {"path": "tests/test_apps/.flaskenv", "type": "blob", "size": 28}, {"path": "tests/test_apps/blueprintapp", "type": "tree", "size": null}, {"path": "tests/test_apps/blueprintapp/__init__.py", "type": "blob", "size": 256}, {"path": "tests/test_apps/blueprintapp/apps", "type": "tree", "size": null}, {"path": "tests/test_apps/blueprintapp/apps/__init__.py", "type": "blob", "size": 0}, {"path": "tests/test_apps/blueprintapp/apps/admin", "type": "tree", "size": null}, {"path": "tests/test_apps/blueprintapp/apps/admin/__init__.py", "type": "blob", "size": 366}, {"path": "tests/test_apps/blueprintapp/apps/admin/static", "type": "tree", "size": null}, {"path": "tests/test_apps/blueprintapp/apps/admin/static/css", "type": "tree", "size": null}, {"path": "tests/test_apps/blueprintapp/apps/admin/static/css/test.css", "type": "blob", "size": 18}, {"path": "tests/test_apps/blueprintapp/apps/admin/static/test.txt", "type": "blob", "size": 11}, {"path": "tests/test_apps/blueprintapp/apps/admin/templates", "type": "tree", "size": null}, {"path": "tests/test_apps/blueprintapp/apps/admin/templates/admin", "type": "tree", "size": null}, {"path": "tests/test_apps/blueprintapp/apps/admin/templates/admin/index.html", "type": "blob", "size": 21}, {"path": "tests/test_apps/blueprintapp/apps/frontend", "type": "tree", "size": null}, {"path": "tests/test_apps/blueprintapp/apps/frontend/__init__.py", "type": "blob", "size": 327}, {"path": "tests/test_apps/blueprintapp/apps/frontend/templates", "type": "tree", "size": null}, {"path": "tests/test_apps/blueprintapp/apps/frontend/templates/frontend", "type": "tree", "size": null}, {"path": "tests/test_apps/blueprintapp/apps/frontend/templates/frontend/index.html", "type": "blob", "size": 24}, {"path": "tests/test_apps/cliapp", "type": "tree", "size": null}, {"path": "tests/test_apps/cliapp/__init__.py", "type": "blob", "size": 0}, {"path": "tests/test_apps/cliapp/app.py", "type": "blob", "size": 52}, {"path": "tests/test_apps/cliapp/factory.py", "type": "blob", "size": 169}, {"path": "tests/test_apps/cliapp/importerrorapp.py", "type": "blob", "size": 73}, {"path": "tests/test_apps/cliapp/inner1", "type": "tree", "size": null}, {"path": "tests/test_apps/cliapp/inner1/__init__.py", "type": "blob", "size": 55}, {"path": "tests/test_apps/cliapp/inner1/inner2", "type": "tree", "size": null}, {"path": "tests/test_apps/cliapp/inner1/inner2/__init__.py", "type": "blob", "size": 0}, {"path": "tests/test_apps/cliapp/inner1/inner2/flask.py", "type": "blob", "size": 47}, {"path": "tests/test_apps/cliapp/message.txt", "type": "blob", "size": 38}, {"path": "tests/test_apps/cliapp/multiapp.py", "type": "blob", "size": 67}, {"path": "tests/test_apps/helloworld", "type": "tree", "size": null}, {"path": "tests/test_apps/helloworld/hello.py", "type": "blob", "size": 104}, {"path": "tests/test_apps/helloworld/wsgi.py", "type": "blob", "size": 36}, {"path": "tests/test_apps/subdomaintestmodule", "type": "tree", "size": null}, {"path": "tests/test_apps/subdomaintestmodule/__init__.py", "type": "blob", "size": 73}, {"path": "tests/test_apps/subdomaintestmodule/static", "type": "tree", "size": null}, {"path": "tests/test_apps/subdomaintestmodule/static/hello.txt", "type": "blob", "size": 16}, {"path": "tests/test_async.py", "type": "blob", "size": 3335}, {"path": "tests/test_basic.py", "type": "blob", "size": 53603}, {"path": "tests/test_blueprints.py", "type": "blob", "size": 31909}, {"path": "tests/test_cli.py", "type": "blob", "size": 20382}, {"path": "tests/test_config.py", "type": "blob", "size": 7748}, {"path": "tests/test_converters.py", "type": "blob", "size": 1093}, {"path": "tests/test_helpers.py", "type": "blob", "size": 11224}, {"path": "tests/test_instance_config.py", "type": "blob", "size": 3317}, {"path": "tests/test_json.py", "type": "blob", "size": 8741}, {"path": "tests/test_json_tag.py", "type": "blob", "size": 1998}, {"path": "tests/test_logging.py", "type": "blob", "size": 2529}, {"path": "tests/test_regression.py", "type": "blob", "size": 712}, {"path": "tests/test_reqctx.py", "type": "blob", "size": 8677}, {"path": "tests/test_request.py", "type": "blob", "size": 1902}, {"path": "tests/test_session_interface.py", "type": "blob", "size": 784}, {"path": "tests/test_signals.py", "type": "blob", "size": 4864}, {"path": "tests/test_subclassing.py", "type": "blob", "size": 475}, {"path": "tests/test_templating.py", "type": "blob", "size": 15403}, {"path": "tests/test_testing.py", "type": "blob", "size": 10259}, {"path": "tests/test_user_error_handler.py", "type": "blob", "size": 8436}, {"path": "tests/test_views.py", "type": "blob", "size": 6732}, {"path": "tests/type_check", "type": "tree", "size": null}, {"path": "tests/type_check/typing_app_decorators.py", "type": "blob", "size": 589}, {"path": "tests/type_check/typing_error_handler.py", "type": "blob", "size": 647}, {"path": "tests/type_check/typing_route.py", "type": "blob", "size": 2532}, {"path": "uv.lock", "type": "blob", "size": 226474}], "contributors": {"davidism": 1773, "mitsuhiko": 1189, "untitaker": 274, "rduplain": 122, "greyli": 105, "dependabot-preview[bot]": 91, "DasIch": 86, "dependabot[bot]": 61, "kennethreitz": 59, "anon:Kenneth Reitz": 57, "pgjones": 50, "pre-commit-ci[bot]": 39, "keyan": 36, "anon:Daniel Neuh\u00e4user": 32, "ThiefMaster": 28, "defuz": 26, "lepture": 25, "ThomasWaldmann": 23, "jeffwidman": 22, "jab": 22, "SimonSapin": 18, "lord63": 17, "florentx": 17, "adambyrtek": 13, "dag": 13, "wgwz": 13, "s3rvac": 12, "flying-sheep": 12, "dawranliou": 10, "plaes": 9, "garenchan": 9, "methane": 8, "anon:Eugene M. Kim": 8, "vtbassmatt": 8, "tullyrankin": 7, "miguelgrinberg": 7, "yuxiaoy1": 7, "cvrebert": 6, "anon:matt swanson": 6, "anon:Sven-Hendrik Haase": 6, "Winnetou": 6, "aqt01": 6, "hendrikmakait": 6, "singingwolfboy": 6, "lordmauve": 6, "antsar": 6, "alexpantyukhin": 6, "akavlie": 6, "jfinkels": 5, "anon:max demian": 5, "svenstaro": 5, "s0undt3ch": 5, "atdt": 5, "ivanovmg": 5, "anon:Matt Wright": 5, "mwcampbell": 5, "justquick": 5, "homeworkprod": 5, "anon:Jimmy McCarthy": 5, "dhaaker": 5, "doobeh": 5, "adamzap": 5, "Akasurde": 4, "garbados": 4, "MikeTheReader": 4, "njl": 4, "Jalkhov": 4, "RaHus": 4, "cerickson": 4, "wilsaj": 4, "hyunchel": 4, "anon:Alex Couper": 4, "aphedges": 4, "antlarr": 4, "bsutherland": 4, "Diggsey": 4, "dmishe": 4, "EtiennePelletier": 4, "anon:FND": 4, "anon:Kevin Burke": 4, "lgiordani": 4, "davebarkerxyz": 3, "rzelayafavila": 3, "sebest": 3, "sourya": 3, "anon:ThomasWaldmann": 3, "tony": 3, "Yourun-proger": 3, "mrluanma": 3, "awsum": 3, "jackwardell": 3, "jgraeme": 3, "mvantellingen": 3, "wimglenn": 3, "drewja": 3, "tirkarthi": 3, "lobeck": 3, "accraze": 3, "dave-shawley": 3, "eso31": 3}, "_source": {"fetched_at": 1758916340.1014218, "api_base": "https://api.github.com/repos/pallets/flask", "limits": {"max_files": 25, "max_total_bytes": 1000000}}}, "fetched_at": 1758916340.1014218}}